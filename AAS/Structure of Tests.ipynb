{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1daf53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT NOTE: This notebook contains the creation of almost all models. Therefore a teststructure was build and reused\n",
    "#The different cells implement different settings or ML structures where the loops and the MODEL HERE part are changed\n",
    "\n",
    "#For each ML architecture, the results are written to a specific csv file\n",
    "#There, Each model is represented by a new line with a unique index, the used parameter settings as well as different metrics\n",
    "\n",
    "#This cell initializes the DataFrame for the XGB models. Running it would overwrite all stored results\n",
    "'''#XGB Results\n",
    "results = pd.DataFrame(columns=[\"max_depth\", \"learning_rate\", \"n_estimators\", \"min_child_weight\", \"subsample\", \"objective\", \"eval_metric\", \"Seed\", \"VBS_test\", \"SBS_test\", \"Accuracy_test\", \"Score_test\", \"Gap_test\", \"VBS_train\", \"SBS_train\",\"Accuracy_train\", \"Score_train\", \"Gap_train\"])\n",
    "results.to_csv(\"XGB_results.csv\", index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")\n",
    "results.to_csv(\"XGB_results_to_c_z.csv\", index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73b08052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization of the second result file for the XGBClassifier\n",
    "'''results = pd.DataFrame(columns=[\"max_depth\", \"learning_rate\", \"n_estimators\", \"min_child_weight\", \"subsample\", \"Seed\", \"VBS_test\", \"SBS_test\", \"Accuracy_test\", \"Score_test\", \"Gap_test\", \"VBS_train\", \"SBS_train\",\"Accuracy_train\", \"Score_train\", \"Gap_train\"])\n",
    "results.to_csv(\"XGB_results2.csv\", index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual GridSearch for XGBoost Constraints\n",
    "#Already import the packages necessary for all models\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display\n",
    "\n",
    "#Function to convert labels to integers\n",
    "def label_to_int(label):\n",
    "    for index in label.index: #For every instance\n",
    "        match label.at[index, \"algorithm\"]: #Based on the algorithm, write an integer value\n",
    "            case \"greedy\": label.at[index, \"algorithm\"] = 0\n",
    "            case \"dynamic_programming\": label.at[index, \"algorithm\"] = 1\n",
    "            case \"branch_and_bound\": label.at[index, \"algorithm\"] = 2\n",
    "            case _: pass \n",
    "\n",
    "#Function to convert integers to labels\n",
    "def int_to_label(label):\n",
    "    for index in label.index:\n",
    "        match label.at[index, \"algorithm\"]:\n",
    "            case 0: label.at[index, \"algorithm\"] = \"greedy\"\n",
    "            case 1: label.at[index, \"algorithm\"] = \"dynamic_programming\"\n",
    "            case 2: label.at[index, \"algorithm\"] = \"branch_and_bound\"\n",
    "            case _: pass\n",
    "\n",
    "#Function to determine the average score given the predictions vector\n",
    "#Given the scores and the predictions as dataframe with an index and a single column \"algorithm\"\n",
    "def calculate_score(scores, predictions):\n",
    "    for index in predictions.index: #For each instance\n",
    "        \n",
    "        #Read the prediction\n",
    "        algorithm = predictions.at[index, \"algorithm\"]\n",
    "        \n",
    "        #Write the score to a new column \"score\"\n",
    "        predictions.at[index, \"score\"] = scores.at[index, algorithm]\n",
    "    \n",
    "    #Return the mean of the scores\n",
    "    return predictions[\"score\"].mean()\n",
    "\n",
    "#For loops to test every single combination of parameters\n",
    "\n",
    "#Parameter settings for the first generation of XGB models\n",
    "'''for max_depth in [3, 6]:\n",
    "    for learning_rate in [0.01, 0.1]:\n",
    "        for n_estimators in [100, 200]:\n",
    "            for min_child_weight in [1, 2]:\n",
    "                for subsample in [0.8, 1]:\n",
    "                    #Based on the scaled instances, the result path is set accordingly\n",
    "                    for scaled_instances in [\"instances.csv\", \"instances_to_c_z.csv\"]'''\n",
    "\n",
    "for max_depth in [2, 3, 4]:\n",
    "    for learning_rate in [0.001, 0.01]:\n",
    "        for n_estimators in [50, 100, 200]:\n",
    "            for min_child_weight in [0.5, 1, 2]:\n",
    "                for subsample in [0.6, 0.7, 0.8, 0.9]:\n",
    "                    \n",
    "                    #Use identical split for the train and test set\n",
    "                    seed = 1\n",
    "                    \n",
    "                    #Define a model description that contains every parameter to create a unique name for each model\n",
    "                    model_description = \"XGB_depth_\"+str(max_depth)+\"_lr_\"+str(learning_rate)+\"_estimators_\"+str(n_estimators)+\"_childweight_\"+str(min_child_weight)+\"_subsample_\"+str(subsample)+\"_seed_\"+str(seed)\n",
    "                    \n",
    "                    #Set the result and instance path\n",
    "                    result_path = \"XGB_results2.csv\"\n",
    "                    scaled_instances = \"instances.csv\"\n",
    "                    \n",
    "                    #Read the scores (independent from the instance path)\n",
    "                    scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "                    scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "                    \n",
    "                    #Load the data and results from the given paths\n",
    "                    data = pd.DataFrame(pd.read_csv(scaled_instances, header=0, index_col=0))\n",
    "                    results = pd.DataFrame(pd.read_csv(result_path, header=0, index_col=0))\n",
    "\n",
    "                    #Write the parameter settings to the DataFrame results (Twice as they are already part of the index)\n",
    "                    results.at[model_description, \"max_depth\"] = max_depth\n",
    "                    results.at[model_description, \"learning_rate\"] = learning_rate\n",
    "                    results.at[model_description, \"n_estimators\"] = n_estimators\n",
    "                    results.at[model_description, \"min_child_weight\"] = min_child_weight\n",
    "                    results.at[model_description, \"subsample\"] = subsample\n",
    "                    results.at[model_description, \"Seed\"] = seed\n",
    "\n",
    "                    #Split the data into train sed and test set using the set seed of 1\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(data, scores, test_size=0.2, random_state=seed)\n",
    "\n",
    "                    \n",
    "                    #Calculation of SBS and VBS for the test set (identical across all instances)\n",
    "                    test_scores = pd.DataFrame(y_test)\n",
    "                    \n",
    "                    #Add a column containing the minimum value of the three algorithms\n",
    "                    test_scores[\"best_solver\"] = test_scores.min(axis=1)\n",
    "                    \n",
    "                    #Calculate the average scores for each algorithm (minimum is used as SBS) and the VBS\n",
    "                    average_greedy, average_dynamic_programming, average_branch_and_bound, test_average_best_solver = test_scores.mean(axis=0)\n",
    "                    test_average_single_best_solver = min(average_greedy, average_dynamic_programming, average_branch_and_bound)\n",
    "\n",
    "                    #Write the scores to the DataFrame results\n",
    "                    results.at[model_description, \"VBS_test\"] = test_average_best_solver\n",
    "                    results.at[model_description, \"SBS_test\"] = test_average_single_best_solver\n",
    "\n",
    "                    \n",
    "                    #Calculation of SBS and VBS for the train set (identical to the calculation on the test set)\n",
    "                    train_scores = pd.DataFrame(y_train)\n",
    "                    train_scores[\"best_solver\"] = train_scores.min(axis=1)\n",
    "                    \n",
    "                    #SBS and VBS as described before\n",
    "                    average_greedy, average_dynamic_programming, average_branch_and_bound, train_average_best_solver = train_scores.mean(axis=0)\n",
    "                    train_average_single_best_solver = min(average_greedy, average_dynamic_programming, average_branch_and_bound)\n",
    "\n",
    "                    #Write the scores to the DataFrame results (identical across all instances)\n",
    "                    results.at[model_description, \"VBS_train\"] = train_average_best_solver\n",
    "                    results.at[model_description, \"SBS_train\"] = train_average_single_best_solver\n",
    "\n",
    "                    #Prepare the labels as a DataFrame with a single column containing the optimal algorithm for each instance\n",
    "                    y_train = pd.DataFrame(y_train.idxmin(axis=1), columns=[\"algorithm\"])\n",
    "                    y_test = pd.DataFrame(y_test.idxmin(axis=1), columns=[\"algorithm\"])\n",
    "                    \n",
    "                    #Copy the labels to use them for the calculation of the accuracy\n",
    "                    y_train_accuracy = y_train.copy()\n",
    "                    y_test_accuracy = y_test.copy()\n",
    "                    \n",
    "                    #Convert the labels to an integer representation\n",
    "                    label_to_int(y_train)\n",
    "                    y_train = y_train.astype(int)\n",
    "                    label_to_int(y_test)\n",
    "                    y_test = y_test.astype(int)\n",
    "\n",
    "                    #Save the indices of the labels\n",
    "                    y_train_indices = y_train.index\n",
    "                    y_test_indices = y_test.index\n",
    "\n",
    "                    #---------------------------------------------------------------------------------------------------------------\n",
    "                    #Part where the model is defined and used to make predictions\n",
    "                    #This is the main difference across the cells in this notebook\n",
    "                    \n",
    "                    #MODEL HERE\n",
    "                    \n",
    "                    #Define the XGBClassifier model with the given parameters\n",
    "                    model = XGBClassifier(\n",
    "                        max_depth=max_depth,\n",
    "                        learning_rate=learning_rate,\n",
    "                        n_estimators=n_estimators,\n",
    "                        min_child_weight=min_child_weight,\n",
    "                        subsample=subsample,\n",
    "                        seed=1\n",
    "                    )\n",
    "\n",
    "                    #Train the model on the train data\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                    #Use the trained model to predict the algorithms on the train set (with according indices)\n",
    "                    predictions_train = pd.DataFrame(model.predict(X_train), columns=[\"algorithm\"], index=y_train_indices)\n",
    "                    \n",
    "                    #Use the trained model to predict the algorithms on the test set (with according indices)\n",
    "                    predictions_test = pd.DataFrame(model.predict(X_test), columns=[\"algorithm\"], index=y_test_indices)\n",
    "                    \n",
    "                    #End of model description part, following content is identical across the cells\n",
    "                    #---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                    \n",
    "                    #Convert the predictions on the train set to labels\n",
    "                    int_to_label(predictions_train)\n",
    "\n",
    "                    #Load the scores so that they are uneffected by any previous calculations\n",
    "                    scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "                    scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "\n",
    "                    #Calculate the score for the train set and save it in the DataFrame results\n",
    "                    score_train = calculate_score(scores, predictions_train)\n",
    "                    results.at[model_description, \"Score_train\"] = score_train\n",
    "\n",
    "                    \n",
    "                    #Convert the predictions on the test set to labels\n",
    "                    int_to_label(predictions_test)\n",
    "\n",
    "                    #Load the scores once more\n",
    "                    scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "                    scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "\n",
    "                    #Calculate the score on the test set and write it to the results\n",
    "                    score_test = calculate_score(scores, predictions_test)\n",
    "                    results.at[model_description, \"Score_test\"] = score_test\n",
    "\n",
    "                    \n",
    "                    #Calculation of the accuracy\n",
    "                    predictions_train = pd.DataFrame(predictions_train)\n",
    "                    predictions_test = pd.DataFrame(predictions_test)\n",
    "\n",
    "                    #Use sklearn.metrics function to calculate the accuracy and round it for an equal length\n",
    "                    accuracy_train = accuracy_score(y_train_accuracy, predictions_train[\"algorithm\"]).round(4)\n",
    "                    accuracy_test = accuracy_score(y_test_accuracy, predictions_test[\"algorithm\"]).round(4)\n",
    "                    \n",
    "                    #Write the accuracy to the results\n",
    "                    results.at[model_description, \"Accuracy_train\"] = accuracy_train\n",
    "                    results.at[model_description, \"Accuracy_test\"] = accuracy_test\n",
    "\n",
    "                    \n",
    "                    #Calculation of the gaps\n",
    "                    #Load the previously calculated scores for the train and test set\n",
    "                    VBS_train = train_average_best_solver\n",
    "                    SBS_train = train_average_single_best_solver\n",
    "                    AAS_train = score_train\n",
    "                    VBS_test = test_average_best_solver\n",
    "                    SBS_test = test_average_single_best_solver\n",
    "                    AAS_test = score_test\n",
    "\n",
    "                    #Calculate the gaps in the sets (independend of the model)\n",
    "                    Gap_train = SBS_train - VBS_train\n",
    "                    Gap_test = SBS_test - VBS_test\n",
    "                    \n",
    "                    #Calculate the achived score by subtracting the optimal score of the VBS\n",
    "                    Score_train = AAS_train - VBS_train\n",
    "                    Score_test = AAS_test - VBS_test\n",
    "\n",
    "                    #Calculate the gap closed\n",
    "                    GapClose_train = 1-(Score_train/Gap_train)\n",
    "                    GapClose_test = 1-(Score_test/Gap_test)\n",
    "\n",
    "                    #Save the gap closed in the DataFrame results\n",
    "                    results.at[model_description, \"Gap_train\"] = GapClose_train\n",
    "                    results.at[model_description, \"Gap_test\"] = GapClose_test\n",
    "\n",
    "                    \n",
    "                    #Save the DataFrame results by overwriting the csv file (This appends the new row and does not change the rest)\n",
    "                    results.to_csv(result_path, index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")\n",
    "                    \n",
    "                    #Output to keep track of the models\n",
    "                    print(str(model_description)+\" calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07562aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the results. Those results are revisited and describes in the notebook \"ResultDiscussion.ipynb\"\n",
    "\n",
    "#Load the results\n",
    "table = pd.DataFrame(pd.read_csv(\"XGB_results2.csv\", header=0, index_col=0))\n",
    "\n",
    "#Output the describtion of the results (presents average Scores, minima and maxima) \n",
    "print(table.describe())\n",
    "\n",
    "#Only present the top of the table\n",
    "table = table[:10]\n",
    "\n",
    "#Display the table\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71908231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove identical entrys from the first generation of XGB models\n",
    "XGB = pd.DataFrame(pd.read_csv(\"XGB_results.csv\", header=0, index_col=0))\n",
    "\n",
    "#Sort the rows by their VBS-SBS gap closure\n",
    "XGB = XGB.sort_values(by=[\"Gap_test\"], ascending=False)\n",
    "\n",
    "#Only keep one of the four identical entrys\n",
    "XGB = XGB[XGB.objective == \"binary:logistic\"]\n",
    "XGB = XGB[XGB.eval_metric == \"merror\"]\n",
    "\n",
    "#Drop the unused columns\n",
    "XGB = XGB.drop(labels=[\"objective\", \"eval_metric\"], axis=1)\n",
    "XGB = XGB.sort_values(by=[\"Gap_test\"], ascending=False)\n",
    "\n",
    "#Print the describtion of the results and overwrite the file\n",
    "print(XGB.describe())\n",
    "XGB.to_csv(\"XGB_results.csv\", index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce71653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bavogtaas-admin/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Define the results file and initialize the columns used for the different models\n",
    "#Running this code would overwrite the results!\n",
    "'''results = pd.DataFrame(columns=[\"C\", \"kernel\", \"class_weight\", \"degree\", \"VBS_test\", \"SBS_test\", \"Accuracy_test\", \"Score_test\", \"Gap_test\", \"VBS_train\", \"SBS_train\",\"Accuracy_train\", \"Score_train\", \"Gap_train\"])\n",
    "results.to_csv(\"SVM_results.csv\", index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757314bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_C_0.01_kernel_rbf_classweight_None_degree_3 calculated, 1/32\n",
      "SVM_C_0.01_kernel_rbf_classweight_balanced_degree_3 calculated, 2/32\n",
      "SVM_C_0.1_kernel_rbf_classweight_None_degree_3 calculated, 3/32\n",
      "SVM_C_0.1_kernel_rbf_classweight_balanced_degree_3 calculated, 4/32\n",
      "SVM_C_0.2_kernel_rbf_classweight_None_degree_3 calculated, 5/32\n",
      "SVM_C_0.2_kernel_rbf_classweight_balanced_degree_3 calculated, 6/32\n",
      "SVM_C_1_kernel_rbf_classweight_None_degree_3 calculated, 7/32\n",
      "SVM_C_1_kernel_rbf_classweight_balanced_degree_3 calculated, 8/32\n",
      "SVM_C_0.01_kernel_poly_classweight_None_degree_2 calculated, 9/32\n",
      "SVM_C_0.01_kernel_poly_classweight_None_degree_3 calculated, 10/32\n",
      "SVM_C_0.01_kernel_poly_classweight_balanced_degree_2 calculated, 11/32\n",
      "SVM_C_0.01_kernel_poly_classweight_balanced_degree_3 calculated, 12/32\n",
      "SVM_C_0.1_kernel_poly_classweight_None_degree_2 calculated, 13/32\n",
      "SVM_C_0.1_kernel_poly_classweight_None_degree_3 calculated, 14/32\n",
      "SVM_C_0.1_kernel_poly_classweight_balanced_degree_2 calculated, 15/32\n",
      "SVM_C_0.1_kernel_poly_classweight_balanced_degree_3 calculated, 16/32\n",
      "SVM_C_0.2_kernel_poly_classweight_None_degree_2 calculated, 17/32\n",
      "SVM_C_0.2_kernel_poly_classweight_None_degree_3 calculated, 18/32\n",
      "SVM_C_0.2_kernel_poly_classweight_balanced_degree_2 calculated, 19/32\n",
      "SVM_C_0.2_kernel_poly_classweight_balanced_degree_3 calculated, 20/32\n",
      "SVM_C_1_kernel_poly_classweight_None_degree_2 calculated, 21/32\n",
      "SVM_C_1_kernel_poly_classweight_None_degree_3 calculated, 22/32\n",
      "SVM_C_1_kernel_poly_classweight_balanced_degree_2 calculated, 23/32\n",
      "SVM_C_1_kernel_poly_classweight_balanced_degree_3 calculated, 24/32\n",
      "SVM_C_0.01_kernel_linear_classweight_None_degree_3 calculated, 25/32\n",
      "SVM_C_0.01_kernel_linear_classweight_balanced_degree_3 calculated, 26/32\n",
      "SVM_C_0.1_kernel_linear_classweight_None_degree_3 calculated, 27/32\n",
      "SVM_C_0.1_kernel_linear_classweight_balanced_degree_3 calculated, 28/32\n",
      "SVM_C_0.2_kernel_linear_classweight_None_degree_3 calculated, 29/32\n",
      "SVM_C_0.2_kernel_linear_classweight_balanced_degree_3 calculated, 30/32\n",
      "SVM_C_1_kernel_linear_classweight_None_degree_3 calculated, 31/32\n",
      "SVM_C_1_kernel_linear_classweight_balanced_degree_3 calculated, 32/32\n"
     ]
    }
   ],
   "source": [
    "#IMPORTANT NOTE: Most of the code is identical to the generation of the XGB models. Therefore it is not documented in detail\n",
    "#General approach for the SVC: define the code in functions to use two function calls if the kernel is \"poly\" \n",
    "#Grid Search for SVC\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display\n",
    "\n",
    "#Function to convert labels to integers and back\n",
    "def label_to_int(label):\n",
    "    for index in label.index:\n",
    "        match label.at[index, \"algorithm\"]:\n",
    "            case \"greedy\": label.at[index, \"algorithm\"] = 0\n",
    "            case \"dynamic_programming\": label.at[index, \"algorithm\"] = 1\n",
    "            case \"branch_and_bound\": label.at[index, \"algorithm\"] = 2\n",
    "            case _: pass \n",
    "\n",
    "def int_to_label(label):\n",
    "    for index in label.index:\n",
    "        match label.at[index, \"algorithm\"]:\n",
    "            case 0: label.at[index, \"algorithm\"] = \"greedy\"\n",
    "            case 1: label.at[index, \"algorithm\"] = \"dynamic_programming\"\n",
    "            case 2: label.at[index, \"algorithm\"] = \"branch_and_bound\"\n",
    "            case _: pass\n",
    "\n",
    "#Function to determine the average score given the predictions vector\n",
    "#Given the scores and the predictions as dataframe with an index and a single column \"algorithm\"\n",
    "def calculate_score(scores, predictions):\n",
    "    for index in predictions.index:\n",
    "        algorithm = predictions.at[index, \"algorithm\"]\n",
    "        predictions.at[index, \"score\"] = scores.at[index, algorithm]\n",
    "    return predictions[\"score\"].mean()\n",
    "\n",
    "#Define the code as function to execute twice if kernel = poly\n",
    "def loop_iteration(kernel, C, class_weight, counter, degree=3):\n",
    "    \n",
    "    #Set seed and tolerance equal across all instances\n",
    "    seed = 1\n",
    "    tolerance = 0.001\n",
    "    \n",
    "    #Define the unique model description used as index\n",
    "    model_description = \"SVM_C_\"+str(C)+\"_kernel_\"+str(kernel)+\"_classweight_\"+str(class_weight)+\"_degree_\"+str(degree)\n",
    "    result_path = \"SVM_results.csv\"\n",
    "    scaled_instances = \"instances.csv\"\n",
    "    \n",
    "    #Load scores, data and results\n",
    "    scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "    scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "    data = pd.DataFrame(pd.read_csv(scaled_instances, header=0, index_col=0))\n",
    "    results = pd.DataFrame(pd.read_csv(result_path, header=0, index_col=0))\n",
    "\n",
    "    #Save the parameter settings in the DataFrame results \n",
    "    results.at[model_description, \"C\"] = C\n",
    "    results.at[model_description, \"kernel\"] = kernel\n",
    "    results.at[model_description, \"class_weight\"] = class_weight\n",
    "    results.at[model_description, \"degree\"] = degree\n",
    "\n",
    "    #Split the data into train set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, scores, test_size=0.2, random_state=1)\n",
    "\n",
    "    #Calculation of SBS and VBS for test\n",
    "    test_scores = pd.DataFrame(y_test)\n",
    "    test_scores[\"best_solver\"] = test_scores.min(axis=1)\n",
    "\n",
    "    average_greedy, average_dynamic_programming, average_branch_and_bound, test_average_best_solver = test_scores.mean(axis=0)\n",
    "    test_average_single_best_solver = min(average_greedy, average_dynamic_programming, average_branch_and_bound)\n",
    "\n",
    "    results.at[model_description, \"VBS_test\"] = test_average_best_solver\n",
    "    results.at[model_description, \"SBS_test\"] = test_average_single_best_solver\n",
    "\n",
    "    #Calculation of SBS and VBS for train\n",
    "    train_scores = pd.DataFrame(y_train)\n",
    "    train_scores[\"best_solver\"] = train_scores.min(axis=1)\n",
    "\n",
    "    average_greedy, average_dynamic_programming, average_branch_and_bound, train_average_best_solver = train_scores.mean(axis=0)\n",
    "    train_average_single_best_solver = min(average_greedy, average_dynamic_programming, average_branch_and_bound)\n",
    "\n",
    "    results.at[model_description, \"VBS_train\"] = train_average_best_solver\n",
    "    results.at[model_description, \"SBS_train\"] = train_average_single_best_solver\n",
    "\n",
    "    #Prepare the labels\n",
    "    y_train = pd.DataFrame(y_train.idxmin(axis=1), columns=[\"algorithm\"])\n",
    "    y_test = pd.DataFrame(y_test.idxmin(axis=1), columns=[\"algorithm\"])\n",
    "    y_train_accuracy = y_train.copy()\n",
    "    y_test_accuracy = y_test.copy()\n",
    "\n",
    "    #Convert the labels to integers\n",
    "    label_to_int(y_train)\n",
    "    y_train = y_train.astype(int)\n",
    "    label_to_int(y_test)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    #Save the indices\n",
    "    y_train_indices = y_train.index\n",
    "    y_test_indices = y_test.index\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "    #MODEL HERE\n",
    "\n",
    "    #Define the SVM classifier (SVC)\n",
    "    model = SVC(\n",
    "        C=C,\n",
    "        kernel=kernel,\n",
    "        tol=0.001,\n",
    "        class_weight=class_weight,\n",
    "        degree = degree #For kernel = poly, all other kernels ignore this parameter\n",
    "        )\n",
    "\n",
    "    #Train the model\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    #Use the model to predict the algorithms on the train set as well as the test set\n",
    "    predictions_train = pd.DataFrame(model.predict(X_train), columns=[\"algorithm\"], index=y_train_indices)\n",
    "    predictions_test = pd.DataFrame(model.predict(X_test), columns=[\"algorithm\"], index=y_test_indices)\n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    #Calculate the score on the train set\n",
    "    int_to_label(predictions_train)\n",
    "\n",
    "    scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "    scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "\n",
    "    score_train = calculate_score(scores, predictions_train)\n",
    "    results.at[model_description, \"Score_train\"] = score_train\n",
    "\n",
    "    #Calculate the score on the test set\n",
    "    int_to_label(predictions_test)\n",
    "\n",
    "    scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "    scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "\n",
    "    score_test = calculate_score(scores, predictions_test)\n",
    "    results.at[model_description, \"Score_test\"] = score_test\n",
    "\n",
    "    #Calculate the accuracy\n",
    "    predictions_train = pd.DataFrame(predictions_train)\n",
    "    predictions_test = pd.DataFrame(predictions_test)\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train_accuracy, predictions_train[\"algorithm\"]).round(4)\n",
    "    accuracy_test = accuracy_score(y_test_accuracy, predictions_test[\"algorithm\"]).round(4)\n",
    "    results.at[model_description, \"Accuracy_train\"] = accuracy_train\n",
    "    results.at[model_description, \"Accuracy_test\"] = accuracy_test\n",
    "\n",
    "    #Calculate the gaps\n",
    "    VBS_train = train_average_best_solver\n",
    "    SBS_train = train_average_single_best_solver\n",
    "    AAS_train = score_train\n",
    "    VBS_test = test_average_best_solver\n",
    "    SBS_test = test_average_single_best_solver\n",
    "    AAS_test = score_test\n",
    "\n",
    "    Gap_train = SBS_train - VBS_train\n",
    "    Gap_test = SBS_test - VBS_test\n",
    "    Score_train = AAS_train - VBS_train\n",
    "    Score_test = AAS_test - VBS_test\n",
    "\n",
    "    GapClose_train = 1-(Score_train/Gap_train)\n",
    "    GapClose_test = 1-(Score_test/Gap_test)\n",
    "    results.at[model_description, \"Gap_train\"] = GapClose_train\n",
    "    results.at[model_description, \"Gap_test\"] = GapClose_test\n",
    "\n",
    "    results.to_csv(result_path, index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")\n",
    "    print(str(model_description)+\" calculated, \"+str(counter)+\"/32\")\n",
    "    counter += 1\n",
    "    return counter #Return counter for better progress information\n",
    "\n",
    "counter = 1 #Initialize counter\n",
    "\n",
    "#For loops to test every combination of the parameters\n",
    "for kernel in [\"rbf\", \"poly\", \"linear\"]:\n",
    "    for C in [0.01, 0.1, 0.2, 1]:\n",
    "        for class_weight in [None, \"balanced\"]:\n",
    "            \n",
    "            #Execute the defined loop twice with different degree settings if the kernel is poly\n",
    "            if kernel == \"poly\":\n",
    "                counter = loop_iteration(kernel, C, class_weight, counter, degree = 2)\n",
    "                counter = loop_iteration(kernel, C, class_weight, counter, degree = 3)\n",
    "            #Only train one model for the combination of parameters if the kernel is not poly\n",
    "            else:\n",
    "                counter = loop_iteration(kernel, C, class_weight, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfca71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_C_0.3_kernel_rbf_classweight_None_degree_3 calculated, 1/4\n",
      "SVM_C_0.5_kernel_rbf_classweight_None_degree_3 calculated, 2/4\n",
      "SVM_C_0.7_kernel_rbf_classweight_None_degree_3 calculated, 3/4\n",
      "SVM_C_0.9_kernel_rbf_classweight_None_degree_3 calculated, 4/4\n"
     ]
    }
   ],
   "source": [
    "#Additional SVC models, kernel = rbf, class_weight = None, C = [0.3, 0.5, 0.7, 0.9]\n",
    "#The same result file is used for the two generations of SVC models\n",
    "counter = 1\n",
    "for kernel in [\"rbf\"]:\n",
    "    for C in [0.3, 0.5, 0.7, 0.9]:\n",
    "        for class_weight in [None]:\n",
    "            #Using the function defined in the previous cell\n",
    "            counter = loop_iteration(kernel, C, class_weight, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5b90d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the results file for the first MLP models\n",
    "'''results = pd.DataFrame(columns=[\"model\", \"epochs\", \"weights\", \"criterion\", \"optimizer\", \"learning_rate\", \"L1_norm\", \"VBS_test\", \"SBS_test\", \"Accuracy_test\", \"Score_test\", \"Gap_test\", \"VBS_train\", \"SBS_train\",\"Accuracy_train\", \"Score_train\", \"Gap_train\"])\n",
    "results.to_csv(\"MLP_results.csv\", index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_modelID_1_weights_balanced_optimizer_Adam_learningrate_0.0003_l1Norm_0.0001 calculated, 2/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0 calculated, 3/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0.0001 calculated, 4/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0 calculated, 5/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0.0001 calculated, 6/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_Adam_learningrate_1_l1Norm_0 calculated, 7/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_Adam_learningrate_1_l1Norm_0.0001 calculated, 8/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_LBFGS_learningrate_0.0003_l1Norm_0 calculated, 9/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_LBFGS_learningrate_0.0003_l1Norm_0.0001 calculated, 10/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_LBFGS_learningrate_0.01_l1Norm_0 calculated, 11/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_LBFGS_learningrate_0.01_l1Norm_0.0001 calculated, 12/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_LBFGS_learningrate_0.1_l1Norm_0 calculated, 13/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_LBFGS_learningrate_0.1_l1Norm_0.0001 calculated, 14/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_LBFGS_learningrate_1_l1Norm_0 calculated, 15/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_LBFGS_learningrate_1_l1Norm_0.0001 calculated, 16/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_SGD_learningrate_0.0003_l1Norm_0 calculated, 17/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_SGD_learningrate_0.0003_l1Norm_0.0001 calculated, 18/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0 calculated, 19/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.0001 calculated, 20/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_SGD_learningrate_0.1_l1Norm_0 calculated, 21/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_SGD_learningrate_0.1_l1Norm_0.0001 calculated, 22/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_SGD_learningrate_1_l1Norm_0 calculated, 23/192\n",
      "MLP_modelID_1_weights_balanced_optimizer_SGD_learningrate_1_l1Norm_0.0001 calculated, 24/192\n",
      "MLP_modelID_1_weights_normal_optimizer_Adam_learningrate_0.0003_l1Norm_0 calculated, 25/192\n",
      "MLP_modelID_1_weights_normal_optimizer_Adam_learningrate_0.0003_l1Norm_0.0001 calculated, 26/192\n",
      "MLP_modelID_1_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0 calculated, 27/192\n",
      "MLP_modelID_1_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0.0001 calculated, 28/192\n",
      "MLP_modelID_1_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0 calculated, 29/192\n",
      "MLP_modelID_1_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0.0001 calculated, 30/192\n",
      "MLP_modelID_1_weights_normal_optimizer_Adam_learningrate_1_l1Norm_0 calculated, 31/192\n",
      "MLP_modelID_1_weights_normal_optimizer_Adam_learningrate_1_l1Norm_0.0001 calculated, 32/192\n",
      "MLP_modelID_1_weights_normal_optimizer_LBFGS_learningrate_0.0003_l1Norm_0 calculated, 33/192\n",
      "MLP_modelID_1_weights_normal_optimizer_LBFGS_learningrate_0.0003_l1Norm_0.0001 calculated, 34/192\n",
      "MLP_modelID_1_weights_normal_optimizer_LBFGS_learningrate_0.01_l1Norm_0 calculated, 35/192\n",
      "MLP_modelID_1_weights_normal_optimizer_LBFGS_learningrate_0.01_l1Norm_0.0001 calculated, 36/192\n",
      "MLP_modelID_1_weights_normal_optimizer_LBFGS_learningrate_0.1_l1Norm_0 calculated, 37/192\n",
      "MLP_modelID_1_weights_normal_optimizer_LBFGS_learningrate_0.1_l1Norm_0.0001 calculated, 38/192\n",
      "MLP_modelID_1_weights_normal_optimizer_LBFGS_learningrate_1_l1Norm_0 calculated, 39/192\n",
      "MLP_modelID_1_weights_normal_optimizer_LBFGS_learningrate_1_l1Norm_0.0001 calculated, 40/192\n",
      "MLP_modelID_1_weights_normal_optimizer_SGD_learningrate_0.0003_l1Norm_0 calculated, 41/192\n",
      "MLP_modelID_1_weights_normal_optimizer_SGD_learningrate_0.0003_l1Norm_0.0001 calculated, 42/192\n",
      "MLP_modelID_1_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0 calculated, 43/192\n",
      "MLP_modelID_1_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0.0001 calculated, 44/192\n",
      "MLP_modelID_1_weights_normal_optimizer_SGD_learningrate_0.1_l1Norm_0 calculated, 45/192\n",
      "MLP_modelID_1_weights_normal_optimizer_SGD_learningrate_0.1_l1Norm_0.0001 calculated, 46/192\n",
      "MLP_modelID_1_weights_normal_optimizer_SGD_learningrate_1_l1Norm_0 calculated, 47/192\n",
      "MLP_modelID_1_weights_normal_optimizer_SGD_learningrate_1_l1Norm_0.0001 calculated, 48/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_Adam_learningrate_0.0003_l1Norm_0 calculated, 49/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_Adam_learningrate_0.0003_l1Norm_0.0001 calculated, 50/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0 calculated, 51/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0.0001 calculated, 52/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0 calculated, 53/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0.0001 calculated, 54/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_Adam_learningrate_1_l1Norm_0 calculated, 55/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_Adam_learningrate_1_l1Norm_0.0001 calculated, 56/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_LBFGS_learningrate_0.0003_l1Norm_0 calculated, 57/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_LBFGS_learningrate_0.0003_l1Norm_0.0001 calculated, 58/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_LBFGS_learningrate_0.01_l1Norm_0 calculated, 59/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_LBFGS_learningrate_0.01_l1Norm_0.0001 calculated, 60/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_LBFGS_learningrate_0.1_l1Norm_0 calculated, 61/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_LBFGS_learningrate_0.1_l1Norm_0.0001 calculated, 62/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_LBFGS_learningrate_1_l1Norm_0 calculated, 63/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_LBFGS_learningrate_1_l1Norm_0.0001 calculated, 64/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_0.0003_l1Norm_0 calculated, 65/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_0.0003_l1Norm_0.0001 calculated, 66/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0 calculated, 67/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.0001 calculated, 68/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_0.1_l1Norm_0 calculated, 69/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_0.1_l1Norm_0.0001 calculated, 70/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_1_l1Norm_0 calculated, 71/192\n",
      "MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_1_l1Norm_0.0001 calculated, 72/192\n",
      "MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_0.0003_l1Norm_0 calculated, 73/192\n",
      "MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_0.0003_l1Norm_0.0001 calculated, 74/192\n",
      "MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0 calculated, 75/192\n",
      "MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0.0001 calculated, 76/192\n",
      "MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0 calculated, 77/192\n",
      "MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0.0001 calculated, 78/192\n",
      "MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_1_l1Norm_0 calculated, 79/192\n",
      "MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_1_l1Norm_0.0001 calculated, 80/192\n",
      "MLP_modelID_2_weights_normal_optimizer_LBFGS_learningrate_0.0003_l1Norm_0 calculated, 81/192\n",
      "MLP_modelID_2_weights_normal_optimizer_LBFGS_learningrate_0.0003_l1Norm_0.0001 calculated, 82/192\n",
      "MLP_modelID_2_weights_normal_optimizer_LBFGS_learningrate_0.01_l1Norm_0 calculated, 83/192\n",
      "MLP_modelID_2_weights_normal_optimizer_LBFGS_learningrate_0.01_l1Norm_0.0001 calculated, 84/192\n",
      "MLP_modelID_2_weights_normal_optimizer_LBFGS_learningrate_0.1_l1Norm_0 calculated, 85/192\n",
      "MLP_modelID_2_weights_normal_optimizer_LBFGS_learningrate_0.1_l1Norm_0.0001 calculated, 86/192\n",
      "MLP_modelID_2_weights_normal_optimizer_LBFGS_learningrate_1_l1Norm_0 calculated, 87/192\n",
      "MLP_modelID_2_weights_normal_optimizer_LBFGS_learningrate_1_l1Norm_0.0001 calculated, 88/192\n",
      "MLP_modelID_2_weights_normal_optimizer_SGD_learningrate_0.0003_l1Norm_0 calculated, 89/192\n",
      "MLP_modelID_2_weights_normal_optimizer_SGD_learningrate_0.0003_l1Norm_0.0001 calculated, 90/192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_modelID_2_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0 calculated, 91/192\n",
      "MLP_modelID_2_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0.0001 calculated, 92/192\n",
      "MLP_modelID_2_weights_normal_optimizer_SGD_learningrate_0.1_l1Norm_0 calculated, 93/192\n",
      "MLP_modelID_2_weights_normal_optimizer_SGD_learningrate_0.1_l1Norm_0.0001 calculated, 94/192\n",
      "MLP_modelID_2_weights_normal_optimizer_SGD_learningrate_1_l1Norm_0 calculated, 95/192\n",
      "MLP_modelID_2_weights_normal_optimizer_SGD_learningrate_1_l1Norm_0.0001 calculated, 96/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_Adam_learningrate_0.0003_l1Norm_0 calculated, 97/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_Adam_learningrate_0.0003_l1Norm_0.0001 calculated, 98/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0 calculated, 99/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0.0001 calculated, 100/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0 calculated, 101/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0.0001 calculated, 102/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_Adam_learningrate_1_l1Norm_0 calculated, 103/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_Adam_learningrate_1_l1Norm_0.0001 calculated, 104/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_LBFGS_learningrate_0.0003_l1Norm_0 calculated, 105/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_LBFGS_learningrate_0.0003_l1Norm_0.0001 calculated, 106/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_LBFGS_learningrate_0.01_l1Norm_0 calculated, 107/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_LBFGS_learningrate_0.01_l1Norm_0.0001 calculated, 108/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_LBFGS_learningrate_0.1_l1Norm_0 calculated, 109/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_LBFGS_learningrate_0.1_l1Norm_0.0001 calculated, 110/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_LBFGS_learningrate_1_l1Norm_0 calculated, 111/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_LBFGS_learningrate_1_l1Norm_0.0001 calculated, 112/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_SGD_learningrate_0.0003_l1Norm_0 calculated, 113/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_SGD_learningrate_0.0003_l1Norm_0.0001 calculated, 114/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0 calculated, 115/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.0001 calculated, 116/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_SGD_learningrate_0.1_l1Norm_0 calculated, 117/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_SGD_learningrate_0.1_l1Norm_0.0001 calculated, 118/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_SGD_learningrate_1_l1Norm_0 calculated, 119/192\n",
      "MLP_modelID_3_weights_balanced_optimizer_SGD_learningrate_1_l1Norm_0.0001 calculated, 120/192\n",
      "MLP_modelID_3_weights_normal_optimizer_Adam_learningrate_0.0003_l1Norm_0 calculated, 121/192\n",
      "MLP_modelID_3_weights_normal_optimizer_Adam_learningrate_0.0003_l1Norm_0.0001 calculated, 122/192\n",
      "MLP_modelID_3_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0 calculated, 123/192\n",
      "MLP_modelID_3_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0.0001 calculated, 124/192\n",
      "MLP_modelID_3_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0 calculated, 125/192\n",
      "MLP_modelID_3_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0.0001 calculated, 126/192\n"
     ]
    }
   ],
   "source": [
    "#Again, a detailed description of the calculation steps independent from the model is given in the first cell\n",
    "#Grid Search for MLP\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#Function to convert labels to integers and back\n",
    "def label_to_int(label):\n",
    "    for index in label.index:\n",
    "        match label.at[index, \"algorithm\"]:\n",
    "            case \"greedy\": label.at[index, \"algorithm\"] = 0\n",
    "            case \"dynamic_programming\": label.at[index, \"algorithm\"] = 1\n",
    "            case \"branch_and_bound\": label.at[index, \"algorithm\"] = 2\n",
    "            case _: pass \n",
    "\n",
    "def int_to_label(label):\n",
    "    for index in label.index:\n",
    "        match label.at[index, \"algorithm\"]:\n",
    "            case 0: label.at[index, \"algorithm\"] = \"greedy\"\n",
    "            case 1: label.at[index, \"algorithm\"] = \"dynamic_programming\"\n",
    "            case 2: label.at[index, \"algorithm\"] = \"branch_and_bound\"\n",
    "            case _: pass\n",
    "\n",
    "#Function to determine the average score given the predictions vector\n",
    "#Given the scores and the predictions as dataframe with an index and a single column \"algorithm\"\n",
    "def calculate_score(scores, predictions):\n",
    "    for index in predictions.index:\n",
    "        algorithm = predictions.at[index, \"algorithm\"]\n",
    "        predictions.at[index, \"score\"] = scores.at[index, algorithm]\n",
    "    return predictions[\"score\"].mean()\n",
    "\n",
    "#Four functions that define the four neural network structures used for the first generation of models. Only the layers and dropout changes\n",
    "#Function that returns a neural network with id 1\n",
    "def create_model_1(l2_reg=0):\n",
    "    #Create a class for the model\n",
    "    class Model_Small_Dropout(nn.Module):\n",
    "        #Define the initialization\n",
    "        def __init__(self, l2_reg=0):\n",
    "            super(Model_Small_Dropout, self).__init__()\n",
    "            \n",
    "            #Defining the different layers and parameters of the model\n",
    "            self.fc1 = nn.Linear(2002, 500) #Fully connected\n",
    "            self.bn1 = nn.BatchNorm1d(500) #BatchNorm\n",
    "            self.dropout1 = nn.Dropout(p=0.5) #Dropout\n",
    "            self.fc2 = nn.Linear(500, 3) #Fully Connected\n",
    "            self.relu = nn.ReLU() #Activation\n",
    "            self.softmax = nn.Softmax(dim=1) #Softmax for the final layer\n",
    "            self.l2_reg = l2_reg\n",
    "\n",
    "        #Definition of the forward pass\n",
    "        def forward(self, x):\n",
    "            #Sequence of layers defined in the init function\n",
    "            x = self.fc1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "    #When the function is called, it returns the model\n",
    "    return Model_Small_Dropout(l2_reg=l2_reg)\n",
    "    \n",
    "#Function that returns a neural network with id 2\n",
    "def create_model_2(l2_reg=0):\n",
    "    #This class of MLP models consists of the same structure as model 1 but does not implement dropout\n",
    "    class Model_Small(nn.Module):\n",
    "        def __init__(self, l2_reg=0):\n",
    "            super(Model_Small, self).__init__()\n",
    "            self.fc1 = nn.Linear(2002, 500)\n",
    "            self.bn1 = nn.BatchNorm1d(500)\n",
    "            self.fc2 = nn.Linear(500, 3)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.l2_reg = l2_reg\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "    return Model_Small(l2_reg=l2_reg)\n",
    "\n",
    "#Function that returns a neural network with id 3\n",
    "def create_model_3(l2_reg=0):\n",
    "    class Model_Large_Dropout(nn.Module):\n",
    "        def __init__(self, l2_reg=0):\n",
    "            super(Model_Large_Dropout, self).__init__()\n",
    "            #Compared to the first two models, this one implements more layers and thus increases the complexity\n",
    "            self.fc1 = nn.Linear(2002, 1000) \n",
    "            self.bn1 = nn.BatchNorm1d(1000)\n",
    "            self.dropout1 = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(1000, 500)\n",
    "            self.bn2 = nn.BatchNorm1d(500)\n",
    "            self.dropout2 = nn.Dropout(p=0.5)\n",
    "            self.fc3 = nn.Linear(500, 100)\n",
    "            self.bn3 = nn.BatchNorm1d(100)\n",
    "            self.dropout3 = nn.Dropout(p=0.5)\n",
    "            self.fc4 = nn.Linear(100, 3)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.l2_reg = l2_reg\n",
    "\n",
    "        #Structure of the forward pass is similar to the first models\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = self.fc4(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "    return Model_Large_Dropout(l2_reg=l2_reg)\n",
    "\n",
    "#Function that returns a neural network with id 4\n",
    "def create_model_4(l2_reg=0):\n",
    "    class Model_Large(nn.Module):\n",
    "        #Identical structure to model 3, this time without dropout\n",
    "        def __init__(self, l2_reg=0):\n",
    "            super(Model_Large, self).__init__()\n",
    "            self.fc1 = nn.Linear(2002, 1000)\n",
    "            self.bn1 = nn.BatchNorm1d(1000)\n",
    "            self.fc2 = nn.Linear(1000, 500)\n",
    "            self.bn2 = nn.BatchNorm1d(500)\n",
    "            self.fc3 = nn.Linear(500, 100)\n",
    "            self.bn3 = nn.BatchNorm1d(100)\n",
    "            self.fc4 = nn.Linear(100, 3)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.l2_reg = l2_reg\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc4(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "    return Model_Large(l2_reg=l2_reg)\n",
    "\n",
    "#Definition of the closure function to train with the optimizer LBFGS\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train.squeeze(1))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "#Initialize counter\n",
    "counter = 1\n",
    "\n",
    "#For loops to test every combination of parameters\n",
    "for model_id in [1, 2, 3, 4]:\n",
    "    for weights_name in [\"balanced\", \"normal\"]:\n",
    "        for optimizer_name in [\"Adam\", \"LBFGS\", \"SGD\"]:\n",
    "            for learning_rate in [0.0003, 0.01, 0.1, 1]:\n",
    "                for l1_norm in [0, 0.0001]:\n",
    "                    \n",
    "                    #Set the epochs and train_test_split seed equal across the instances\n",
    "                    epochs = 200\n",
    "                    seed = 1\n",
    "                    \n",
    "                    #Unique index for each model\n",
    "                    model_description = \"MLP_modelID_\"+str(model_id)+\"_weights_\"+str(weights_name)+\"_optimizer_\"+str(optimizer_name)+\"_learningrate_\"+str(learning_rate)+\"_l1Norm_\"+str(l1_norm)\n",
    "                    \n",
    "                    #Load the necessary data\n",
    "                    result_path = \"MLP_results.csv\"\n",
    "                    scaled_instances = \"instances.csv\"\n",
    "                    scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "                    scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "                    data = pd.DataFrame(pd.read_csv(scaled_instances, header=0, index_col=0))\n",
    "                    results = pd.DataFrame(pd.read_csv(result_path, header=0, index_col=0))\n",
    "\n",
    "                    #Write the parameters to the result file\n",
    "                    results.at[model_description, \"epochs\"] = epochs\n",
    "                    results.at[model_description, \"weights\"] = weights_name\n",
    "                    results.at[model_description, \"optimizer\"] = optimizer_name\n",
    "                    results.at[model_description, \"learning_rate\"] = learning_rate\n",
    "                    results.at[model_description, \"L1_norm\"] = l1_norm\n",
    "                    results.at[model_description, \"criterion\"] = \"CrossEntropyLoss\"\n",
    "\n",
    "                    #Split the data into train set and test set\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(data, scores, test_size=0.2, random_state=seed)\n",
    "\n",
    "                    #Calculation of SBS and VBS for the test set\n",
    "                    test_scores = pd.DataFrame(y_test)\n",
    "                    test_scores[\"best_solver\"] = test_scores.min(axis=1)\n",
    "\n",
    "                    average_greedy, average_dynamic_programming, average_branch_and_bound, test_average_best_solver = test_scores.mean(axis=0)\n",
    "                    test_average_single_best_solver = min(average_greedy, average_dynamic_programming, average_branch_and_bound)\n",
    "\n",
    "                    results.at[model_description, \"VBS_test\"] = test_average_best_solver\n",
    "                    results.at[model_description, \"SBS_test\"] = test_average_single_best_solver\n",
    "\n",
    "                    #Calculation of SBS and VBS for the train set\n",
    "                    train_scores = pd.DataFrame(y_train)\n",
    "                    train_scores[\"best_solver\"] = train_scores.min(axis=1)\n",
    "\n",
    "                    average_greedy, average_dynamic_programming, average_branch_and_bound, train_average_best_solver = train_scores.mean(axis=0)\n",
    "                    train_average_single_best_solver = min(average_greedy, average_dynamic_programming, average_branch_and_bound)\n",
    "\n",
    "                    results.at[model_description, \"VBS_train\"] = train_average_best_solver\n",
    "                    results.at[model_description, \"SBS_train\"] = train_average_single_best_solver\n",
    "\n",
    "                    #Prepare the labels\n",
    "                    y_train = pd.DataFrame(y_train.idxmin(axis=1), columns=[\"algorithm\"])\n",
    "                    y_test = pd.DataFrame(y_test.idxmin(axis=1), columns=[\"algorithm\"])\n",
    "                    y_train_accuracy = y_train.copy()\n",
    "                    y_test_accuracy = y_test.copy()\n",
    "\n",
    "                    label_to_int(y_train)\n",
    "                    y_train = y_train.astype(int)\n",
    "                    label_to_int(y_test)\n",
    "                    y_test = y_test.astype(int)\n",
    "\n",
    "                    #Convert labels to integers\n",
    "                    label_to_int(y_train)\n",
    "                    label_to_int(y_test)\n",
    "\n",
    "\n",
    "                    y_train_indices = y_train.index\n",
    "                    y_test_indices = y_test.index\n",
    "\n",
    "                    #---------------------------------------------------------------------------------------------------------------\n",
    "                    #MODEL HERE\n",
    "                    \n",
    "                    #Convert the data to torch tensors with the data types float (data) or long (labels)\n",
    "                    X_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "                    y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "                    X_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "                    y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "                    #Create the specific model and write the according description to the DataFrame results\n",
    "                    match model_id:\n",
    "                        case 1:\n",
    "                            results.at[model_description, \"model\"] = \"Small_Dropout\"\n",
    "                            model = create_model_1()\n",
    "                        case 2:\n",
    "                            results.at[model_description, \"model\"] = \"Small\"\n",
    "                            model = create_model_2()\n",
    "                        case 3:\n",
    "                            results.at[model_description, \"model\"] = \"Large_Dropout\"\n",
    "                            model = create_model_3()\n",
    "                        case 4:\n",
    "                            results.at[model_description, \"model\"] = \"Large\"\n",
    "                            model = create_model_4()\n",
    "\n",
    "                    #Define the weights\n",
    "                    match weights_name:\n",
    "                        case \"normal\":\n",
    "                            #Equal weights\n",
    "                            weights = [1., 1., 1.]\n",
    "                        case \"balanced\":\n",
    "                            #Balanced with class_weight(i) = #instances/(#classes*#instances_in_class(i))\n",
    "                            weights = [0.5678, 1.1445, 2.7385]\n",
    "\n",
    "                    #Define the loss function and optimizer                    \n",
    "                    criterion = nn.CrossEntropyLoss(torch.tensor(weights))\n",
    "                    match optimizer_name:\n",
    "                        case \"Adam\":\n",
    "                            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                        case \"LBFGS\":\n",
    "                            optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)\n",
    "                        case \"SGD\": \n",
    "                            #SGD is implemented with a momentum of 0.9\n",
    "                            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "                    #Train the model\n",
    "                    if optimizer_name == \"LBFGS\":\n",
    "                        #LBFGS is trained with the defined Closure\n",
    "                        for i in range(epochs): #Iterate over epochs \n",
    "                            optimizer.step(closure)\n",
    "                    else:\n",
    "                        #Training of Adam and SGD\n",
    "                        for epoch in range(epochs): #Iterate over epochs                          \n",
    "                            #Reset the gradients to zero\n",
    "                            optimizer.zero_grad()\n",
    "                            \n",
    "                            #Calculate the predictions of the model\n",
    "                            outputs = model(X_train)\n",
    "                            \n",
    "                            #Calculate the loss\n",
    "                            loss = criterion(outputs, y_train.squeeze(1))\n",
    "\n",
    "                            #Add activity regularization (l1 norm)\n",
    "                            l1_lambda = l1_norm #Parameter for impact of the regularization\n",
    "                            l1_reg = torch.tensor(0.) #Initialize the value\n",
    "                            #Add the value of each parameter to l1_reg \n",
    "                            for name, parameter in model.named_parameters():\n",
    "                                l1_reg += torch.norm(parameter, 1)\n",
    "                            #Add the scaled values to the overall loss\n",
    "                            loss += l1_lambda * l1_reg\n",
    "\n",
    "                            #Compute gradients of the loss\n",
    "                            loss.backward()\n",
    "                            \n",
    "                            #Update parameters of the model\n",
    "                            optimizer.step()\n",
    "\n",
    "                    #Evaluate the model on the train set\n",
    "                    with torch.no_grad(): #Do not change the gradients\n",
    "                        \n",
    "                        #Calculate the predictions\n",
    "                        outputs = model(X_train)\n",
    "                        \n",
    "                        #Reduce the output values to the model with the highest score\n",
    "                        predicted = torch.argmax(outputs, dim=1) \n",
    "                    \n",
    "                    #Define the predictions as DataFrame to calculate the score\n",
    "                    predictions_train = pd.DataFrame(predicted, columns=[\"algorithm\"], index=y_train_indices)\n",
    "\n",
    "                    #Evaluate the model on the test set\n",
    "                    with torch.no_grad():\n",
    "                        \n",
    "                        #Get predictions for each instance\n",
    "                        outputs = model(X_test)\n",
    "                        predicted = torch.argmax(outputs, dim=1) \n",
    "\n",
    "                    #Define the predictions as DataFrame to calculate the score\n",
    "                    predictions_test = pd.DataFrame(predicted, columns=[\"algorithm\"], index=y_test_indices)\n",
    "                    #---------------------------------------------------------------------------------------------------------------\n",
    "                    \n",
    "                    #Calculate the score of the predictions on the train set\n",
    "                    int_to_label(predictions_train)\n",
    "\n",
    "                    scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "                    scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "\n",
    "                    score_train = calculate_score(scores, predictions_train)\n",
    "                    results.at[model_description, \"Score_train\"] = score_train\n",
    "\n",
    "                    #Calculate the score of the predictions on the test set\n",
    "                    int_to_label(predictions_test)\n",
    "\n",
    "                    scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "                    scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "\n",
    "                    score_test = calculate_score(scores, predictions_test)\n",
    "                    results.at[model_description, \"Score_test\"] = score_test\n",
    "\n",
    "                    #Calculate the accuracy of the predictions\n",
    "                    predictions_train = pd.DataFrame(predictions_train)\n",
    "                    predictions_test = pd.DataFrame(predictions_test)\n",
    "\n",
    "                    accuracy_train = accuracy_score(y_train_accuracy, predictions_train[\"algorithm\"]).round(4)\n",
    "                    accuracy_test = accuracy_score(y_test_accuracy, predictions_test[\"algorithm\"]).round(4)\n",
    "                    results.at[model_description, \"Accuracy_train\"] = accuracy_train\n",
    "                    results.at[model_description, \"Accuracy_test\"] = accuracy_test\n",
    "\n",
    "                    #Calculate the gaps\n",
    "                    VBS_train = train_average_best_solver\n",
    "                    SBS_train = train_average_single_best_solver\n",
    "                    AAS_train = score_train\n",
    "                    VBS_test = test_average_best_solver\n",
    "                    SBS_test = test_average_single_best_solver\n",
    "                    AAS_test = score_test\n",
    "\n",
    "                    Gap_train = SBS_train - VBS_train\n",
    "                    Gap_test = SBS_test - VBS_test\n",
    "                    Score_train = AAS_train - VBS_train\n",
    "                    Score_test = AAS_test - VBS_test\n",
    "\n",
    "                    GapClose_train = 1-(Score_train/Gap_train)\n",
    "                    GapClose_test = 1-(Score_test/Gap_test)\n",
    "                    results.at[model_description, \"Gap_train\"] = GapClose_train\n",
    "                    results.at[model_description, \"Gap_test\"] = GapClose_test\n",
    "\n",
    "                    #Overwrite the previous results file\n",
    "                    results.to_csv(result_path, index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")\n",
    "                    \n",
    "                    #Output to keep track of progress and increase the counter\n",
    "                    print(str(model_description)+\" calculated, \"+str(counter)+\"/192\")\n",
    "                    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2faac2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       epochs  learning_rate    L1_norm      VBS_test     SBS_test  \\\n",
      "count   192.0     192.000000  192.00000  1.920000e+02   192.000000   \n",
      "mean    200.0       0.277575    0.00005  1.910834e+03  5106.399455   \n",
      "std       0.0       0.419995    0.00005  2.279681e-13     0.000000   \n",
      "min     200.0       0.000300    0.00000  1.910834e+03  5106.399455   \n",
      "25%     200.0       0.007575    0.00000  1.910834e+03  5106.399455   \n",
      "50%     200.0       0.055000    0.00005  1.910834e+03  5106.399455   \n",
      "75%     200.0       0.325000    0.00010  1.910834e+03  5106.399455   \n",
      "max     200.0       1.000000    0.00010  1.910834e+03  5106.399455   \n",
      "\n",
      "       Accuracy_test    Score_test    Gap_test     VBS_train     SBS_train  \\\n",
      "count     192.000000    192.000000  192.000000  1.920000e+02  1.920000e+02   \n",
      "mean        0.550481   9108.116141   -1.252272  1.936892e+03  4.801997e+03   \n",
      "std         0.076873   6272.587019    1.962903  2.279681e-13  9.118725e-13   \n",
      "min         0.129300   5105.056378   -9.598629  1.936892e+03  4.801997e+03   \n",
      "25%         0.539425   5106.399455   -2.247055  1.936892e+03  4.801997e+03   \n",
      "50%         0.586300   5106.399455    0.000000  1.936892e+03  4.801997e+03   \n",
      "75%         0.586300  12287.010453    0.000000  1.936892e+03  4.801997e+03   \n",
      "max         0.606700  35779.448924    0.000420  1.936892e+03  4.801997e+03   \n",
      "\n",
      "       Accuracy_train   Score_train   Gap_train  \n",
      "count      192.000000    192.000000  192.000000  \n",
      "mean         0.666831   6018.708148   -0.424666  \n",
      "std          0.184603   5873.364523    2.049965  \n",
      "min          0.119800   1936.891656  -10.662625  \n",
      "25%          0.587200   4801.996742   -0.004541  \n",
      "50%          0.587200   4801.996742    0.000000  \n",
      "75%          0.833425   4815.007858    0.000000  \n",
      "max          1.000000  35351.537984    1.000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>epochs</th>\n",
       "      <th>weights</th>\n",
       "      <th>criterion</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>L1_norm</th>\n",
       "      <th>VBS_test</th>\n",
       "      <th>SBS_test</th>\n",
       "      <th>Accuracy_test</th>\n",
       "      <th>Score_test</th>\n",
       "      <th>Gap_test</th>\n",
       "      <th>VBS_train</th>\n",
       "      <th>SBS_train</th>\n",
       "      <th>Accuracy_train</th>\n",
       "      <th>Score_train</th>\n",
       "      <th>Gap_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_3_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0.0001</th>\n",
       "      <td>Large_Dropout</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5868</td>\n",
       "      <td>5105.056378</td>\n",
       "      <td>4.202941e-04</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5873</td>\n",
       "      <td>4801.661205</td>\n",
       "      <td>0.000117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_3_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0</th>\n",
       "      <td>Large_Dropout</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5865</td>\n",
       "      <td>5106.398092</td>\n",
       "      <td>4.265369e-07</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5873</td>\n",
       "      <td>4804.008512</td>\n",
       "      <td>-0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_1_l1Norm_0</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_4_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.0001</th>\n",
       "      <td>Large</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_1_l1Norm_0.0001</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_1_l1Norm_0</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_4_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0</th>\n",
       "      <td>Large</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_4_weights_balanced_optimizer_SGD_learningrate_0.0003_l1Norm_0.0001</th>\n",
       "      <td>Large</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_4_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0</th>\n",
       "      <td>Large</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_4_weights_balanced_optimizer_SGD_learningrate_0.0003_l1Norm_0</th>\n",
       "      <td>Large</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_3_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0</th>\n",
       "      <td>Large_Dropout</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_1_l1Norm_0.0001</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.0001</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_balanced_optimizer_Adam_learningrate_1_l1Norm_0</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            model  epochs  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  Large_Dropout   200.0   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  Large_Dropout   200.0   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          Small   200.0   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          Large   200.0   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          Small   200.0   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          Small   200.0   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          Large   200.0   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          Small   200.0   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          Large   200.0   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...          Large   200.0   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          Large   200.0   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  Large_Dropout   200.0   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          Small   200.0   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          Small   200.0   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...          Small   200.0   \n",
       "\n",
       "                                                     weights  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...    normal   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...    normal   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...    normal   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...    normal   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...    normal   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  balanced   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...    normal   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  balanced   \n",
       "\n",
       "                                                           criterion  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  CrossEntropyLoss   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  CrossEntropyLoss   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  CrossEntropyLoss   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "\n",
       "                                                   optimizer  learning_rate  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...       SGD         0.0100   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...       SGD         0.0100   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...       SGD         1.0000   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...       SGD         0.0100   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...      Adam         1.0000   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...      Adam         1.0000   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...       SGD         0.0100   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...      Adam         0.1000   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...       SGD         0.0003   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...      Adam         0.1000   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...       SGD         0.0003   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...      Adam         0.1000   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...       SGD         1.0000   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...       SGD         0.0100   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...      Adam         1.0000   \n",
       "\n",
       "                                                    L1_norm     VBS_test  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...   0.0001  1910.833772   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...   0.0000  1910.833772   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...   0.0000  1910.833772   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...   0.0001  1910.833772   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...   0.0001  1910.833772   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...   0.0000  1910.833772   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...   0.0000  1910.833772   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...   0.0000  1910.833772   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...   0.0001  1910.833772   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...   0.0000  1910.833772   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...   0.0000  1910.833772   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...   0.0000  1910.833772   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...   0.0001  1910.833772   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...   0.0001  1910.833772   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...   0.0000  1910.833772   \n",
       "\n",
       "                                                       SBS_test  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  5106.399455   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  5106.399455   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  5106.399455   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  5106.399455   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  5106.399455   \n",
       "\n",
       "                                                    Accuracy_test  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...         0.5868   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...         0.5865   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...         0.5863   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...         0.5863   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...         0.5863   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...         0.5863   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "\n",
       "                                                     Score_test      Gap_test  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  5105.056378  4.202941e-04   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  5106.398092  4.265369e-07   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000e+00   \n",
       "\n",
       "                                                      VBS_train    SBS_train  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  1936.891656  4801.996742   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  1936.891656  4801.996742   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  1936.891656  4801.996742   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "\n",
       "                                                    Accuracy_train  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...          0.5873   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...          0.5873   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          0.5872   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          0.5872   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          0.5872   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...          0.5872   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "\n",
       "                                                    Score_train  Gap_train  \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  4801.661205   0.000117  \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  4804.008512  -0.000702  \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  4801.996742   0.000000  \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  4801.996742   0.000000  \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Display the results (A more detailed version in detail ResultDiscussion.ipynb)\n",
    "table = pd.DataFrame(pd.read_csv(\"MLP_results.csv\", header=0, index_col=0))\n",
    "print(table.describe())\n",
    "table = table.sort_values([\"Gap_test\"], ascending=False)\n",
    "table = table[:15]\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af83a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second iteration of MLP models, more complex models\n",
    "#This code would clear the results file\n",
    "'''results = pd.DataFrame(columns=[\"model\", \"dropout\", \"activation\", \"epochs\", \"weights\", \"criterion\", \"optimizer\", \"learning_rate\", \"L1_norm\", \"L2_norm\", \"VBS_test\", \"SBS_test\", \"Accuracy_test\", \"Score_test\", \"Gap_test\", \"VBS_train\", \"SBS_train\",\"Accuracy_train\", \"Score_train\", \"Gap_train\"])\n",
    "results.to_csv(\"MLP_results2.csv\", index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82129720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_modelID_5_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 1/192\n",
      "MLP_modelID_5_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 2/192\n",
      "MLP_modelID_5_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 3/192\n",
      "MLP_modelID_5_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 4/192\n",
      "MLP_modelID_6_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 5/192\n",
      "MLP_modelID_6_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 6/192\n",
      "MLP_modelID_6_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 7/192\n",
      "MLP_modelID_6_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 8/192\n",
      "MLP_modelID_7_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 9/192\n",
      "MLP_modelID_7_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 10/192\n",
      "MLP_modelID_7_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 11/192\n",
      "MLP_modelID_7_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 12/192\n",
      "MLP_modelID_8_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 13/192\n",
      "MLP_modelID_8_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 14/192\n",
      "MLP_modelID_8_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 15/192\n",
      "MLP_modelID_8_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0 calculated, 16/192\n",
      "MLP_modelID_5_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 17/192\n",
      "MLP_modelID_5_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 18/192\n",
      "MLP_modelID_5_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 19/192\n",
      "MLP_modelID_5_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 20/192\n",
      "MLP_modelID_6_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 21/192\n",
      "MLP_modelID_6_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 22/192\n",
      "MLP_modelID_6_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 23/192\n",
      "MLP_modelID_6_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 24/192\n",
      "MLP_modelID_7_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 25/192\n",
      "MLP_modelID_7_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 26/192\n",
      "MLP_modelID_7_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 27/192\n",
      "MLP_modelID_7_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 28/192\n",
      "MLP_modelID_8_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 29/192\n",
      "MLP_modelID_8_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 30/192\n",
      "MLP_modelID_8_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 31/192\n",
      "MLP_modelID_8_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.0001 calculated, 32/192\n",
      "MLP_modelID_5_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 33/192\n",
      "MLP_modelID_5_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 34/192\n",
      "MLP_modelID_5_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 35/192\n",
      "MLP_modelID_5_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 36/192\n",
      "MLP_modelID_6_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 37/192\n",
      "MLP_modelID_6_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 38/192\n",
      "MLP_modelID_6_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 39/192\n",
      "MLP_modelID_6_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 40/192\n",
      "MLP_modelID_7_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 41/192\n",
      "MLP_modelID_7_weights_normal_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 42/192\n",
      "MLP_modelID_7_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 43/192\n",
      "MLP_modelID_7_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 44/192\n",
      "MLP_modelID_8_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.01 calculated, 45/192\n"
     ]
    }
   ],
   "source": [
    "#Second Grid Search for MLPs\n",
    "#Most of the code is identical to the first generation. For the documentation, the models were trained in different cells\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#Function to convert labels to integers and back\n",
    "def label_to_int(label):\n",
    "    for index in label.index:\n",
    "        match label.at[index, \"algorithm\"]:\n",
    "            case \"greedy\": label.at[index, \"algorithm\"] = 0\n",
    "            case \"dynamic_programming\": label.at[index, \"algorithm\"] = 1\n",
    "            case \"branch_and_bound\": label.at[index, \"algorithm\"] = 2\n",
    "            case _: pass \n",
    "\n",
    "def int_to_label(label):\n",
    "    for index in label.index:\n",
    "        match label.at[index, \"algorithm\"]:\n",
    "            case 0: label.at[index, \"algorithm\"] = \"greedy\"\n",
    "            case 1: label.at[index, \"algorithm\"] = \"dynamic_programming\"\n",
    "            case 2: label.at[index, \"algorithm\"] = \"branch_and_bound\"\n",
    "            case _: pass\n",
    "\n",
    "#Function to determine the average score given the predictions vector\n",
    "#Given the scores and the predictions as dataframe with an index and a single column \"algorithm\"\n",
    "def calculate_score(scores, predictions):\n",
    "    for index in predictions.index:\n",
    "        algorithm = predictions.at[index, \"algorithm\"]\n",
    "        predictions.at[index, \"score\"] = scores.at[index, algorithm]\n",
    "    return predictions[\"score\"].mean()\n",
    "\n",
    "#Define the deep neural network 5\n",
    "def create_model_5(l2_reg=0.01): #l2_reg is overwritten later\n",
    "    class Model_Very_Large_Dropout(nn.Module): #ReLU with dropout\n",
    "        def __init__(self, l2_reg=0.01):\n",
    "            #All models consists of the same structure, some with dropout and some with Leaky ReLu activation\n",
    "            super(Model_Very_Large_Dropout, self).__init__()\n",
    "            self.fc1 = nn.Linear(2002, 1500)\n",
    "            self.bn1 = nn.BatchNorm1d(1500)\n",
    "            self.dropout1 = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(1500, 1000)\n",
    "            self.bn2 = nn.BatchNorm1d(1000)\n",
    "            self.dropout2 = nn.Dropout(p=0.5)\n",
    "            self.fc3 = nn.Linear(1000, 500)\n",
    "            self.bn3 = nn.BatchNorm1d(500)\n",
    "            self.dropout3 = nn.Dropout(p=0.5)\n",
    "            self.fc4 = nn.Linear(500, 100)\n",
    "            self.bn4 = nn.BatchNorm1d(100)\n",
    "            self.dropout4 = nn.Dropout(p=0.5)\n",
    "            self.fc5 = nn.Linear(100, 50)\n",
    "            self.bn5 = nn.BatchNorm1d(50)\n",
    "            self.dropout5 = nn.Dropout(p=0.5)\n",
    "            self.fc6 = nn.Linear(50, 3)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.l2_reg = l2_reg\n",
    "\n",
    "        #Forward pass, compared to the first generation only the amount of layers is increased\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = self.fc4(x)\n",
    "            x = self.bn4(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout4(x)\n",
    "            x = self.fc5(x)\n",
    "            x = self.bn5(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout5(x)\n",
    "            x = self.fc6(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "    return Model_Very_Large_Dropout(l2_reg=l2_reg)\n",
    "\n",
    "#Define the deep neural network 6\n",
    "def create_model_6(l2_reg=0.01):\n",
    "    class Model_Very_Large(nn.Module): #ReLU, without dropout\n",
    "        def __init__(self, l2_reg=0.01):\n",
    "            super(Model_Very_Large, self).__init__()\n",
    "            self.fc1 = nn.Linear(2002, 1500)\n",
    "            self.bn1 = nn.BatchNorm1d(1500)\n",
    "            self.fc2 = nn.Linear(1500, 1000)\n",
    "            self.bn2 = nn.BatchNorm1d(1000)\n",
    "            self.fc3 = nn.Linear(1000, 500)\n",
    "            self.bn3 = nn.BatchNorm1d(500)\n",
    "            self.fc4 = nn.Linear(500, 100)\n",
    "            self.bn4 = nn.BatchNorm1d(100)\n",
    "            self.fc5 = nn.Linear(100, 50)\n",
    "            self.bn5 = nn.BatchNorm1d(50)\n",
    "            self.fc6 = nn.Linear(50, 3)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.l2_reg = l2_reg\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc4(x)\n",
    "            x = self.bn4(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc5(x)\n",
    "            x = self.bn5(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc6(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "    return Model_Very_Large(l2_reg=l2_reg)\n",
    "\n",
    "#Define the deep neural network 7\n",
    "def create_model_7(l2_reg=0.01):\n",
    "    class Model_Very_Large_Dropout_Leaky(nn.Module): #Leaky ReLU with dropout\n",
    "        def __init__(self, l2_reg=0.01):\n",
    "            super(Model_Very_Large_Dropout_Leaky, self).__init__()\n",
    "            self.fc1 = nn.Linear(2002, 1500)\n",
    "            self.bn1 = nn.BatchNorm1d(1500)\n",
    "            self.dropout1 = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(1500, 1000)\n",
    "            self.bn2 = nn.BatchNorm1d(1000)\n",
    "            self.dropout2 = nn.Dropout(p=0.5)\n",
    "            self.fc3 = nn.Linear(1000, 500)\n",
    "            self.bn3 = nn.BatchNorm1d(500)\n",
    "            self.dropout3 = nn.Dropout(p=0.5)\n",
    "            self.fc4 = nn.Linear(500, 100)\n",
    "            self.bn4 = nn.BatchNorm1d(100)\n",
    "            self.dropout4 = nn.Dropout(p=0.5)\n",
    "            self.fc5 = nn.Linear(100, 50)\n",
    "            self.bn5 = nn.BatchNorm1d(50)\n",
    "            self.dropout5 = nn.Dropout(p=0.5)\n",
    "            self.fc6 = nn.Linear(50, 3)\n",
    "            self.leaky_relu = nn.LeakyReLU()\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.l2_reg = l2_reg\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = self.fc4(x)\n",
    "            x = self.bn4(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.dropout4(x)\n",
    "            x = self.fc5(x)\n",
    "            x = self.bn5(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.dropout5(x)\n",
    "            x = self.fc6(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "    return Model_Very_Large_Dropout_Leaky(l2_reg=l2_reg)\n",
    "\n",
    "#Define the deep neural network 8\n",
    "def create_model_8(l2_reg=0.01):\n",
    "    class Model_Very_Large_Leaky(nn.Module): #Leaky ReLU without dropout\n",
    "        def __init__(self, l2_reg=0.01):\n",
    "            super(Model_Very_Large_Leaky, self).__init__()\n",
    "            self.fc1 = nn.Linear(2002, 1500)\n",
    "            self.bn1 = nn.BatchNorm1d(1500)\n",
    "            self.fc2 = nn.Linear(1500, 1000)\n",
    "            self.bn2 = nn.BatchNorm1d(1000)\n",
    "            self.fc3 = nn.Linear(1000, 500)\n",
    "            self.bn3 = nn.BatchNorm1d(500)\n",
    "            self.fc4 = nn.Linear(500, 100)\n",
    "            self.bn4 = nn.BatchNorm1d(100)\n",
    "            self.fc5 = nn.Linear(100, 50)\n",
    "            self.bn5 = nn.BatchNorm1d(50)\n",
    "            self.fc6 = nn.Linear(50, 3)\n",
    "            self.leaky_relu = nn.LeakyReLU()\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.l2_reg = l2_reg\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.fc3(x)\n",
    "            x = self.bn3(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.fc4(x)\n",
    "            x = self.bn4(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.fc5(x)\n",
    "            x = self.bn5(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.fc6(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "    return Model_Very_Large_Leaky(l2_reg=l2_reg)\n",
    "\n",
    "#Initialize the counter and test every combination of the parameters\n",
    "counter = 1\n",
    "for learning_rate in [0.01, 0.1]:\n",
    "    for l1_norm in [0, 0.001]:\n",
    "        for l2_norm in [0, 0.0001, 0.01]:\n",
    "            for model_id in [5, 6, 7, 8]:\n",
    "                for weights_name in [\"normal\", \"balanced\"]:\n",
    "                    for optimizer_name in [\"SGD\", \"Adam\"]:\n",
    "                        \n",
    "                        #A very large part of the code is identical to the first generation and not described in detail\n",
    "                        epochs = 200\n",
    "                        seed = 1\n",
    "                        model_description = \"MLP_modelID_\"+str(model_id)+\"_weights_\"+str(weights_name)+\"_optimizer_\"+str(optimizer_name)+\"_learningrate_\"+str(learning_rate)+\"_l1Norm_\"+str(l1_norm)+\"_l2Norm_\"+str(l2_norm)\n",
    "                        result_path = \"MLP_results2.csv\"\n",
    "                        scaled_instances = \"instances.csv\"\n",
    "                        \n",
    "                        #Load the data\n",
    "                        scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "                        scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "                        data = pd.DataFrame(pd.read_csv(scaled_instances, header=0, index_col=0))\n",
    "                        results = pd.DataFrame(pd.read_csv(result_path, header=0, index_col=0))\n",
    "\n",
    "                        #Write parameter combination to the results\n",
    "                        results.at[model_description, \"epochs\"] = epochs\n",
    "                        results.at[model_description, \"weights\"] = weights_name\n",
    "                        results.at[model_description, \"optimizer\"] = optimizer_name\n",
    "                        results.at[model_description, \"learning_rate\"] = learning_rate\n",
    "                        results.at[model_description, \"L1_norm\"] = l1_norm\n",
    "                        results.at[model_description, \"L2_norm\"] = l2_norm\n",
    "                        results.at[model_description, \"criterion\"] = \"CrossEntropyLoss\"\n",
    "\n",
    "                        #Split the data into training and testing sets\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(data, scores, test_size=0.2, random_state=seed)\n",
    "\n",
    "                        #Calculation of SBS and VBS for the test set\n",
    "                        test_scores = pd.DataFrame(y_test)\n",
    "                        test_scores[\"best_solver\"] = test_scores.min(axis=1)\n",
    "\n",
    "                        average_greedy, average_dynamic_programming, average_branch_and_bound, test_average_best_solver = test_scores.mean(axis=0)\n",
    "                        test_average_single_best_solver = min(average_greedy, average_dynamic_programming, average_branch_and_bound)\n",
    "\n",
    "                        results.at[model_description, \"VBS_test\"] = test_average_best_solver\n",
    "                        results.at[model_description, \"SBS_test\"] = test_average_single_best_solver\n",
    "\n",
    "                        #Calculation of SBS and VBS for the train set\n",
    "                        train_scores = pd.DataFrame(y_train)\n",
    "                        train_scores[\"best_solver\"] = train_scores.min(axis=1)\n",
    "\n",
    "                        average_greedy, average_dynamic_programming, average_branch_and_bound, train_average_best_solver = train_scores.mean(axis=0)\n",
    "                        train_average_single_best_solver = min(average_greedy, average_dynamic_programming, average_branch_and_bound)\n",
    "\n",
    "                        results.at[model_description, \"VBS_train\"] = train_average_best_solver\n",
    "                        results.at[model_description, \"SBS_train\"] = train_average_single_best_solver\n",
    "\n",
    "                        #Prepare the labels\n",
    "                        y_train = pd.DataFrame(y_train.idxmin(axis=1), columns=[\"algorithm\"])\n",
    "                        y_test = pd.DataFrame(y_test.idxmin(axis=1), columns=[\"algorithm\"])\n",
    "                        y_train_accuracy = y_train.copy()\n",
    "                        y_test_accuracy = y_test.copy()\n",
    "\n",
    "                        label_to_int(y_train)\n",
    "                        y_train = y_train.astype(int)\n",
    "                        label_to_int(y_test)\n",
    "                        y_test = y_test.astype(int)\n",
    "\n",
    "                        #Convert labels to integers\n",
    "                        label_to_int(y_train)\n",
    "                        label_to_int(y_test)\n",
    "\n",
    "                        y_train_indices = y_train.index\n",
    "                        y_test_indices = y_test.index\n",
    "\n",
    "                        #---------------------------------------------------------------------------------------------------------------\n",
    "                        #MODEL HERE\n",
    "                        #Convert to torch tensors\n",
    "                        X_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "                        y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "                        X_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "                        y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "                        #Define the model and write the information to the DataFrame results\n",
    "                        match model_id:\n",
    "                            case 5:\n",
    "                                results.at[model_description, \"model\"] = \"Very_Large_Dropout\"\n",
    "                                results.at[model_description, \"dropout\"] = \"Yes\"\n",
    "                                results.at[model_description, \"activation\"] = \"ReLU\"\n",
    "                                model = create_model_5(l2_norm) #Create with l2 norm\n",
    "                            case 6:\n",
    "                                results.at[model_description, \"model\"] = \"Very_Large\"\n",
    "                                results.at[model_description, \"dropout\"] = \"No\"\n",
    "                                results.at[model_description, \"activation\"] = \"ReLU\"\n",
    "                                model = create_model_6(l2_norm) #Create with l2 norm\n",
    "                            case 7:\n",
    "                                results.at[model_description, \"model\"] = \"Very_Large_Dropout_Leaky\"\n",
    "                                results.at[model_description, \"dropout\"] = \"Yes\"\n",
    "                                results.at[model_description, \"activation\"] = \"Leaky ReLU\"\n",
    "                                model = create_model_7(l2_norm) #Create with l2 norm\n",
    "                            case 8:\n",
    "                                results.at[model_description, \"model\"] = \"Very_Large_Leaky\"\n",
    "                                results.at[model_description, \"dropout\"] = \"No\"\n",
    "                                results.at[model_description, \"activation\"] = \"Leaky ReLU\"\n",
    "                                model = create_model_8(l2_norm) #Create with l2 norm\n",
    "\n",
    "                        #Define the weights\n",
    "                        match weights_name:\n",
    "                            case \"normal\":\n",
    "                                weights = [1., 1., 1.]\n",
    "                            case \"balanced\":\n",
    "                                weights = [0.5678, 1.1445, 2.7385]\n",
    "\n",
    "                        #Define the loss function and optimizer                    \n",
    "                        criterion = nn.CrossEntropyLoss(torch.tensor(weights))\n",
    "                        #This time, without the LBFGS optimizer\n",
    "                        match optimizer_name:\n",
    "                            case \"Adam\":\n",
    "                                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                            case \"SGD\": \n",
    "                                optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "                        #Train the model\n",
    "                        for epoch in range(epochs):                          \n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = model(X_train)\n",
    "                            loss = criterion(outputs, y_train.squeeze(1))\n",
    "\n",
    "                            #Add l1 norm\n",
    "                            l1_lambda = l1_norm\n",
    "                            l1_reg = torch.tensor(0.)\n",
    "                            for name, param in model.named_parameters():\n",
    "                                l1_reg += torch.norm(param, 1)\n",
    "                            loss += l1_lambda * l1_reg\n",
    "\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                        #Evaluate the model on the train set\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(X_train)\n",
    "                            predicted = torch.argmax(outputs, dim=1) \n",
    "\n",
    "                        predictions_train = pd.DataFrame(predicted, columns=[\"algorithm\"], index=y_train_indices)\n",
    "\n",
    "                        #Evaluate the model on the test set\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(X_test)\n",
    "                            predicted = torch.argmax(outputs, dim=1) \n",
    "\n",
    "                        predictions_test = pd.DataFrame(predicted, columns=[\"algorithm\"], index=y_test_indices)\n",
    "                        #---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                        #Calculate score on the train set\n",
    "                        int_to_label(predictions_train)\n",
    "\n",
    "                        scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "                        scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "\n",
    "                        score_train = calculate_score(scores, predictions_train)\n",
    "                        results.at[model_description, \"Score_train\"] = score_train\n",
    "\n",
    "                        #Calculate score on the test set\n",
    "                        int_to_label(predictions_test)\n",
    "\n",
    "                        scores = pd.DataFrame(pd.read_csv(\"scores.csv\", header=0, index_col=0))\n",
    "                        scores = scores.rename(columns={\"dynamic_programming_bellman_array\":\"dynamic_programming\", \"branch_and_bound_sort\": \"branch_and_bound\"})\n",
    "\n",
    "                        score_test = calculate_score(scores, predictions_test)\n",
    "                        results.at[model_description, \"Score_test\"] = score_test\n",
    "\n",
    "                        #Calculate the accuracy\n",
    "                        predictions_train = pd.DataFrame(predictions_train)\n",
    "                        predictions_test = pd.DataFrame(predictions_test)\n",
    "\n",
    "                        accuracy_train = accuracy_score(y_train_accuracy, predictions_train[\"algorithm\"]).round(4)\n",
    "                        accuracy_test = accuracy_score(y_test_accuracy, predictions_test[\"algorithm\"]).round(4)\n",
    "                        results.at[model_description, \"Accuracy_train\"] = accuracy_train\n",
    "                        results.at[model_description, \"Accuracy_test\"] = accuracy_test\n",
    "\n",
    "                        #Calculate the gaps\n",
    "                        VBS_train = train_average_best_solver\n",
    "                        SBS_train = train_average_single_best_solver\n",
    "                        AAS_train = score_train\n",
    "                        VBS_test = test_average_best_solver\n",
    "                        SBS_test = test_average_single_best_solver\n",
    "                        AAS_test = score_test\n",
    "\n",
    "                        Gap_train = SBS_train - VBS_train\n",
    "                        Gap_test = SBS_test - VBS_test\n",
    "                        Score_train = AAS_train - VBS_train\n",
    "                        Score_test = AAS_test - VBS_test\n",
    "\n",
    "                        GapClose_train = 1-(Score_train/Gap_train)\n",
    "                        GapClose_test = 1-(Score_test/Gap_test)\n",
    "                        results.at[model_description, \"Gap_train\"] = GapClose_train\n",
    "                        results.at[model_description, \"Gap_test\"] = GapClose_test\n",
    "\n",
    "                        #Overwrite the results file\n",
    "                        results.to_csv(result_path, index=True, sep=\",\", header=True, index_label=None, na_rep=\"\")\n",
    "                        \n",
    "                        #Output the progress\n",
    "                        print(str(model_description)+\" calculated, \"+str(counter)+\"/192\")\n",
    "                        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87597636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       epochs  learning_rate     L1_norm     L2_norm      VBS_test  \\\n",
      "count   192.0     192.000000  192.000000  192.000000  1.920000e+02   \n",
      "mean    200.0       0.055000    0.000500    0.003367  1.910834e+03   \n",
      "std       0.0       0.045118    0.000501    0.004703  2.279681e-13   \n",
      "min     200.0       0.010000    0.000000    0.000000  1.910834e+03   \n",
      "25%     200.0       0.010000    0.000000    0.000000  1.910834e+03   \n",
      "50%     200.0       0.055000    0.000500    0.000100  1.910834e+03   \n",
      "75%     200.0       0.100000    0.001000    0.010000  1.910834e+03   \n",
      "max     200.0       0.100000    0.001000    0.010000  1.910834e+03   \n",
      "\n",
      "          SBS_test  Accuracy_test    Score_test    Gap_test     VBS_train  \\\n",
      "count   192.000000     192.000000    192.000000  192.000000  1.920000e+02   \n",
      "mean   5106.399455       0.557578   9075.447949   -1.242049  1.936892e+03   \n",
      "std       0.000000       0.043381   4591.248420    1.436756  2.279681e-13   \n",
      "min    5106.399455       0.445100   5077.828221   -4.097955  1.936892e+03   \n",
      "25%    5106.399455       0.533075   5106.399455   -2.479323  1.936892e+03   \n",
      "50%    5106.399455       0.586300   5352.425191   -0.076990  1.936892e+03   \n",
      "75%    5106.399455       0.586300  13029.238475    0.000000  1.936892e+03   \n",
      "max    5106.399455       0.611200  18201.682409    0.008941  1.936892e+03   \n",
      "\n",
      "          SBS_train  Accuracy_train  Score_train   Gap_train  \n",
      "count  1.920000e+02      192.000000   192.000000  192.000000  \n",
      "mean   4.801997e+03        0.728368  4390.599592    0.143589  \n",
      "std    9.118725e-13        0.164154  1515.475636    0.528942  \n",
      "min    4.801997e+03        0.587200  1936.891656   -1.221926  \n",
      "25%    4.801997e+03        0.587200  3238.638735    0.000000  \n",
      "50%    4.801997e+03        0.663300  4801.996742    0.000000  \n",
      "75%    4.801997e+03        0.880825  4801.996742    0.545655  \n",
      "max    4.801997e+03        1.000000  8302.942260    1.000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>epochs</th>\n",
       "      <th>weights</th>\n",
       "      <th>criterion</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>L1_norm</th>\n",
       "      <th>L2_norm</th>\n",
       "      <th>VBS_test</th>\n",
       "      <th>SBS_test</th>\n",
       "      <th>Accuracy_test</th>\n",
       "      <th>Score_test</th>\n",
       "      <th>Gap_test</th>\n",
       "      <th>VBS_train</th>\n",
       "      <th>SBS_train</th>\n",
       "      <th>Accuracy_train</th>\n",
       "      <th>Score_train</th>\n",
       "      <th>Gap_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_8_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.001_l2Norm_0.01</th>\n",
       "      <td>Very_Large_Leaky</td>\n",
       "      <td>No</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5935</td>\n",
       "      <td>5077.828221</td>\n",
       "      <td>0.008941</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6180</td>\n",
       "      <td>4690.462641</td>\n",
       "      <td>0.038928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_8_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.01</th>\n",
       "      <td>Very_Large_Leaky</td>\n",
       "      <td>No</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5879</td>\n",
       "      <td>5099.412340</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6469</td>\n",
       "      <td>4223.346248</td>\n",
       "      <td>0.201965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_8_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0</th>\n",
       "      <td>Very_Large_Leaky</td>\n",
       "      <td>No</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5865</td>\n",
       "      <td>5105.057725</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6002</td>\n",
       "      <td>4748.737471</td>\n",
       "      <td>0.018589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_6_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0</th>\n",
       "      <td>Very_Large</td>\n",
       "      <td>No</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5105.650236</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6230</td>\n",
       "      <td>4294.525410</td>\n",
       "      <td>0.177121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.001_l2Norm_0.01</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_5_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.001_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Dropout</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_5_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0.001_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Dropout</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_8_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0.001_l2Norm_0</th>\n",
       "      <td>Very_Large_Leaky</td>\n",
       "      <td>No</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_balanced_optimizer_SGD_learningrate_0.1_l1Norm_0_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_8_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Leaky</td>\n",
       "      <td>No</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0.001_l2Norm_0</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.001_l2Norm_0</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_5_weights_balanced_optimizer_SGD_learningrate_0.1_l1Norm_0_l2Norm_0.01</th>\n",
       "      <td>Very_Large_Dropout</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       model  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          Very_Large_Leaky   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          Very_Large_Leaky   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          Very_Large_Leaky   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...                Very_Large   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...        Very_Large_Dropout   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...        Very_Large_Dropout   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...          Very_Large_Leaky   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...          Very_Large_Leaky   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...        Very_Large_Dropout   \n",
       "\n",
       "                                                   dropout  activation  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...      No  Leaky ReLU   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...      No  Leaky ReLU   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...      No  Leaky ReLU   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...      No        ReLU   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...     Yes  Leaky ReLU   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...     Yes        ReLU   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...     Yes  Leaky ReLU   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...     Yes        ReLU   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...      No  Leaky ReLU   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...     Yes  Leaky ReLU   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...     Yes  Leaky ReLU   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...      No  Leaky ReLU   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...     Yes  Leaky ReLU   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...     Yes  Leaky ReLU   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...     Yes        ReLU   \n",
       "\n",
       "                                                    epochs   weights  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...   200.0    normal   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...   200.0    normal   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...   200.0  balanced   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...   200.0  balanced   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...   200.0  balanced   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...   200.0  balanced   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "\n",
       "                                                           criterion  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  CrossEntropyLoss   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...  CrossEntropyLoss   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "\n",
       "                                                   optimizer  learning_rate  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...      Adam           0.10   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...       SGD           0.01   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...      Adam           0.01   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...       SGD           0.10   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...      Adam           0.10   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...      Adam           0.10   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...      Adam           0.01   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...       SGD           0.10   \n",
       "\n",
       "                                                    L1_norm  L2_norm  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...    0.001   0.0100   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...    0.000   0.0100   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...    0.000   0.0000   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...    0.000   0.0000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...    0.001   0.0100   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...    0.001   0.0001   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...    0.000   0.0001   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...    0.001   0.0001   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...    0.001   0.0000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...    0.000   0.0001   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...    0.000   0.0001   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...    0.000   0.0001   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...    0.001   0.0000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...    0.001   0.0000   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...    0.000   0.0100   \n",
       "\n",
       "                                                       VBS_test     SBS_test  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  1910.833772  5106.399455   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...  1910.833772  5106.399455   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  1910.833772  5106.399455   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "\n",
       "                                                    Accuracy_test  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...         0.5935   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...         0.5879   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...         0.5865   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...         0.5863   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...         0.5863   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "\n",
       "                                                     Score_test  Gap_test  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  5077.828221  0.008941   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  5099.412340  0.002187   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  5105.057725  0.000420   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...  5105.650236  0.000234   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  5106.399455  0.000000   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...  5106.399455  0.000000   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000   \n",
       "\n",
       "                                                      VBS_train    SBS_train  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  1936.891656  4801.996742   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...  1936.891656  4801.996742   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "\n",
       "                                                    Accuracy_train  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          0.6180   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          0.6469   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          0.6002   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...          0.6230   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...          0.5872   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...          0.5872   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "\n",
       "                                                    Score_train  Gap_train  \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  4690.462641   0.038928  \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  4223.346248   0.201965  \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  4748.737471   0.018589  \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...  4294.525410   0.177121  \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  4801.996742   0.000000  \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...  4801.996742   0.000000  \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Display the results\n",
    "table = pd.DataFrame(pd.read_csv(\"MLP_results2.csv\", header=0, index_col=0))\n",
    "print(table.describe())\n",
    "table = table.sort_values([\"Gap_test\"], ascending=False)\n",
    "table = table[:15]\n",
    "display(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
