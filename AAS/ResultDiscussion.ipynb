{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d4a23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General approach described in third cell. Each result file is loaded, sorted after \"gap_test\" and the best models depicted\n",
    "\n",
    "#Imports\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f02a20a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>Accuracy_test</th>\n",
       "      <th>Gap_test</th>\n",
       "      <th>Accuracy_train</th>\n",
       "      <th>Gap_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7913</td>\n",
       "      <td>0.150508</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>0.982833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>0.080845</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.982499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>0.076622</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.980847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.059384</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.981270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7848</td>\n",
       "      <td>0.053339</td>\n",
       "      <td>0.9709</td>\n",
       "      <td>0.924482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  learning_rate  n_estimators  min_child_weight  subsample  \\\n",
       "0        6.0            0.1         200.0               1.0        0.8   \n",
       "1        6.0            0.1         200.0               2.0        0.8   \n",
       "2        6.0            0.1         200.0               2.0        1.0   \n",
       "3        6.0            0.1         200.0               1.0        1.0   \n",
       "4        6.0            0.1         100.0               1.0        0.8   \n",
       "\n",
       "   Accuracy_test  Gap_test  Accuracy_train  Gap_train  \n",
       "0         0.7913  0.150508          0.9971   0.982833  \n",
       "1         0.7942  0.080845          0.9968   0.982499  \n",
       "2         0.7942  0.076622          0.9968   0.980847  \n",
       "3         0.7926  0.059384          0.9975   0.981270  \n",
       "4         0.7848  0.053339          0.9709   0.924482  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First generation on scaling to_c_z\n",
    "\n",
    "#Load the data\n",
    "table = pd.DataFrame(pd.read_csv(\"XGB_results_to_c_z.csv\", header=0, index_col=0))\n",
    "\n",
    "#Sort the values after \"Gap_test\"\n",
    "table = table.sort_values([\"Gap_test\"], ascending=False)\n",
    "\n",
    "#Drop identical attributes across instances\n",
    "table = table.drop([\"VBS_test\", \"SBS_test\", \"VBS_train\", \"SBS_train\", \"Score_test\", \"Score_train\"], axis = 1)\n",
    "\n",
    "#Remove the instance name\n",
    "table = table.reset_index(drop = True)\n",
    "\n",
    "#Show the first five rows\n",
    "table = table[:5]\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbd66294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>Accuracy_test</th>\n",
       "      <th>Gap_test</th>\n",
       "      <th>Accuracy_train</th>\n",
       "      <th>Gap_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.667890</td>\n",
       "      <td>0.6642</td>\n",
       "      <td>0.624179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.663382</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.625261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>0.658483</td>\n",
       "      <td>0.6648</td>\n",
       "      <td>0.624107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>0.658084</td>\n",
       "      <td>0.6651</td>\n",
       "      <td>0.623914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6812</td>\n",
       "      <td>0.641274</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.627049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  learning_rate  n_estimators  min_child_weight  subsample  \\\n",
       "0        3.0           0.01         100.0               2.0        0.8   \n",
       "1        3.0           0.01         100.0               1.0        0.8   \n",
       "2        3.0           0.01         100.0               2.0        1.0   \n",
       "3        3.0           0.01         100.0               1.0        1.0   \n",
       "4        3.0           0.01         200.0               1.0        0.8   \n",
       "\n",
       "   Accuracy_test  Gap_test  Accuracy_train  Gap_train  \n",
       "0         0.6788  0.667890          0.6642   0.624179  \n",
       "1         0.6786  0.663382          0.6645   0.625261  \n",
       "2         0.6783  0.658483          0.6648   0.624107  \n",
       "3         0.6783  0.658084          0.6651   0.623914  \n",
       "4         0.6812  0.641274          0.6737   0.627049  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generation 1 of XGBClassifier\n",
    "table = pd.DataFrame(pd.read_csv(\"XGB_results.csv\", header=0, index_col=0))\n",
    "table = table.sort_values([\"Gap_test\"], ascending=False)\n",
    "table = table.drop([\"VBS_test\", \"SBS_test\", \"VBS_train\", \"SBS_train\", \"Score_test\", \"Score_train\", \"Seed\"], axis = 1)\n",
    "table = table.reset_index(drop = True)\n",
    "table = table[:5]\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9da5a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Accuracy_test    Gap_test\n",
      "count     216.000000  216.000000\n",
      "mean        0.678998    0.302367\n",
      "std         0.005770    0.702210\n",
      "min         0.673200   -1.217264\n",
      "25%         0.675300    0.603851\n",
      "50%         0.677200    0.665752\n",
      "75%         0.679100    0.672165\n",
      "max         0.696000    0.681592\n",
      "       Accuracy_test    Gap_test\n",
      "count     174.000000  174.000000\n",
      "mean        0.678915    0.641981\n",
      "std         0.006279    0.097345\n",
      "min         0.673200    0.021017\n",
      "25%         0.675100    0.647852\n",
      "50%         0.676400    0.668310\n",
      "75%         0.678800    0.672817\n",
      "max         0.696000    0.681592\n",
      "       Accuracy_test    Gap_test  Accuracy_train   Gap_train\n",
      "count     216.000000  216.000000      216.000000  216.000000\n",
      "mean        0.678998    0.302367        0.670196    0.241632\n",
      "std         0.005770    0.702210        0.011665    0.766901\n",
      "min         0.673200   -1.217264        0.659600   -1.441319\n",
      "25%         0.675300    0.603851        0.661350    0.610833\n",
      "50%         0.677200    0.665752        0.663600    0.626496\n",
      "75%         0.679100    0.672165        0.677625    0.631744\n",
      "max         0.696000    0.681592        0.697400    0.644562\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>Seed</th>\n",
       "      <th>VBS_test</th>\n",
       "      <th>SBS_test</th>\n",
       "      <th>Accuracy_test</th>\n",
       "      <th>Score_test</th>\n",
       "      <th>Gap_test</th>\n",
       "      <th>VBS_train</th>\n",
       "      <th>SBS_train</th>\n",
       "      <th>Accuracy_train</th>\n",
       "      <th>Score_train</th>\n",
       "      <th>Gap_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6780</td>\n",
       "      <td>2928.327679</td>\n",
       "      <td>0.681592</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6627</td>\n",
       "      <td>2964.306987</td>\n",
       "      <td>0.641404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6772</td>\n",
       "      <td>2932.234644</td>\n",
       "      <td>0.680369</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6620</td>\n",
       "      <td>2976.835237</td>\n",
       "      <td>0.637031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6772</td>\n",
       "      <td>2932.234644</td>\n",
       "      <td>0.680369</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6622</td>\n",
       "      <td>2976.486518</td>\n",
       "      <td>0.637153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6767</td>\n",
       "      <td>2934.082739</td>\n",
       "      <td>0.679791</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6622</td>\n",
       "      <td>2976.486497</td>\n",
       "      <td>0.637153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6780</td>\n",
       "      <td>2940.479166</td>\n",
       "      <td>0.677789</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6628</td>\n",
       "      <td>2967.312374</td>\n",
       "      <td>0.640355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>2940.798471</td>\n",
       "      <td>0.677689</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6612</td>\n",
       "      <td>2984.412505</td>\n",
       "      <td>0.634387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>2940.798471</td>\n",
       "      <td>0.677689</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6612</td>\n",
       "      <td>2984.412505</td>\n",
       "      <td>0.634387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>2940.798471</td>\n",
       "      <td>0.677689</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>2984.535105</td>\n",
       "      <td>0.634344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>2940.960620</td>\n",
       "      <td>0.677639</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6608</td>\n",
       "      <td>2982.271926</td>\n",
       "      <td>0.635134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>2940.960620</td>\n",
       "      <td>0.677639</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6607</td>\n",
       "      <td>2985.616485</td>\n",
       "      <td>0.633966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  learning_rate  n_estimators  min_child_weight  subsample  Seed  \\\n",
       "0        3.0          0.001          50.0               2.0        0.8   1.0   \n",
       "1        3.0          0.001         100.0               2.0        0.8   1.0   \n",
       "2        3.0          0.001         100.0               0.5        0.8   1.0   \n",
       "3        3.0          0.001         100.0               1.0        0.8   1.0   \n",
       "4        3.0          0.001          50.0               0.5        0.8   1.0   \n",
       "5        2.0          0.010          50.0               1.0        0.7   1.0   \n",
       "6        2.0          0.010          50.0               2.0        0.7   1.0   \n",
       "7        2.0          0.010          50.0               0.5        0.7   1.0   \n",
       "8        2.0          0.001         200.0               1.0        0.6   1.0   \n",
       "9        2.0          0.001         200.0               2.0        0.6   1.0   \n",
       "\n",
       "      VBS_test     SBS_test  Accuracy_test   Score_test  Gap_test  \\\n",
       "0  1910.833772  5106.399455         0.6780  2928.327679  0.681592   \n",
       "1  1910.833772  5106.399455         0.6772  2932.234644  0.680369   \n",
       "2  1910.833772  5106.399455         0.6772  2932.234644  0.680369   \n",
       "3  1910.833772  5106.399455         0.6767  2934.082739  0.679791   \n",
       "4  1910.833772  5106.399455         0.6780  2940.479166  0.677789   \n",
       "5  1910.833772  5106.399455         0.6759  2940.798471  0.677689   \n",
       "6  1910.833772  5106.399455         0.6759  2940.798471  0.677689   \n",
       "7  1910.833772  5106.399455         0.6759  2940.798471  0.677689   \n",
       "8  1910.833772  5106.399455         0.6756  2940.960620  0.677639   \n",
       "9  1910.833772  5106.399455         0.6756  2940.960620  0.677639   \n",
       "\n",
       "     VBS_train    SBS_train  Accuracy_train  Score_train  Gap_train  \n",
       "0  1936.891656  4801.996742          0.6627  2964.306987   0.641404  \n",
       "1  1936.891656  4801.996742          0.6620  2976.835237   0.637031  \n",
       "2  1936.891656  4801.996742          0.6622  2976.486518   0.637153  \n",
       "3  1936.891656  4801.996742          0.6622  2976.486497   0.637153  \n",
       "4  1936.891656  4801.996742          0.6628  2967.312374   0.640355  \n",
       "5  1936.891656  4801.996742          0.6612  2984.412505   0.634387  \n",
       "6  1936.891656  4801.996742          0.6612  2984.412505   0.634387  \n",
       "7  1936.891656  4801.996742          0.6611  2984.535105   0.634344  \n",
       "8  1936.891656  4801.996742          0.6608  2982.271926   0.635134  \n",
       "9  1936.891656  4801.996742          0.6607  2985.616485   0.633966  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generation 2 of XGBClassifier\n",
    "table = pd.DataFrame(pd.read_csv(\"XGB_results2.csv\", header=0, index_col=0))\n",
    "\n",
    "#Total amount of models that performed better on the test set than the SBS (in terms of the gap closure)\n",
    "amount_better_than_SBS = table.copy()\n",
    "amount_better_than_SBS = amount_better_than_SBS[amount_better_than_SBS[\"Gap_test\"]>0] \n",
    "print(amount_better_than_SBS.loc[:, [\"Accuracy_test\", \"Gap_test\"]].describe()) #Print all with a positive \"Gap_test\"\n",
    "\n",
    "#General procedure\n",
    "table = table.sort_values([\"Gap_test\"], ascending=False)\n",
    "print(table.loc[:, [\"Accuracy_test\", \"Gap_test\", \"Accuracy_train\", \"Gap_train\"]].describe())\n",
    "table = table.reset_index(drop = True)\n",
    "table = table[:10]\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f35d3404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Accuracy_test   Gap_test  Accuracy_train  Gap_train\n",
      "count      32.000000  32.000000       32.000000  32.000000\n",
      "mean        0.723091   0.344138        0.790500   0.627000\n",
      "std         0.029786   0.204767        0.110199   0.190248\n",
      "min         0.677200   0.005117        0.664200   0.385124\n",
      "25%         0.705200   0.200399        0.718875   0.497864\n",
      "50%         0.726300   0.305048        0.749350   0.591270\n",
      "75%         0.751275   0.493164        0.860025   0.669939\n",
      "max         0.760100   0.667890        0.992300   0.996167\n",
      "       Accuracy_test   Gap_test  Accuracy_train  Gap_train\n",
      "count      32.000000  32.000000       32.000000  32.000000\n",
      "mean        0.747988  -0.350390        0.835697   0.122554\n",
      "std         0.041801   0.388358        0.105989   0.739355\n",
      "min         0.667000  -1.164763        0.672700  -1.299096\n",
      "25%         0.733075  -0.536197        0.787575  -0.110602\n",
      "50%         0.760100  -0.248047        0.835400   0.280203\n",
      "75%         0.778125  -0.079506        0.883700   0.560078\n",
      "max         0.794200   0.150508        0.997500   0.982833\n",
      "       Accuracy_test   Gap_test  Accuracy_train  Gap_train\n",
      "count      32.000000  32.000000       32.000000  32.000000\n",
      "mean        0.723091   0.344138        0.790500   0.627000\n",
      "std         0.029786   0.204767        0.110199   0.190248\n",
      "min         0.677200   0.005117        0.664200   0.385124\n",
      "25%         0.705200   0.200399        0.718875   0.497864\n",
      "50%         0.726300   0.305048        0.749350   0.591270\n",
      "75%         0.751275   0.493164        0.860025   0.669939\n",
      "max         0.760100   0.667890        0.992300   0.996167\n",
      "       Accuracy_test  Gap_test  Accuracy_train  Gap_train\n",
      "count       6.000000  6.000000        6.000000   6.000000\n",
      "mean        0.790583  0.071996        0.987833   0.961070\n",
      "std         0.004042  0.045733        0.014312   0.032374\n",
      "min         0.784800  0.011278        0.967900   0.914491\n",
      "25%         0.787625  0.054850        0.977375   0.938573\n",
      "50%         0.791950  0.068003        0.996800   0.981059\n",
      "75%         0.793800  0.079790        0.997025   0.982192\n",
      "max         0.794200  0.150508        0.997500   0.982833\n"
     ]
    }
   ],
   "source": [
    "#Amount of models that outperformed the SBS in terms of the gap close\n",
    "#Comparison of the first generation of the XGBClassifier on the \"instances.csv\" and \"instances_to_c_z.csv\"\n",
    "table1 = pd.DataFrame(pd.read_csv(\"XGB_results.csv\", header=0, index_col=0))\n",
    "table2 = pd.DataFrame(pd.read_csv(\"XGB_results_to_c_z.csv\", header=0, index_col=0))\n",
    "print(table1.loc[:, [\"Accuracy_test\", \"Gap_test\", \"Accuracy_train\", \"Gap_train\"]].describe())\n",
    "print(table2.loc[:, [\"Accuracy_test\", \"Gap_test\", \"Accuracy_train\", \"Gap_train\"]].describe())\n",
    "table1 = table1[table1[\"Gap_test\"]>0]\n",
    "table2 = table2[table2[\"Gap_test\"]>0]\n",
    "print(table1.loc[:, [\"Accuracy_test\", \"Gap_test\", \"Accuracy_train\", \"Gap_train\"]].describe())\n",
    "print(table2.loc[:, [\"Accuracy_test\", \"Gap_test\", \"Accuracy_train\", \"Gap_train\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c45666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8f48cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Accuracy_test   Gap_test  Accuracy_train  Gap_train\n",
      "count      36.000000  36.000000       36.000000  36.000000\n",
      "mean        0.530492  -2.094133        0.694128  -1.251683\n",
      "std         0.128563   2.632910        0.214026   2.885008\n",
      "min         0.129300  -8.653757        0.119800  -9.721175\n",
      "25%         0.459975  -4.348523        0.607825  -2.949924\n",
      "50%         0.592550  -0.937007        0.649850   0.021047\n",
      "75%         0.616000   0.000000        0.836300   0.367954\n",
      "max         0.619500   0.040711        1.000000   1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>kernel</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>degree</th>\n",
       "      <th>VBS_test</th>\n",
       "      <th>SBS_test</th>\n",
       "      <th>Accuracy_test</th>\n",
       "      <th>Score_test</th>\n",
       "      <th>Gap_test</th>\n",
       "      <th>VBS_train</th>\n",
       "      <th>SBS_train</th>\n",
       "      <th>Accuracy_train</th>\n",
       "      <th>Score_train</th>\n",
       "      <th>Gap_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.9_kernel_rbf_classweight_None_degree_3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6179</td>\n",
       "      <td>4976.304403</td>\n",
       "      <td>0.040711</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.7502</td>\n",
       "      <td>4225.188890</td>\n",
       "      <td>0.201322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.1_kernel_rbf_classweight_None_degree_3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6160</td>\n",
       "      <td>4983.015718</td>\n",
       "      <td>0.038611</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6183</td>\n",
       "      <td>4676.818013</td>\n",
       "      <td>0.043691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.7_kernel_rbf_classweight_None_degree_3</th>\n",
       "      <td>0.70</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6160</td>\n",
       "      <td>4983.015718</td>\n",
       "      <td>0.038611</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>4517.807353</td>\n",
       "      <td>0.099190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.2_kernel_rbf_classweight_None_degree_3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6160</td>\n",
       "      <td>4983.015718</td>\n",
       "      <td>0.038611</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6183</td>\n",
       "      <td>4676.818013</td>\n",
       "      <td>0.043691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.5_kernel_rbf_classweight_None_degree_3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6160</td>\n",
       "      <td>4983.015718</td>\n",
       "      <td>0.038611</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6235</td>\n",
       "      <td>4654.934446</td>\n",
       "      <td>0.051329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.3_kernel_rbf_classweight_None_degree_3</th>\n",
       "      <td>0.30</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6160</td>\n",
       "      <td>4983.015718</td>\n",
       "      <td>0.038611</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6183</td>\n",
       "      <td>4676.818013</td>\n",
       "      <td>0.043691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_1_kernel_rbf_classweight_None_degree_3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>5007.938476</td>\n",
       "      <td>0.030812</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>4046.157315</td>\n",
       "      <td>0.263809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.01_kernel_poly_classweight_None_degree_3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>4776.493391</td>\n",
       "      <td>0.008901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.01_kernel_rbf_classweight_None_degree_3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.01_kernel_poly_classweight_None_degree_2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.1_kernel_poly_classweight_None_degree_2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>poly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5873</td>\n",
       "      <td>5197.283904</td>\n",
       "      <td>-0.028441</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6172</td>\n",
       "      <td>4706.898784</td>\n",
       "      <td>0.033192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.01_kernel_linear_classweight_None_degree_3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5999</td>\n",
       "      <td>5743.585836</td>\n",
       "      <td>-0.199397</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6426</td>\n",
       "      <td>4902.086379</td>\n",
       "      <td>-0.034934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.2_kernel_poly_classweight_None_degree_2</th>\n",
       "      <td>0.20</td>\n",
       "      <td>poly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5978</td>\n",
       "      <td>5762.887261</td>\n",
       "      <td>-0.205437</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6729</td>\n",
       "      <td>4650.178837</td>\n",
       "      <td>0.052989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.1_kernel_linear_classweight_None_degree_3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6150</td>\n",
       "      <td>6761.566316</td>\n",
       "      <td>-0.517957</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6870</td>\n",
       "      <td>5133.728399</td>\n",
       "      <td>-0.115783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.2_kernel_linear_classweight_None_degree_3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6166</td>\n",
       "      <td>7115.197727</td>\n",
       "      <td>-0.628621</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>5084.316866</td>\n",
       "      <td>-0.098537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.1_kernel_poly_classweight_None_degree_3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>poly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6177</td>\n",
       "      <td>7201.025683</td>\n",
       "      <td>-0.655479</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.9626</td>\n",
       "      <td>2371.965829</td>\n",
       "      <td>0.848147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_1_kernel_poly_classweight_None_degree_2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>poly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6177</td>\n",
       "      <td>7730.965452</td>\n",
       "      <td>-0.821315</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>2375.832792</td>\n",
       "      <td>0.846798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.2_kernel_poly_classweight_None_degree_3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>poly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6093</td>\n",
       "      <td>8095.066880</td>\n",
       "      <td>-0.935255</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>1937.052224</td>\n",
       "      <td>0.999944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_1_kernel_poly_classweight_None_degree_3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>poly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6099</td>\n",
       "      <td>8106.269031</td>\n",
       "      <td>-0.938760</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_1_kernel_poly_classweight_balanced_degree_3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>poly</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6099</td>\n",
       "      <td>8106.269031</td>\n",
       "      <td>-0.938760</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.2_kernel_poly_classweight_balanced_degree_3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>poly</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6109</td>\n",
       "      <td>8233.349729</td>\n",
       "      <td>-0.978528</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>1960.030724</td>\n",
       "      <td>0.991924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_1_kernel_linear_classweight_None_degree_3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.6058</td>\n",
       "      <td>8717.703708</td>\n",
       "      <td>-1.130099</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.7395</td>\n",
       "      <td>5093.772219</td>\n",
       "      <td>-0.101838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.1_kernel_poly_classweight_balanced_degree_3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>poly</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5648</td>\n",
       "      <td>11607.780424</td>\n",
       "      <td>-2.034501</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>2667.544190</td>\n",
       "      <td>0.744982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.2_kernel_rbf_classweight_balanced_degree_3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>rbf</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.4937</td>\n",
       "      <td>11972.050317</td>\n",
       "      <td>-2.148493</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6419</td>\n",
       "      <td>7663.751118</td>\n",
       "      <td>-0.998831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_1_kernel_poly_classweight_balanced_degree_2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>poly</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5627</td>\n",
       "      <td>12071.128919</td>\n",
       "      <td>-2.179498</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.9767</td>\n",
       "      <td>2770.444911</td>\n",
       "      <td>0.709067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_1_kernel_rbf_classweight_balanced_degree_3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>rbf</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5694</td>\n",
       "      <td>12078.478559</td>\n",
       "      <td>-2.181798</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.9666</td>\n",
       "      <td>2852.605489</td>\n",
       "      <td>0.680391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_1_kernel_linear_classweight_balanced_degree_3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>linear</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.4661</td>\n",
       "      <td>18707.245865</td>\n",
       "      <td>-4.256162</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>13336.027034</td>\n",
       "      <td>-2.978610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.2_kernel_linear_classweight_balanced_degree_3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>linear</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.4416</td>\n",
       "      <td>19887.824639</td>\n",
       "      <td>-4.625605</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6248</td>\n",
       "      <td>14600.519175</td>\n",
       "      <td>-3.419952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.2_kernel_poly_classweight_balanced_degree_2</th>\n",
       "      <td>0.20</td>\n",
       "      <td>poly</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.4188</td>\n",
       "      <td>20363.463561</td>\n",
       "      <td>-4.774449</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6738</td>\n",
       "      <td>13226.443965</td>\n",
       "      <td>-2.940362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.1_kernel_linear_classweight_balanced_degree_3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>linear</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.4263</td>\n",
       "      <td>20387.527811</td>\n",
       "      <td>-4.781979</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6096</td>\n",
       "      <td>15223.574684</td>\n",
       "      <td>-3.637416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.1_kernel_poly_classweight_balanced_degree_2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>poly</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.3958</td>\n",
       "      <td>21659.407397</td>\n",
       "      <td>-5.179993</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6025</td>\n",
       "      <td>15872.813812</td>\n",
       "      <td>-3.864018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.01_kernel_linear_classweight_balanced_degree_3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>linear</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.3901</td>\n",
       "      <td>22016.923357</td>\n",
       "      <td>-5.291872</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>17322.684449</td>\n",
       "      <td>-4.370062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.01_kernel_poly_classweight_balanced_degree_3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.3947</td>\n",
       "      <td>22272.599488</td>\n",
       "      <td>-5.371881</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>16812.624269</td>\n",
       "      <td>-4.192037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.01_kernel_poly_classweight_balanced_degree_2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.3112</td>\n",
       "      <td>29226.081073</td>\n",
       "      <td>-7.547860</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.3982</td>\n",
       "      <td>26454.278528</td>\n",
       "      <td>-7.557238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.1_kernel_rbf_classweight_balanced_degree_3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.1591</td>\n",
       "      <td>32739.961607</td>\n",
       "      <td>-8.647471</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>32573.048457</td>\n",
       "      <td>-9.692856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM_C_0.01_kernel_rbf_classweight_balanced_degree_3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>rbf</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.1293</td>\n",
       "      <td>32760.049681</td>\n",
       "      <td>-8.653757</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.1198</td>\n",
       "      <td>32654.185306</td>\n",
       "      <td>-9.721175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       C  kernel class_weight  \\\n",
       "SVM_C_0.9_kernel_rbf_classweight_None_degree_3      0.90     rbf          NaN   \n",
       "SVM_C_0.1_kernel_rbf_classweight_None_degree_3      0.10     rbf          NaN   \n",
       "SVM_C_0.7_kernel_rbf_classweight_None_degree_3      0.70     rbf          NaN   \n",
       "SVM_C_0.2_kernel_rbf_classweight_None_degree_3      0.20     rbf          NaN   \n",
       "SVM_C_0.5_kernel_rbf_classweight_None_degree_3      0.50     rbf          NaN   \n",
       "SVM_C_0.3_kernel_rbf_classweight_None_degree_3      0.30     rbf          NaN   \n",
       "SVM_C_1_kernel_rbf_classweight_None_degree_3        1.00     rbf          NaN   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_3    0.01    poly          NaN   \n",
       "SVM_C_0.01_kernel_rbf_classweight_None_degree_3     0.01     rbf          NaN   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_2    0.01    poly          NaN   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_2     0.10    poly          NaN   \n",
       "SVM_C_0.01_kernel_linear_classweight_None_degree_3  0.01  linear          NaN   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_2     0.20    poly          NaN   \n",
       "SVM_C_0.1_kernel_linear_classweight_None_degree_3   0.10  linear          NaN   \n",
       "SVM_C_0.2_kernel_linear_classweight_None_degree_3   0.20  linear          NaN   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_3     0.10    poly          NaN   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_2       1.00    poly          NaN   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_3     0.20    poly          NaN   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_3       1.00    poly          NaN   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_3   1.00    poly     balanced   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...  0.20    poly     balanced   \n",
       "SVM_C_1_kernel_linear_classweight_None_degree_3     1.00  linear          NaN   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...  0.10    poly     balanced   \n",
       "SVM_C_0.2_kernel_rbf_classweight_balanced_degree_3  0.20     rbf     balanced   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_2   1.00    poly     balanced   \n",
       "SVM_C_1_kernel_rbf_classweight_balanced_degree_3    1.00     rbf     balanced   \n",
       "SVM_C_1_kernel_linear_classweight_balanced_degr...  1.00  linear     balanced   \n",
       "SVM_C_0.2_kernel_linear_classweight_balanced_de...  0.20  linear     balanced   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...  0.20    poly     balanced   \n",
       "SVM_C_0.1_kernel_linear_classweight_balanced_de...  0.10  linear     balanced   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...  0.10    poly     balanced   \n",
       "SVM_C_0.01_kernel_linear_classweight_balanced_d...  0.01  linear     balanced   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...  0.01    poly     balanced   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...  0.01    poly     balanced   \n",
       "SVM_C_0.1_kernel_rbf_classweight_balanced_degree_3  0.10     rbf     balanced   \n",
       "SVM_C_0.01_kernel_rbf_classweight_balanced_degr...  0.01     rbf     balanced   \n",
       "\n",
       "                                                    degree     VBS_test  \\\n",
       "SVM_C_0.9_kernel_rbf_classweight_None_degree_3         3.0  1910.833772   \n",
       "SVM_C_0.1_kernel_rbf_classweight_None_degree_3         3.0  1910.833772   \n",
       "SVM_C_0.7_kernel_rbf_classweight_None_degree_3         3.0  1910.833772   \n",
       "SVM_C_0.2_kernel_rbf_classweight_None_degree_3         3.0  1910.833772   \n",
       "SVM_C_0.5_kernel_rbf_classweight_None_degree_3         3.0  1910.833772   \n",
       "SVM_C_0.3_kernel_rbf_classweight_None_degree_3         3.0  1910.833772   \n",
       "SVM_C_1_kernel_rbf_classweight_None_degree_3           3.0  1910.833772   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_3       3.0  1910.833772   \n",
       "SVM_C_0.01_kernel_rbf_classweight_None_degree_3        3.0  1910.833772   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_2       2.0  1910.833772   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_2        2.0  1910.833772   \n",
       "SVM_C_0.01_kernel_linear_classweight_None_degree_3     3.0  1910.833772   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_2        2.0  1910.833772   \n",
       "SVM_C_0.1_kernel_linear_classweight_None_degree_3      3.0  1910.833772   \n",
       "SVM_C_0.2_kernel_linear_classweight_None_degree_3      3.0  1910.833772   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_3        3.0  1910.833772   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_2          2.0  1910.833772   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_3        3.0  1910.833772   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_3          3.0  1910.833772   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_3      3.0  1910.833772   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...     3.0  1910.833772   \n",
       "SVM_C_1_kernel_linear_classweight_None_degree_3        3.0  1910.833772   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...     3.0  1910.833772   \n",
       "SVM_C_0.2_kernel_rbf_classweight_balanced_degree_3     3.0  1910.833772   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_2      2.0  1910.833772   \n",
       "SVM_C_1_kernel_rbf_classweight_balanced_degree_3       3.0  1910.833772   \n",
       "SVM_C_1_kernel_linear_classweight_balanced_degr...     3.0  1910.833772   \n",
       "SVM_C_0.2_kernel_linear_classweight_balanced_de...     3.0  1910.833772   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...     2.0  1910.833772   \n",
       "SVM_C_0.1_kernel_linear_classweight_balanced_de...     3.0  1910.833772   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...     2.0  1910.833772   \n",
       "SVM_C_0.01_kernel_linear_classweight_balanced_d...     3.0  1910.833772   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...     3.0  1910.833772   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...     2.0  1910.833772   \n",
       "SVM_C_0.1_kernel_rbf_classweight_balanced_degree_3     3.0  1910.833772   \n",
       "SVM_C_0.01_kernel_rbf_classweight_balanced_degr...     3.0  1910.833772   \n",
       "\n",
       "                                                       SBS_test  \\\n",
       "SVM_C_0.9_kernel_rbf_classweight_None_degree_3      5106.399455   \n",
       "SVM_C_0.1_kernel_rbf_classweight_None_degree_3      5106.399455   \n",
       "SVM_C_0.7_kernel_rbf_classweight_None_degree_3      5106.399455   \n",
       "SVM_C_0.2_kernel_rbf_classweight_None_degree_3      5106.399455   \n",
       "SVM_C_0.5_kernel_rbf_classweight_None_degree_3      5106.399455   \n",
       "SVM_C_0.3_kernel_rbf_classweight_None_degree_3      5106.399455   \n",
       "SVM_C_1_kernel_rbf_classweight_None_degree_3        5106.399455   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_3    5106.399455   \n",
       "SVM_C_0.01_kernel_rbf_classweight_None_degree_3     5106.399455   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_2    5106.399455   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_2     5106.399455   \n",
       "SVM_C_0.01_kernel_linear_classweight_None_degree_3  5106.399455   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_2     5106.399455   \n",
       "SVM_C_0.1_kernel_linear_classweight_None_degree_3   5106.399455   \n",
       "SVM_C_0.2_kernel_linear_classweight_None_degree_3   5106.399455   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_3     5106.399455   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_2       5106.399455   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_3     5106.399455   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_3       5106.399455   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_3   5106.399455   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...  5106.399455   \n",
       "SVM_C_1_kernel_linear_classweight_None_degree_3     5106.399455   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...  5106.399455   \n",
       "SVM_C_0.2_kernel_rbf_classweight_balanced_degree_3  5106.399455   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_2   5106.399455   \n",
       "SVM_C_1_kernel_rbf_classweight_balanced_degree_3    5106.399455   \n",
       "SVM_C_1_kernel_linear_classweight_balanced_degr...  5106.399455   \n",
       "SVM_C_0.2_kernel_linear_classweight_balanced_de...  5106.399455   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...  5106.399455   \n",
       "SVM_C_0.1_kernel_linear_classweight_balanced_de...  5106.399455   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...  5106.399455   \n",
       "SVM_C_0.01_kernel_linear_classweight_balanced_d...  5106.399455   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...  5106.399455   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...  5106.399455   \n",
       "SVM_C_0.1_kernel_rbf_classweight_balanced_degree_3  5106.399455   \n",
       "SVM_C_0.01_kernel_rbf_classweight_balanced_degr...  5106.399455   \n",
       "\n",
       "                                                    Accuracy_test  \\\n",
       "SVM_C_0.9_kernel_rbf_classweight_None_degree_3             0.6179   \n",
       "SVM_C_0.1_kernel_rbf_classweight_None_degree_3             0.6160   \n",
       "SVM_C_0.7_kernel_rbf_classweight_None_degree_3             0.6160   \n",
       "SVM_C_0.2_kernel_rbf_classweight_None_degree_3             0.6160   \n",
       "SVM_C_0.5_kernel_rbf_classweight_None_degree_3             0.6160   \n",
       "SVM_C_0.3_kernel_rbf_classweight_None_degree_3             0.6160   \n",
       "SVM_C_1_kernel_rbf_classweight_None_degree_3               0.6195   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_3           0.5863   \n",
       "SVM_C_0.01_kernel_rbf_classweight_None_degree_3            0.5863   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_2           0.5863   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_2            0.5873   \n",
       "SVM_C_0.01_kernel_linear_classweight_None_degree_3         0.5999   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_2            0.5978   \n",
       "SVM_C_0.1_kernel_linear_classweight_None_degree_3          0.6150   \n",
       "SVM_C_0.2_kernel_linear_classweight_None_degree_3          0.6166   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_3            0.6177   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_2              0.6177   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_3            0.6093   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_3              0.6099   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_3          0.6099   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...         0.6109   \n",
       "SVM_C_1_kernel_linear_classweight_None_degree_3            0.6058   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...         0.5648   \n",
       "SVM_C_0.2_kernel_rbf_classweight_balanced_degree_3         0.4937   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_2          0.5627   \n",
       "SVM_C_1_kernel_rbf_classweight_balanced_degree_3           0.5694   \n",
       "SVM_C_1_kernel_linear_classweight_balanced_degr...         0.4661   \n",
       "SVM_C_0.2_kernel_linear_classweight_balanced_de...         0.4416   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...         0.4188   \n",
       "SVM_C_0.1_kernel_linear_classweight_balanced_de...         0.4263   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...         0.3958   \n",
       "SVM_C_0.01_kernel_linear_classweight_balanced_d...         0.3901   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...         0.3947   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...         0.3112   \n",
       "SVM_C_0.1_kernel_rbf_classweight_balanced_degree_3         0.1591   \n",
       "SVM_C_0.01_kernel_rbf_classweight_balanced_degr...         0.1293   \n",
       "\n",
       "                                                      Score_test  Gap_test  \\\n",
       "SVM_C_0.9_kernel_rbf_classweight_None_degree_3       4976.304403  0.040711   \n",
       "SVM_C_0.1_kernel_rbf_classweight_None_degree_3       4983.015718  0.038611   \n",
       "SVM_C_0.7_kernel_rbf_classweight_None_degree_3       4983.015718  0.038611   \n",
       "SVM_C_0.2_kernel_rbf_classweight_None_degree_3       4983.015718  0.038611   \n",
       "SVM_C_0.5_kernel_rbf_classweight_None_degree_3       4983.015718  0.038611   \n",
       "SVM_C_0.3_kernel_rbf_classweight_None_degree_3       4983.015718  0.038611   \n",
       "SVM_C_1_kernel_rbf_classweight_None_degree_3         5007.938476  0.030812   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_3     5106.399455  0.000000   \n",
       "SVM_C_0.01_kernel_rbf_classweight_None_degree_3      5106.399455  0.000000   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_2     5106.399455  0.000000   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_2      5197.283904 -0.028441   \n",
       "SVM_C_0.01_kernel_linear_classweight_None_degree_3   5743.585836 -0.199397   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_2      5762.887261 -0.205437   \n",
       "SVM_C_0.1_kernel_linear_classweight_None_degree_3    6761.566316 -0.517957   \n",
       "SVM_C_0.2_kernel_linear_classweight_None_degree_3    7115.197727 -0.628621   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_3      7201.025683 -0.655479   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_2        7730.965452 -0.821315   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_3      8095.066880 -0.935255   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_3        8106.269031 -0.938760   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_3    8106.269031 -0.938760   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...   8233.349729 -0.978528   \n",
       "SVM_C_1_kernel_linear_classweight_None_degree_3      8717.703708 -1.130099   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...  11607.780424 -2.034501   \n",
       "SVM_C_0.2_kernel_rbf_classweight_balanced_degree_3  11972.050317 -2.148493   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_2   12071.128919 -2.179498   \n",
       "SVM_C_1_kernel_rbf_classweight_balanced_degree_3    12078.478559 -2.181798   \n",
       "SVM_C_1_kernel_linear_classweight_balanced_degr...  18707.245865 -4.256162   \n",
       "SVM_C_0.2_kernel_linear_classweight_balanced_de...  19887.824639 -4.625605   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...  20363.463561 -4.774449   \n",
       "SVM_C_0.1_kernel_linear_classweight_balanced_de...  20387.527811 -4.781979   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...  21659.407397 -5.179993   \n",
       "SVM_C_0.01_kernel_linear_classweight_balanced_d...  22016.923357 -5.291872   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...  22272.599488 -5.371881   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...  29226.081073 -7.547860   \n",
       "SVM_C_0.1_kernel_rbf_classweight_balanced_degree_3  32739.961607 -8.647471   \n",
       "SVM_C_0.01_kernel_rbf_classweight_balanced_degr...  32760.049681 -8.653757   \n",
       "\n",
       "                                                      VBS_train    SBS_train  \\\n",
       "SVM_C_0.9_kernel_rbf_classweight_None_degree_3      1936.891656  4801.996742   \n",
       "SVM_C_0.1_kernel_rbf_classweight_None_degree_3      1936.891656  4801.996742   \n",
       "SVM_C_0.7_kernel_rbf_classweight_None_degree_3      1936.891656  4801.996742   \n",
       "SVM_C_0.2_kernel_rbf_classweight_None_degree_3      1936.891656  4801.996742   \n",
       "SVM_C_0.5_kernel_rbf_classweight_None_degree_3      1936.891656  4801.996742   \n",
       "SVM_C_0.3_kernel_rbf_classweight_None_degree_3      1936.891656  4801.996742   \n",
       "SVM_C_1_kernel_rbf_classweight_None_degree_3        1936.891656  4801.996742   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_3    1936.891656  4801.996742   \n",
       "SVM_C_0.01_kernel_rbf_classweight_None_degree_3     1936.891656  4801.996742   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_2    1936.891656  4801.996742   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_2     1936.891656  4801.996742   \n",
       "SVM_C_0.01_kernel_linear_classweight_None_degree_3  1936.891656  4801.996742   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_2     1936.891656  4801.996742   \n",
       "SVM_C_0.1_kernel_linear_classweight_None_degree_3   1936.891656  4801.996742   \n",
       "SVM_C_0.2_kernel_linear_classweight_None_degree_3   1936.891656  4801.996742   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_3     1936.891656  4801.996742   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_2       1936.891656  4801.996742   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_3     1936.891656  4801.996742   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_3       1936.891656  4801.996742   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_3   1936.891656  4801.996742   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...  1936.891656  4801.996742   \n",
       "SVM_C_1_kernel_linear_classweight_None_degree_3     1936.891656  4801.996742   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...  1936.891656  4801.996742   \n",
       "SVM_C_0.2_kernel_rbf_classweight_balanced_degree_3  1936.891656  4801.996742   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_2   1936.891656  4801.996742   \n",
       "SVM_C_1_kernel_rbf_classweight_balanced_degree_3    1936.891656  4801.996742   \n",
       "SVM_C_1_kernel_linear_classweight_balanced_degr...  1936.891656  4801.996742   \n",
       "SVM_C_0.2_kernel_linear_classweight_balanced_de...  1936.891656  4801.996742   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...  1936.891656  4801.996742   \n",
       "SVM_C_0.1_kernel_linear_classweight_balanced_de...  1936.891656  4801.996742   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...  1936.891656  4801.996742   \n",
       "SVM_C_0.01_kernel_linear_classweight_balanced_d...  1936.891656  4801.996742   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...  1936.891656  4801.996742   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...  1936.891656  4801.996742   \n",
       "SVM_C_0.1_kernel_rbf_classweight_balanced_degree_3  1936.891656  4801.996742   \n",
       "SVM_C_0.01_kernel_rbf_classweight_balanced_degr...  1936.891656  4801.996742   \n",
       "\n",
       "                                                    Accuracy_train  \\\n",
       "SVM_C_0.9_kernel_rbf_classweight_None_degree_3              0.7502   \n",
       "SVM_C_0.1_kernel_rbf_classweight_None_degree_3              0.6183   \n",
       "SVM_C_0.7_kernel_rbf_classweight_None_degree_3              0.6617   \n",
       "SVM_C_0.2_kernel_rbf_classweight_None_degree_3              0.6183   \n",
       "SVM_C_0.5_kernel_rbf_classweight_None_degree_3              0.6235   \n",
       "SVM_C_0.3_kernel_rbf_classweight_None_degree_3              0.6183   \n",
       "SVM_C_1_kernel_rbf_classweight_None_degree_3                0.7942   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_3            0.5932   \n",
       "SVM_C_0.01_kernel_rbf_classweight_None_degree_3             0.5872   \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_2            0.5872   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_2             0.6172   \n",
       "SVM_C_0.01_kernel_linear_classweight_None_degree_3          0.6426   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_2             0.6729   \n",
       "SVM_C_0.1_kernel_linear_classweight_None_degree_3           0.6870   \n",
       "SVM_C_0.2_kernel_linear_classweight_None_degree_3           0.7009   \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_3             0.9626   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_2               0.9683   \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_3             0.9999   \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_3               1.0000   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_3           1.0000   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...          0.9995   \n",
       "SVM_C_1_kernel_linear_classweight_None_degree_3             0.7395   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...          0.9785   \n",
       "SVM_C_0.2_kernel_rbf_classweight_balanced_degree_3          0.6419   \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_2           0.9767   \n",
       "SVM_C_1_kernel_rbf_classweight_balanced_degree_3            0.9666   \n",
       "SVM_C_1_kernel_linear_classweight_balanced_degr...          0.6571   \n",
       "SVM_C_0.2_kernel_linear_classweight_balanced_de...          0.6248   \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...          0.6738   \n",
       "SVM_C_0.1_kernel_linear_classweight_balanced_de...          0.6096   \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...          0.6025   \n",
       "SVM_C_0.01_kernel_linear_classweight_balanced_d...          0.5523   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...          0.5921   \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...          0.3982   \n",
       "SVM_C_0.1_kernel_rbf_classweight_balanced_degree_3          0.1522   \n",
       "SVM_C_0.01_kernel_rbf_classweight_balanced_degr...          0.1198   \n",
       "\n",
       "                                                     Score_train  Gap_train  \n",
       "SVM_C_0.9_kernel_rbf_classweight_None_degree_3       4225.188890   0.201322  \n",
       "SVM_C_0.1_kernel_rbf_classweight_None_degree_3       4676.818013   0.043691  \n",
       "SVM_C_0.7_kernel_rbf_classweight_None_degree_3       4517.807353   0.099190  \n",
       "SVM_C_0.2_kernel_rbf_classweight_None_degree_3       4676.818013   0.043691  \n",
       "SVM_C_0.5_kernel_rbf_classweight_None_degree_3       4654.934446   0.051329  \n",
       "SVM_C_0.3_kernel_rbf_classweight_None_degree_3       4676.818013   0.043691  \n",
       "SVM_C_1_kernel_rbf_classweight_None_degree_3         4046.157315   0.263809  \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_3     4776.493391   0.008901  \n",
       "SVM_C_0.01_kernel_rbf_classweight_None_degree_3      4801.996742   0.000000  \n",
       "SVM_C_0.01_kernel_poly_classweight_None_degree_2     4801.996742   0.000000  \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_2      4706.898784   0.033192  \n",
       "SVM_C_0.01_kernel_linear_classweight_None_degree_3   4902.086379  -0.034934  \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_2      4650.178837   0.052989  \n",
       "SVM_C_0.1_kernel_linear_classweight_None_degree_3    5133.728399  -0.115783  \n",
       "SVM_C_0.2_kernel_linear_classweight_None_degree_3    5084.316866  -0.098537  \n",
       "SVM_C_0.1_kernel_poly_classweight_None_degree_3      2371.965829   0.848147  \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_2        2375.832792   0.846798  \n",
       "SVM_C_0.2_kernel_poly_classweight_None_degree_3      1937.052224   0.999944  \n",
       "SVM_C_1_kernel_poly_classweight_None_degree_3        1936.891656   1.000000  \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_3    1936.891656   1.000000  \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...   1960.030724   0.991924  \n",
       "SVM_C_1_kernel_linear_classweight_None_degree_3      5093.772219  -0.101838  \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...   2667.544190   0.744982  \n",
       "SVM_C_0.2_kernel_rbf_classweight_balanced_degree_3   7663.751118  -0.998831  \n",
       "SVM_C_1_kernel_poly_classweight_balanced_degree_2    2770.444911   0.709067  \n",
       "SVM_C_1_kernel_rbf_classweight_balanced_degree_3     2852.605489   0.680391  \n",
       "SVM_C_1_kernel_linear_classweight_balanced_degr...  13336.027034  -2.978610  \n",
       "SVM_C_0.2_kernel_linear_classweight_balanced_de...  14600.519175  -3.419952  \n",
       "SVM_C_0.2_kernel_poly_classweight_balanced_degr...  13226.443965  -2.940362  \n",
       "SVM_C_0.1_kernel_linear_classweight_balanced_de...  15223.574684  -3.637416  \n",
       "SVM_C_0.1_kernel_poly_classweight_balanced_degr...  15872.813812  -3.864018  \n",
       "SVM_C_0.01_kernel_linear_classweight_balanced_d...  17322.684449  -4.370062  \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...  16812.624269  -4.192037  \n",
       "SVM_C_0.01_kernel_poly_classweight_balanced_deg...  26454.278528  -7.557238  \n",
       "SVM_C_0.1_kernel_rbf_classweight_balanced_degree_3  32573.048457  -9.692856  \n",
       "SVM_C_0.01_kernel_rbf_classweight_balanced_degr...  32654.185306  -9.721175  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Only one table for both generation (36 models in total)\n",
    "table = pd.DataFrame(pd.read_csv(\"SVM_results.csv\", header=0, index_col=0))\n",
    "print(table.loc[:, [\"Accuracy_test\", \"Gap_test\", \"Accuracy_train\", \"Gap_train\"]].describe())\n",
    "table = table.sort_values([\"Gap_test\"], ascending=False)\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0758dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c288410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Accuracy_test    Gap_test  Accuracy_train   Gap_train\n",
      "count     192.000000  192.000000      192.000000  192.000000\n",
      "mean        0.550481   -1.252272        0.666831   -0.424666\n",
      "std         0.076873    1.962903        0.184603    2.049965\n",
      "min         0.129300   -9.598629        0.119800  -10.662625\n",
      "25%         0.539425   -2.247055        0.587200   -0.004541\n",
      "50%         0.586300    0.000000        0.587200    0.000000\n",
      "75%         0.586300    0.000000        0.833425    0.000000\n",
      "max         0.606700    0.000420        1.000000    1.000000\n",
      "Amount of underfitting models: 96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>epochs</th>\n",
       "      <th>weights</th>\n",
       "      <th>criterion</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>L1_norm</th>\n",
       "      <th>VBS_test</th>\n",
       "      <th>SBS_test</th>\n",
       "      <th>Accuracy_test</th>\n",
       "      <th>Score_test</th>\n",
       "      <th>Gap_test</th>\n",
       "      <th>VBS_train</th>\n",
       "      <th>SBS_train</th>\n",
       "      <th>Accuracy_train</th>\n",
       "      <th>Score_train</th>\n",
       "      <th>Gap_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_3_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0.0001</th>\n",
       "      <td>Large_Dropout</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5868</td>\n",
       "      <td>5105.056378</td>\n",
       "      <td>4.202941e-04</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5873</td>\n",
       "      <td>4801.661205</td>\n",
       "      <td>0.000117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_3_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0</th>\n",
       "      <td>Large_Dropout</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5865</td>\n",
       "      <td>5106.398092</td>\n",
       "      <td>4.265369e-07</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5873</td>\n",
       "      <td>4804.008512</td>\n",
       "      <td>-0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_1_l1Norm_0</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_4_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.0001</th>\n",
       "      <td>Large</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_1_l1Norm_0.0001</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_1_l1Norm_0</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_4_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0</th>\n",
       "      <td>Large</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_4_weights_balanced_optimizer_SGD_learningrate_0.0003_l1Norm_0.0001</th>\n",
       "      <td>Large</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_4_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0</th>\n",
       "      <td>Large</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_4_weights_balanced_optimizer_SGD_learningrate_0.0003_l1Norm_0</th>\n",
       "      <td>Large</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_3_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0</th>\n",
       "      <td>Large_Dropout</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_1_l1Norm_0.0001</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.0001</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_2_weights_balanced_optimizer_Adam_learningrate_1_l1Norm_0</th>\n",
       "      <td>Small</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            model  epochs  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  Large_Dropout   200.0   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  Large_Dropout   200.0   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          Small   200.0   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          Large   200.0   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          Small   200.0   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          Small   200.0   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          Large   200.0   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          Small   200.0   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          Large   200.0   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...          Large   200.0   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          Large   200.0   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  Large_Dropout   200.0   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          Small   200.0   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          Small   200.0   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...          Small   200.0   \n",
       "\n",
       "                                                     weights  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...    normal   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...    normal   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...    normal   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...    normal   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...    normal   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  balanced   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...    normal   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  balanced   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  balanced   \n",
       "\n",
       "                                                           criterion  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  CrossEntropyLoss   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  CrossEntropyLoss   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  CrossEntropyLoss   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "\n",
       "                                                   optimizer  learning_rate  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...       SGD         0.0100   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...       SGD         0.0100   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...       SGD         1.0000   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...       SGD         0.0100   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...      Adam         1.0000   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...      Adam         1.0000   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...       SGD         0.0100   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...      Adam         0.1000   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...       SGD         0.0003   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...      Adam         0.1000   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...       SGD         0.0003   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...      Adam         0.1000   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...       SGD         1.0000   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...       SGD         0.0100   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...      Adam         1.0000   \n",
       "\n",
       "                                                    L1_norm     VBS_test  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...   0.0001  1910.833772   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...   0.0000  1910.833772   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...   0.0000  1910.833772   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...   0.0001  1910.833772   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...   0.0001  1910.833772   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...   0.0000  1910.833772   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...   0.0000  1910.833772   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...   0.0000  1910.833772   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...   0.0001  1910.833772   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...   0.0000  1910.833772   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...   0.0000  1910.833772   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...   0.0000  1910.833772   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...   0.0001  1910.833772   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...   0.0001  1910.833772   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...   0.0000  1910.833772   \n",
       "\n",
       "                                                       SBS_test  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  5106.399455   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  5106.399455   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  5106.399455   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  5106.399455   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  5106.399455   \n",
       "\n",
       "                                                    Accuracy_test  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...         0.5868   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...         0.5865   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...         0.5863   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...         0.5863   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...         0.5863   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...         0.5863   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "\n",
       "                                                     Score_test      Gap_test  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  5105.056378  4.202941e-04   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  5106.398092  4.265369e-07   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000e+00   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000e+00   \n",
       "\n",
       "                                                      VBS_train    SBS_train  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  1936.891656  4801.996742   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  1936.891656  4801.996742   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  1936.891656  4801.996742   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "\n",
       "                                                    Accuracy_train  \\\n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...          0.5873   \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...          0.5873   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          0.5872   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          0.5872   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...          0.5872   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...          0.5872   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "\n",
       "                                                    Score_train  Gap_train  \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  4801.661205   0.000117  \n",
       "MLP_modelID_3_weights_normal_optimizer_SGD_lear...  4804.008512  -0.000702  \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  4801.996742   0.000000  \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_normal_optimizer_Adam_lea...  4801.996742   0.000000  \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_4_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  \n",
       "MLP_modelID_4_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_3_weights_normal_optimizer_Adam_lea...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_2_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First generation of MLP models\n",
    "table = pd.DataFrame(pd.read_csv(\"MLP_results.csv\", header=0, index_col=0))\n",
    "print(table.loc[:, [\"Accuracy_test\", \"Gap_test\", \"Accuracy_train\", \"Gap_train\"]].describe())\n",
    "table = table.sort_values([\"Gap_test\"], ascending=False)\n",
    "\n",
    "#Calculate the amount of underfitting models\n",
    "underfitting_models = table[table.Gap_test == 0]\n",
    "print(\"Amount of underfitting models: \" + str(len(underfitting_models)))\n",
    "\n",
    "table = table[:15]\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "259b3384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Accuracy_test    Gap_test  Accuracy_train   Gap_train\n",
      "count     192.000000  192.000000      192.000000  192.000000\n",
      "mean        0.557578   -1.242049        0.728368    0.143589\n",
      "std         0.043381    1.436756        0.164154    0.528942\n",
      "min         0.445100   -4.097955        0.587200   -1.221926\n",
      "25%         0.533075   -2.479323        0.587200    0.000000\n",
      "50%         0.586300   -0.076990        0.663300    0.000000\n",
      "75%         0.586300    0.000000        0.880825    0.545655\n",
      "max         0.611200    0.008941        1.000000    1.000000\n",
      "Amount of underfitting models: 87\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>epochs</th>\n",
       "      <th>weights</th>\n",
       "      <th>criterion</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>L1_norm</th>\n",
       "      <th>L2_norm</th>\n",
       "      <th>VBS_test</th>\n",
       "      <th>SBS_test</th>\n",
       "      <th>Accuracy_test</th>\n",
       "      <th>Score_test</th>\n",
       "      <th>Gap_test</th>\n",
       "      <th>VBS_train</th>\n",
       "      <th>SBS_train</th>\n",
       "      <th>Accuracy_train</th>\n",
       "      <th>Score_train</th>\n",
       "      <th>Gap_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_8_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.001_l2Norm_0.01</th>\n",
       "      <td>Very_Large_Leaky</td>\n",
       "      <td>No</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5935</td>\n",
       "      <td>5077.828221</td>\n",
       "      <td>0.008941</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6180</td>\n",
       "      <td>4690.462641</td>\n",
       "      <td>0.038928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_8_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0.01</th>\n",
       "      <td>Very_Large_Leaky</td>\n",
       "      <td>No</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5879</td>\n",
       "      <td>5099.412340</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6469</td>\n",
       "      <td>4223.346248</td>\n",
       "      <td>0.201965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_8_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0</th>\n",
       "      <td>Very_Large_Leaky</td>\n",
       "      <td>No</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5865</td>\n",
       "      <td>5105.057725</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6002</td>\n",
       "      <td>4748.737471</td>\n",
       "      <td>0.018589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_6_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0_l2Norm_0</th>\n",
       "      <td>Very_Large</td>\n",
       "      <td>No</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5105.650236</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.6230</td>\n",
       "      <td>4294.525410</td>\n",
       "      <td>0.177121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.001_l2Norm_0.01</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_5_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.001_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Dropout</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_normal_optimizer_Adam_learningrate_0.1_l1Norm_0_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_5_weights_normal_optimizer_SGD_learningrate_0.01_l1Norm_0.001_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Dropout</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_8_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0.001_l2Norm_0</th>\n",
       "      <td>Very_Large_Leaky</td>\n",
       "      <td>No</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_balanced_optimizer_SGD_learningrate_0.1_l1Norm_0_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_8_weights_balanced_optimizer_Adam_learningrate_0.1_l1Norm_0_l2Norm_0.0001</th>\n",
       "      <td>Very_Large_Leaky</td>\n",
       "      <td>No</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_balanced_optimizer_Adam_learningrate_0.01_l1Norm_0.001_l2Norm_0</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_7_weights_balanced_optimizer_SGD_learningrate_0.01_l1Norm_0.001_l2Norm_0</th>\n",
       "      <td>Very_Large_Dropout_Leaky</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_modelID_5_weights_balanced_optimizer_SGD_learningrate_0.1_l1Norm_0_l2Norm_0.01</th>\n",
       "      <td>Very_Large_Dropout</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>200.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1910.833772</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>5106.399455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1936.891656</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>4801.996742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       model  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          Very_Large_Leaky   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          Very_Large_Leaky   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          Very_Large_Leaky   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...                Very_Large   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...        Very_Large_Dropout   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...        Very_Large_Dropout   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...          Very_Large_Leaky   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...          Very_Large_Leaky   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  Very_Large_Dropout_Leaky   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...        Very_Large_Dropout   \n",
       "\n",
       "                                                   dropout  activation  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...      No  Leaky ReLU   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...      No  Leaky ReLU   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...      No  Leaky ReLU   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...      No        ReLU   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...     Yes  Leaky ReLU   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...     Yes        ReLU   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...     Yes  Leaky ReLU   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...     Yes        ReLU   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...      No  Leaky ReLU   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...     Yes  Leaky ReLU   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...     Yes  Leaky ReLU   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...      No  Leaky ReLU   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...     Yes  Leaky ReLU   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...     Yes  Leaky ReLU   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...     Yes        ReLU   \n",
       "\n",
       "                                                    epochs   weights  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...   200.0    normal   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...   200.0    normal   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...   200.0  balanced   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...   200.0  balanced   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...   200.0  balanced   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...   200.0  balanced   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...   200.0  balanced   \n",
       "\n",
       "                                                           criterion  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  CrossEntropyLoss   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...  CrossEntropyLoss   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  CrossEntropyLoss   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  CrossEntropyLoss   \n",
       "\n",
       "                                                   optimizer  learning_rate  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...      Adam           0.10   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...       SGD           0.01   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...      Adam           0.01   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...       SGD           0.10   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...      Adam           0.10   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...      Adam           0.10   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...      Adam           0.01   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...       SGD           0.01   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...       SGD           0.10   \n",
       "\n",
       "                                                    L1_norm  L2_norm  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...    0.001   0.0100   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...    0.000   0.0100   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...    0.000   0.0000   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...    0.000   0.0000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...    0.001   0.0100   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...    0.001   0.0001   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...    0.000   0.0001   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...    0.001   0.0001   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...    0.001   0.0000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...    0.000   0.0001   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...    0.000   0.0001   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...    0.000   0.0001   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...    0.001   0.0000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...    0.001   0.0000   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...    0.000   0.0100   \n",
       "\n",
       "                                                       VBS_test     SBS_test  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  1910.833772  5106.399455   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...  1910.833772  5106.399455   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  1910.833772  5106.399455   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  1910.833772  5106.399455   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  1910.833772  5106.399455   \n",
       "\n",
       "                                                    Accuracy_test  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...         0.5935   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...         0.5879   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...         0.5865   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...         0.5863   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...         0.5863   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...         0.5863   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...         0.5863   \n",
       "\n",
       "                                                     Score_test  Gap_test  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  5077.828221  0.008941   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  5099.412340  0.002187   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  5105.057725  0.000420   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...  5105.650236  0.000234   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  5106.399455  0.000000   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...  5106.399455  0.000000   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  5106.399455  0.000000   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  5106.399455  0.000000   \n",
       "\n",
       "                                                      VBS_train    SBS_train  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  1936.891656  4801.996742   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...  1936.891656  4801.996742   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  1936.891656  4801.996742   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  1936.891656  4801.996742   \n",
       "\n",
       "                                                    Accuracy_train  \\\n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          0.6180   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          0.6469   \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...          0.6002   \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...          0.6230   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...          0.5872   \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...          0.5872   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...          0.5872   \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...          0.5872   \n",
       "\n",
       "                                                    Score_train  Gap_train  \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  4690.462641   0.038928  \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  4223.346248   0.201965  \n",
       "MLP_modelID_8_weights_balanced_optimizer_SGD_le...  4748.737471   0.018589  \n",
       "MLP_modelID_6_weights_balanced_optimizer_SGD_le...  4294.525410   0.177121  \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_7_weights_normal_optimizer_Adam_lea...  4801.996742   0.000000  \n",
       "MLP_modelID_5_weights_normal_optimizer_SGD_lear...  4801.996742   0.000000  \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  \n",
       "MLP_modelID_8_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  \n",
       "MLP_modelID_7_weights_balanced_optimizer_Adam_l...  4801.996742   0.000000  \n",
       "MLP_modelID_7_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  \n",
       "MLP_modelID_5_weights_balanced_optimizer_SGD_le...  4801.996742   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Second generation of MLP models\n",
    "table = pd.DataFrame(pd.read_csv(\"MLP_results2.csv\", header=0, index_col=0))\n",
    "print(table.loc[:, [\"Accuracy_test\", \"Gap_test\", \"Accuracy_train\", \"Gap_train\"]].describe())\n",
    "table = table.sort_values([\"Gap_test\"], ascending=False)\n",
    "\n",
    "#Calculate the amount of underfitting models\n",
    "underfitting_models = table[table.Gap_test == 0]\n",
    "print(\"Amount of underfitting models: \" + str(len(underfitting_models)))\n",
    "\n",
    "table = table[:15]\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "be221bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGdCAYAAADzOWwgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABKHElEQVR4nO3deVhU1eMG8HfYhnVAEEGTTU0Q9y0dLUVFQfmm5m4GLqhpaKm5YeZaaWaa5UILgRakWOaeSCK44YaMW4YbhguLioCisp7fHz7cnxOoYMCI9/08zzyPc8655557QXg599w7CiGEABEREZGM6el6AERERES6xkBEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREsmeg6wFUB0VFRbhx4wYsLCygUCh0PRwiIiIqAyEE7t69izp16kBP7+lzQAxEZXDjxg04ODjoehhERET0HK5evYq6des+tQ0DURlYWFgAeHRCVSqVjkdDREREZZGdnQ0HBwfp9/jTMBCVQfFlMpVKxUBERERUzZRluQsXVRMREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRBVI/PmzYNCodB6ubm5SfXfffcdPDw8oFKpoFAokJmZWaKPjIwMDBs2DCqVClZWVvD398e9e/e02gghsHTpUjRs2BBKpRKvvPIKPv300zKNMTc3Fy1atIBCoYBGo9GqO3XqFN544w0YGxvDwcEBS5YsKfc5ICIiqgz8LLNqpnHjxvjzzz+l9wYG//8lvH//Pry9veHt7Y3AwMBStx82bBhSUlIQFRWF/Px8jBw5EmPHjkV4eLjU5oMPPsDu3buxdOlSNG3aFBkZGcjIyCjT+KZPn446derg5MmTWuXZ2dno0aMHPD09ERQUhNOnT2PUqFGwsrLC2LFjy3MKiIiIKp6gZ8rKyhIARFZWlk7HMXfuXNG8efNnttu7d68AIO7cuaNV/tdffwkA4tixY1LZH3/8IRQKhbh+/brUxsDAQPz999/lHt/OnTuFm5ubOHv2rAAgEhISpLrVq1eLGjVqiNzcXKlsxowZwtXVtdz7qWpz584VALRej4/7wYMH4r333hPW1tbCzMxM9OvXT6Smpkr1ISEhJbYvfqWlpQkhhBg+fHip9e7u7k8d265du0S7du2Eubm5qFmzpujXr59ISkqS6vfv3y86dOggrK2thbGxsXB1dRXLli2r2BNERPSCKs/vb14yq2YuXLiAOnXqoF69ehg2bBiSk5PLvG1cXBysrKzQpk0bqczT0xN6eno4cuQIAGDbtm2oV68etm/fDhcXFzg7O2P06NHPnCFKS0vDmDFj8NNPP8HU1LTUfXfq1AlGRkZSmZeXFxITE3Hnzp0yH4OuNG7cGCkpKdLrwIEDUt3kyZOxbds2bNy4EbGxsbhx4wb69esn1Q8ePFhr25SUFHh5eaFz586oVasWAGDFihVa9VevXoW1tTUGDhz4xDElJSWhT58+6Nq1KzQaDSIjI3Hr1i2tfZuZmWHChAnYt28fzp07h9mzZ2P27Nn47rvvKuEsERFVX7xkVo20a9cOoaGhcHV1RUpKCubPn4833ngDZ86cgYWFxTO3T01NlX4BFzMwMIC1tTVSU1MBAJcvX8Y///yDjRs3Yt26dSgsLMTkyZMxYMAAREdHl9qvEAIjRozAuHHj0KZNG1y5cqXUfbu4uGiV2dnZSXU1atQoyynQGQMDA9jb25coz8rKQnBwMMLDw9G1a1cAQEhICBo1aoTDhw+jffv2MDExgYmJibTNzZs3ER0djeDgYKnM0tISlpaW0vvNmzfjzp07GDly5BPHFB8fj8LCQnzyySfQ03v0t83UqVPRp08f5Ofnw9DQEC1btkTLli2lbZydnbFp0ybs37+flyqJiB7DGaJqpGfPnhg4cCCaNWsGLy8v7Ny5E5mZmYiIiKiwfRQVFSE3Nxfr1q3DG2+8AQ8PDwQHB2Pv3r1ITEwsdZtvvvkGd+/efeK6pZfBk2bm4uPjkZ+fD09PT6mtm5sbHB0dERcXV2pf69atg6mpKQYMGPDE/QUHB8PT0xNOTk5PbNO6dWvo6ekhJCQEhYWFyMrKwk8//QRPT08YGhqWuk1CQgIOHTqEzp07l+WwiYhkg4GoGrOyskLDhg1x8eLFMrW3t7dHenq6VllBQQEyMjKk2Y/atWvDwMAADRs2lNo0atQIAJ54eS46OhpxcXFQKpUwMDBAgwYNAABt2rTB8OHDpX2npaVpbVf8vrSZlxdJ8czcrl27sGbNGiQlJeGNN97A3bt3kZqaCiMjI1hZWWltY2dnJ826/VtwcDDefvttrVmjx924cQN//PEHRo8e/dRxubi4YPfu3Zg1axaUSiWsrKxw7dq1UgNy3bp1oVQq0aZNGwQEBDyzbyIiuWEgqsbu3buHS5cuoXbt2mVqr1arkZmZifj4eKksOjoaRUVFaNeuHQCgY8eOKCgowKVLl6Q258+fB4AnzlZ8/fXXOHnyJDQaDTQaDXbu3AkA2LBhg3S7vlqtxr59+5Cfny9tFxUVBVdX1xf+cllFzszFxcXh3Llz8Pf3f2KbtWvXwsrKCn379n1qX6mpqRgzZgyGDx+OY8eOITY2FkZGRhgwYACEEFpt9+/fj+PHjyMoKAhfffUVfvnll3KPnYjopVbpS7xfAi/KXWYffvihiImJEUlJSeLgwYPC09NT1KxZU6SnpwshhEhJSREJCQni+++/FwDEvn37REJCgrh9+7bUh7e3t2jZsqU4cuSIOHDggHj11VfF0KFDpfrCwkLRqlUr0alTJ3HixAlx/Phx0a5dO9G9e3epzZEjR4Srq6u4du1aqeNMSkoqcZdZZmamsLOzE76+vuLMmTNi/fr1wtTUVHz77bcVfJaqRps2bcTMmTPFnj17Sr2jz9HRsdS7uUaNGiVatGjxxH6LiopEgwYNxKRJk545htmzZ4s2bdpolV29elUAEHFxcU/cbuHChaJhw4bP7J+IqLrjXWYvqWvXrmHo0KFwdXXFoEGDYGNjg8OHD8PW1hYAEBQUhJYtW2LMmDEAgE6dOqFly5bYunWr1EdYWBjc3NzQrVs39OrVC6+//rrWHUd6enrYtm0batasiU6dOsHHxweNGjXC+vXrpTb3799HYmKi1mzPs1haWmL37t1ISkpC69at8eGHH2LOnDnVcmHv4zNzrVu3hqGhIfbs2SPVJyYmIjk5GWq1usR2ERERT50dio2NxcWLF5/aptj9+/elxdTF9PX1ATxaC/YkxevEqpvFixdDoVBg0qRJUtmlS5fw1ltvwdbWFiqVCoMGDdK6NBsTE1PiYabFr2PHjkntxHM8jLR3795wdHSEsbExateuDV9fX9y4cUOrDR9GSlSNVH4+q/5elBki0o1nzcyNGzdOODo6iujoaHH8+HGhVquFWq0u0c8PP/wgjI2NS8wmPe6dd94R7dq1K7Xum2++EV27dpXe79mzRygUCjF//nxx/vx5ER8fL7y8vISTk5O4f/++EEKIlStXiq1bt4rz58+L8+fPix9++EFYWFiIjz766D+ckap39OhR4ezsLJo1ayY++OADIYQQ9+7dE/Xq1RNvvfWWOHXqlDh16pTo06ePaNu2rSgsLBRCCJGbmytSUlK0XqNHjxYuLi6iqKhI6n/ixInC1dVVbNmyRVy+fFkcP35c7N69+6ljWrZsmYiLixNXrlwRBw8eLPF1z8rKEnZ2dmLYsGHizJkz4pdffhEmJibVdlaUqDoqz+9vBqIyYCCSt8GDB4vatWsLIyMj8corr4jBgweLixcvSvXFD2asUaOGMDU1FW+99ZZISUkp0Y9arRZvv/32E/eTmZkpTExMxHfffVdq/dy5c4WTk5NW2S+//CJatmwpzMzMhK2trejdu7c4d+6cVP/111+Lxo0bC1NTU6FSqUTLli3F6tWrpcBQHdy9e1e8+uqrIioqSnTu3FkKRJGRkUJPT0/r/2VmZqZQKBQiKiqq1L7y8vKEra2tWLBggVT2Xx5G+rgtW7YIhUIh8vLyhBDV+2GkRC8LBqIKxkBEpDt+fn7SmqrHA9HWrVuFvr6+ePjwodT24cOHQl9fX8ydO7fUvn799Vehp6cnrl69KpV9/vnnomHDhmLp0qXC2dlZODk5CX9/f621d89y+/ZtMWjQINGxY0epzNfXV/Tp00erXXR0tAAgMjIyytw3ET2/8vz+5oMZXwDOM3foegjVxpXFProeAlWh9evX48SJE1rrfYq1b98eZmZmmDFjBj777DMIITBz5kwUFhYiJSWl1P6Cg4Ph5eWFunXrSmXP8zDSYjNmzMDKlStx//59tG/fHtu3b5fqqvvDSInkhouqieiFdPXqVXzwwQcICwuDsbFxiXpbW1ts3LgR27Ztg7m5OSwtLZGZmYlWrVqVWGwOPLopITIyssSC9ed5GGmxadOmISEhAbt374a+vj78/PxKPPKAiKoHzhCRbHFmrux0MTMXHx+P9PR0tGrVSiorLCzEvn37sHLlSuTm5qJHjx64dOkSbt26BQMDA1hZWcHe3h716tUr0V9ISAhsbGzQu3dvrfJnPYzU1dX1iWOsWbMmatasiYYNG6JRo0ZwcHDA4cOHoVarq/XDSInkSKczRPPmzStxK6ybm5tU//DhQwQEBMDGxgbm5ubo379/iR8wycnJ8PHxgampKWrVqoVp06ahoKBAq01MTAxatWoFpVKJBg0aIDQ0tCoOj4j+g27duuH06dPSAz81Gg3atGmDYcOGQaPRSI8YAB4FEysrK0RHRyM9Pb1E6BFCICQkBH5+fiU+1uR5HkZamuJHHRQ/0qA6P4yUSI50fsnsv3yKeGFhIXx8fJCXl4dDhw5h7dq1CA0NxZw5c6Q2SUlJ8PHxQZcuXaDRaDBp0iSMHj0akZGRVXqcRFQ+FhYWaNKkidbLzMwMNjY2aNKkCYBHsz6HDx/GpUuX8PPPP2PgwIGYPHlyiVmd6OhoJCUllfqRJZ6enmjVqhVGjRqFhIQExMfH491330X37t2lWaOjR4/Czc0N169fBwAcOXIEK1euhEajwT///IPo6GgMHToU9evXl54/9fbbb8PIyAj+/v44e/YsNmzYgBUrVmDKlCmVedqI6Dnp/JLZf/kU8d27d+Ovv/7Cn3/+CTs7O7Ro0QILFy7EjBkzMG/ePBgZGSEoKAguLi748ssvATyaCj9w4ACWL18OLy+vKj1WIqpYiYmJCAwMREZGBpydnfHRRx9h8uTJJdoFBwejQ4cOWjPQxYofRjpx4kR06tQJZmZm6Nmzp/QzAyj5MFJTU1Ns2rQJc+fORU5ODmrXrg1vb2/Mnj0bSqUSwP8/jDQgIACtW7dGzZo1q+3DSInkQOeBqPhTxI2NjaFWq7Fo0SI4Ojo+81PE27dvj7i4ODRt2lS6cwMAvLy8MH78eJw9exYtW7ZEXFycVh/FbR5/2u2/5ebmaj3JNzs7u+IOmIieW0xMjNb7xYsXY/Hixc/cLjw8/Kn1derUwW+//fbEeg8PD63F0k2bNn3mHWgA0KxZM+zfv/+Z7YhI93R6yey/fop4amqqVhgqri+ue1qb7OxsPHjwoNRxLVq0CJaWltLLwcGhIg6XiIiIXlA6nSHq2bOn9O9mzZqhXbt2cHJyQkREBExMTHQ2rsDAQK3r/NnZ2QxFRERELzGdXzJ7nJWVFRo2bIiLFy+ie/fuyMvLQ2ZmptYsUVpamrTmyN7eHkePHtXq49+3tT7p1leVSvXE0KVUKqV1AERUsfi4g7Ljg0iJqo7O7zJ7XHk/RVytVuP06dNIT0+X2kRFRUGlUsHd3V1q83gfxW3+/UnkREREJF86DURTp05FbGwsrly5gkOHDuGtt96Cvr4+hg4dCktLS/j7+2PKlCnYu3cv4uPjMXLkSKjVarRv3x4A0KNHD7i7u8PX1xcnT55EZGQkZs+ejYCAAGmGZ9y4cbh8+TKmT5+Ov//+G6tXr0ZERESpd6IQERGRPOn0ktm1a9cwdOhQ3L59G7a2tnj99ddx+PBh2NraAgCWL18OPT099O/fH7m5ufDy8sLq1aul7fX19bF9+3aMHz8earUaZmZmGD58OBYsWCC1cXFxwY4dOzB58mSsWLECdevWxQ8//MBb7omIiEii00C0fv36p9YbGxtj1apVWLVq1RPbODk5YefOnU/tx8PDAwkJCc81RiIiInr5vVBriIiIiIh0gYGIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZO+FCUSLFy+GQqHApEmTpLKHDx8iICAANjY2MDc3R//+/ZGWlqa1XXJyMnx8fGBqaopatWph2rRpKCgo0GoTExODVq1aQalUokGDBggNDa2CIyIiIqLq4oUIRMeOHcO3336LZs2aaZVPnjwZ27Ztw8aNGxEbG4sbN26gX79+Un1hYSF8fHyQl5eHQ4cOYe3atQgNDcWcOXOkNklJSfDx8UGXLl2g0WgwadIkjB49GpGRkVV2fERERPRi03kgunfvHoYNG4bvv/8eNWrUkMqzsrIQHByMZcuWoWvXrmjdujVCQkJw6NAhHD58GACwe/du/PXXX/j555/RokUL9OzZEwsXLsSqVauQl5cHAAgKCoKLiwu+/PJLNGrUCBMmTMCAAQOwfPlynRwvERERvXh0HogCAgLg4+MDT09PrfL4+Hjk5+drlbu5ucHR0RFxcXEAgLi4ODRt2hR2dnZSGy8vL2RnZ+Ps2bNSm3/37eXlJfVRmtzcXGRnZ2u9iIiI6OVloMudr1+/HidOnMCxY8dK1KWmpsLIyAhWVlZa5XZ2dkhNTZXaPB6GiuuL657WJjs7Gw8ePICJiUmJfS9atAjz589/7uMiIiKi6kVnM0RXr17FBx98gLCwMBgbG+tqGKUKDAxEVlaW9Lp69aquh0RERESVSGeBKD4+Hunp6WjVqhUMDAxgYGCA2NhYfP311zAwMICdnR3y8vKQmZmptV1aWhrs7e0BAPb29iXuOit+/6w2KpWq1NkhAFAqlVCpVFovIiIiennpLBB169YNp0+fhkajkV5t2rTBsGHDpH8bGhpiz5490jaJiYlITk6GWq0GAKjVapw+fRrp6elSm6ioKKhUKri7u0ttHu+juE1xH0REREQ6W0NkYWGBJk2aaJWZmZnBxsZGKvf398eUKVNgbW0NlUqFiRMnQq1Wo3379gCAHj16wN3dHb6+vliyZAlSU1Mxe/ZsBAQEQKlUAgDGjRuHlStXYvr06Rg1ahSio6MRERGBHTt2VO0BExER0QtLp4uqn2X58uXQ09ND//79kZubCy8vL6xevVqq19fXx/bt2zF+/Hio1WqYmZlh+PDhWLBggdTGxcUFO3bswOTJk7FixQrUrVsXP/zwA7y8vHRxSERERPQCeqECUUxMjNZ7Y2NjrFq1CqtWrXriNk5OTti5c+dT+/Xw8EBCQkJFDJGIiIheQjp/DhERERGRrjEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsPVcg2r9/P9555x2o1Wpcv34dAPDTTz/hwIEDFTo4IiIioqpQ7kD022+/wcvLCyYmJkhISEBubi4AICsrC5999lmFD5CIiIiospU7EH3yyScICgrC999/D0NDQ6m8Y8eOOHHiRIUOjoiIiKgqlDsQJSYmolOnTiXKLS0tkZmZWRFjIiIiIqpS5Q5E9vb2uHjxYonyAwcOoF69ehUyKCIiIqKqVO5ANGbMGHzwwQc4cuQIFAoFbty4gbCwMEydOhXjx4+vjDESERERVSqD8m4wc+ZMFBUVoVu3brh//z46deoEpVKJqVOnYuLEiZUxRiIiIqJKVa5AVFhYiIMHDyIgIADTpk3DxYsXce/ePbi7u8Pc3LyyxkhERERUqcoViPT19dGjRw+cO3cOVlZWcHd3r6xxEREREVWZcq8hatKkCS5fvlwZYyEiIiLSied6DtHUqVOxfft2pKSkIDs7W+tFREREVN2Ue1F1r169AAC9e/eGQqGQyoUQUCgUKCwsrLjREREREVWBcgeivXv3VsY4iIiIiHSm3IGoc+fOlTEOIiIiIp0pdyACgMzMTAQHB+PcuXMAgMaNG2PUqFGwtLSs0MERERERVYVyL6o+fvw46tevj+XLlyMjIwMZGRlYtmwZ6tevzw93JSIiomqp3IFo8uTJ6N27N65cuYJNmzZh06ZNSEpKwv/+9z9MmjSpEoZIRERVZdGiRWjbti0sLCxQq1Yt9O3bF4mJiVptLl26hLfeegu2trZQqVQYNGgQ0tLSpPorV67A398fLi4uMDExQf369TF37lzk5eVp9RMZGYn27dvDwsICtra26N+/P65cufLU8fXu3RuOjo4wNjZG7dq14evrixs3bkj1iYmJ6NKlC+zs7GBsbIx69eph9uzZyM/P/+8nh15qzzVDNGPGDBgY/P/VNgMDA0yfPh3Hjx+v0MEREVHVio2NRUBAAA4fPoyoqCjk5+ejR48eyMnJAQDk5OSgR48eUCgUiI6OxsGDB5GXl4c333wTRUVFAIC///4bRUVF+Pbbb3H27FksX74cQUFBmDVrlrSfpKQk9OnTB127doVGo0FkZCRu3bqFfv36PXV8Xbp0QUREBBITE/Hbb7/h0qVLGDBggFRvaGgIPz8/7N69G4mJifjqq6/w/fffY+7cuZVwtuhlUu41RCqVCsnJyXBzc9Mqv3r1KiwsLCpsYEREVPV27dql9T40NBS1atVCfHw8OnXqhIMHD+LKlStISEiASqUCAKxduxY1atRAdHQ0PD094e3tDW9vb6mPevXqITExEWvWrMHSpUsBAPHx8SgsLMQnn3wCPb1Hf5tPnToVffr0QX5+PgwNDUsd3+TJk6V/Ozk5YebMmejbt6+0Tb169VCvXj2tNjExMdi/f3/FnCB6aZV7hmjw4MHw9/fHhg0bcPXqVVy9ehXr16/H6NGjMXTo0MoYIxER6UhWVhYAwNraGgCQm5sLhUIBpVIptTE2Noaenh4OHDjw1H6K+wCA1q1bQ09PDyEhISgsLERWVhZ++ukneHp6PjEM/VtGRgbCwsLQoUOHJ25z8eJF7Nq1i3dI0zOVOxAtXboU/fr1g5+fH5ydneHs7IwRI0ZgwIAB+Pzzz8vV15o1a9CsWTOoVCqoVCqo1Wr88ccfUv3Dhw8REBAAGxsbmJubo3///lrXqQEgOTkZPj4+MDU1Ra1atTBt2jQUFBRotYmJiUGrVq2gVCrRoEEDhIaGlvewiYhkp6ioCJMmTULHjh3RpEkTAED79u1hZmaGGTNm4P79+8jJycHUqVNRWFiIlJSUUvu5ePEivvnmG7z77rtSmYuLC3bv3o1Zs2ZBqVTCysoK165dQ0RExDPHNWPGDJiZmcHGxgbJycnYsmVLiTYdOnSAsbExXn31VbzxxhtYsGDBc54FkotyByIjIyOsWLECd+7cgUajgUajQUZGBpYvX671F0NZ1K1bF4sXL0Z8fDyOHz+Orl27ok+fPjh79iyAR1Oj27Ztw8aNGxEbG4sbN25oXV8uLCyEj48P8vLycOjQIaxduxahoaGYM2eO1CYpKQk+Pj7o0qULNBoNJk2ahNGjRyMyMrK8h05EJCsBAQE4c+YM1q9fL5XZ2tpi48aN2LZtG8zNzWFpaYnMzEy0atVKuvT1uOvXr8Pb2xsDBw7EmDFjpPLU1FSMGTMGw4cPx7FjxxAbGwsjIyMMGDAAQoinjmvatGlISEjA7t27oa+vDz8/vxLbbNiwASdOnEB4eDh27NghXaojehKFeNZ33r9kZWWhsLBQa+oTeDR1aWBgIF1Tfl7W1tb44osvMGDAANja2iI8PFxaMPf333+jUaNGiIuLQ/v27fHHH3/gf//7H27cuAE7OzsAQFBQEGbMmIGbN2/CyMgIM2bMwI4dO3DmzBlpH0OGDEFmZmaJa+VPkp2dDUtLS2RlZf3n4yuN88wdFd7ny+rKYp8K64vnvex43nWjIs97eU2YMAFbtmzBvn374OLiUmqbW7duwcDAAFZWVrC3t8eHH36IadOmSfU3btyAh4cH2rdvj9DQUK3A9PHHH2PXrl04duyYVHbt2jU4ODhIP+PLonibQ4cOQa1Wl9rm559/xtixY3H37l3o6+uXqV96OZTn93e5Z4iGDBmi9ddCsYiICAwZMqS83UkKCwuxfv165OTkQK1WIz4+Hvn5+fD09JTauLm5wdHREXFxcQCAuLg4NG3aVApDAODl5YXs7GxplikuLk6rj+I2xX0QEdH/E0JgwoQJ+P333xEdHf3EMAQANWvWhJWVFaKjo5Geno7evXtLddevX4eHhwdat26NkJCQErNH9+/fL1FWHFaK71Yri+K2ubm5T22Tn59frn5JfsodiI4cOYIuXbqUKPfw8MCRI0fKPYDTp0/D3NwcSqUS48aNw++//w53d3ekpqbCyMgIVlZWWu3t7OyQmpoK4NGU6+NhqLi+uO5pbbKzs/HgwYNSx5Sbm4vs7GytFxGRHAQEBODnn39GeHg4LCwskJqaitTUVK2flyEhITh8+DAuXbqEn3/+GQMHDsTkyZPh6uoK4P/DkKOjI5YuXYqbN29K/RTz8fHBsWPHsGDBAly4cAEnTpzAyJEj4eTkhJYtWwIAjh49Cjc3N1y/fh3Ao98/K1euhEajwT///IPo6GgMHToU9evXl2aHwsLCEBERgXPnzuHy5cuIiIhAYGAgBg8eXObF2iRP5b7tPjc3t8SiZQDIz89/YsB4GldXV2g0GmRlZeHXX3/F8OHDERsbW+5+KtKiRYswf/58nY6BiEgX1qxZA+DRH7mPCwkJwYgRIwA8evhhYGAgMjIy4OzsjI8++kjrdvioqChcvHgRFy9eRN26dbX6KV6l0bVrV4SHh2PJkiVYsmQJTE1NoVarsWvXLpiYmAB4NIuUmJgoPVTR1NQUmzZtwty5c5GTk4PatWvD29sbs2fPltawGhgY4PPPP8f58+chhICTkxMmTJigNT6i0pQ7EL322mv47rvv8M0332iVBwUFoXXr1uUegJGRERo0aADg0W2Yx44dw4oVKzB48GDk5eUhMzNTa5YoLS0N9vb2AAB7e3scPXpUq7/iu9Aeb/PvO9PS0tKgUqmk/3T/FhgYiClTpkjvs7Oz4eDgUO5jIyKqbsqyrHTx4sVYvHjxE+tHjBghhaenGTJkyFOXWnh4eGiNp2nTpoiOjn5qn4MHD8bgwYOfuW+ifyt3IPrkk0/g6emJkydPolu3bgCAPXv24NixY9i9e/d/HlBRURFyc3PRunVrGBoaYs+ePejfvz+AR3+VJCcnS1OjarUan376KdLT01GrVi0Aj/4yUalUcHd3l9rs3LlTax9RUVFPXHwHAEqlstx3zBEREVH1Ve5A1LFjR8TFxeGLL75AREQETExM0KxZMwQHB+PVV18tV1+BgYHo2bMnHB0dcffuXYSHhyMmJgaRkZGwtLSEv78/pkyZAmtra6hUKkycOBFqtVq6+6BHjx5wd3eHr68vlixZgtTUVMyePRsBAQFSoBk3bhxWrlyJ6dOnY9SoUYiOjkZERAR27OCdLkRERPRIuQMRALRo0QJhYWH/eefp6enw8/NDSkoKLC0t0axZM0RGRqJ79+4AgOXLl0NPTw/9+/dHbm4uvLy8sHr1aml7fX19bN++HePHj4darYaZmRmGDx+u9QAuFxcX7NixA5MnT8aKFStQt25d/PDDD/Dy8vrP4yciqi74uIOy0+XjDkh3yhyICgoKUFhYqHUpKS0tDUFBQcjJyUHv3r3x+uuvl2vnwcHBT603NjbGqlWrsGrVqie2cXJyKnFJ7N88PDyQkJBQrrERERGRfJQ5EI0ZMwZGRkb49ttvAQB3795F27Zt8fDhQ9SuXRvLly/Hli1b0KtXr0obLBEREVFlKPNziA4ePCgtbgaAdevWobCwEBcuXMDJkycxZcoUfPHFF5UySCIiIqLKVOZAdP36da1F08V3f1laWgIAhg8fLj0dmoiIiKg6KXMgMjY21nrw4uHDh9GuXTut+nv37lXs6IiIiIiqQJkDUYsWLfDTTz8BAPbv34+0tDR07dpVqr906RLq1KlT8SMkIiIiqmRlXlQ9Z84c9OzZExEREUhJScGIESNQu3Ztqf73339Hx44dK2WQRERERJWpzIGoc+fOiI+Px+7du2Fvb4+BAwdq1bdo0QKvvfZahQ+QiIiIqLKV68GMjRo1QqNGjUqtGzt2bIUMiIiIiKiqlXkNEREREdHLioGIiIiIZI+BiIiIiGSPgYiIiIhkr9yfdl9QUICzZ88iNTUVAGBvbw93d3cYGhpW+OCIiIiIqkKZA1FRURHmzJmDVatWISsrS6vO0tISEyZMwPz586Gnx0knIiIiql7KHIhmzpyJ0NBQLF68GF5eXrCzswMApKWlYffu3fj444+Rl5eHzz//vNIGS0RERFQZyhyI1q1bh59++gleXl5a5c7Ozhg7diycnJzg5+fHQERERETVTpmvb929e/epn1VWu3Zt5OTkVMigiIiIiKpSmQORh4cHpk6dilu3bpWou3XrFmbMmAEPD4+KHBsRERFRlSjzJbOgoCD06tULtWvXRtOmTbXWEJ0+fRru7u7Yvn17pQ2UiIiIqLKUORA5ODjg5MmTiIyMxOHDh6Xb7l977TV89tln6NGjB+8wIyIiomqpXM8h0tPTQ8+ePdGzZ8/KGg8RERFRlauwKZ2cnBzs27evorojIiIiqjIVFoguXryILl26VFR3RERERFWGi36IiIhI9sq8hsja2vqp9YWFhf95MERERES6UOZAlJubi/Hjx6Np06al1v/zzz+YP39+hQ2MiIiIqKqUORC1aNECDg4OGD58eKn1J0+eZCAiIiKiaqnMa4h8fHyQmZn5xHpra2v4+flVxJiIiIiIqlSZZ4hmzZr11HoHBweEhIT85wERERERVbUKvcvswYMHFdkdERERUZWokECUm5uLL7/8Ei4uLhXRHREREVGVKnMgys3NRWBgINq0aYMOHTpg8+bNAICQkBC4uLjgq6++wuTJkytrnERERESVpsxriObMmYNvv/0Wnp6eOHToEAYOHIiRI0fi8OHDWLZsGQYOHAh9ff3KHCsRERFRpShzINq4cSPWrVuH3r1748yZM2jWrBkKCgpw8uRJKBSKyhwjERERUaUq8yWza9euoXXr1gCAJk2aQKlUYvLkyQxDREREVO2VORAVFhbCyMhIem9gYABzc/NKGRQRERFRVSrzJTMhBEaMGAGlUgkAePjwIcaNGwczMzOtdps2barYERIRERFVsjIHon9/ZMc777xT4YMhIiIi0oUyByI+hZqIiIheVhXyYEYhBP744w8MGDCgIrojIiIiqlL/KRAlJSXh448/hqOjI9566y08fPiwosZFREREVGXKfMmsWG5uLn799VcEBwfjwIEDKCwsxNKlS+Hv7w+VSlUZYyQiIiKqVGWeIYqPj8d7770He3t7fPXVV+jbty+uXr0KPT09eHl5MQwRERFRtVXmGaJ27dph4sSJOHz4MFxdXStzTERERERVqsyBqFu3bggODkZ6ejp8fX3h5eXFp1QTERHRS6HMl8wiIyNx9uxZuLq6Yvz48ahduzY++OADAGAwIiIiomqtXHeZOTg4YM6cOUhKSsJPP/2EmzdvwsDAAH369MGsWbNw4sSJyhonERERUaV57tvuu3fvjvDwcNy4cQMTJ07EH3/8gbZt21bk2IiIiIiqRJkDUVFRET7//HN07NgRbdu2xcyZM/HgwQPUqFEDEydOREJCAo4dO1aZYyUiIiKqFGUORJ9++ilmzZoFc3NzvPLKK1ixYgUCAgK02rRq1arCB0hERERU2cociNatW4fVq1cjMjISmzdvxrZt2xAWFoaioqLKHB8RERFRpStzIEpOTkavXr2k956enlAoFLhx40alDIyIiIioqpQ5EBUUFMDY2FirzNDQEPn5+RU+KCIiIqKqVOYHMwohMGLECCiVSqns4cOHGDduHMzMzKSyTZs2VewIiYiIiCpZmWeIhg8fjlq1asHS0lJ6vfPOO6hTp45WGREREZXPvn378Oabb6JOnTpQKBTYvHmzVn1aWhpGjBiBOnXqwNTUFN7e3rhw4YJWm++++w4eHh5QqVRQKBTIzMwssZ+MjAwMGzYMKpUKVlZW8Pf3x7179546ttTUVPj6+sLe3h5mZmZo1aoVfvvtN602vXv3hqOjI4yNjVG7dm34+vpWuyU1ZZ4hCgkJqcxxEBERyVZOTg6aN2+OUaNGoV+/flp1Qgj07dsXhoaG2LJlC1QqFZYtWwZPT0/89ddf0lWa+/fvw9vbG97e3ggMDCx1P8OGDUNKSgqioqKQn5+PkSNHYuzYsQgPD3/i2Pz8/JCZmYmtW7eiZs2aCA8Px6BBg3D8+HG0bNkSANClSxfMmjULtWvXxvXr1zF16lQMGDAAhw4dqqAzVPnKHIiIiIiocvTs2RM9e/Yste7ChQs4fPgwzpw5g8aNGwMA1qxZA3t7e/zyyy8YPXo0AGDSpEkAgJiYmFL7OXfuHHbt2oVjx46hTZs2AIBvvvkGvXr1wtKlS1GnTp1Stzt06BDWrFmD1157DQAwe/ZsLF++HPHx8VIgmjx5stTeyckJM2fORN++fZGfnw9DQ8PynQwdee4nVVeERYsWoW3btrCwsECtWrXQt29fJCYmarV5+PAhAgICYGNjA3Nzc/Tv3x9paWlabZKTk+Hj4wNTU1PUqlUL06ZNQ0FBgVabmJgYtGrVCkqlEg0aNEBoaGhlHx4REdF/lpubCwBaNzbp6elBqVTiwIEDZe4nLi4OVlZWUhgCHt0xrqenhyNHjjxxuw4dOmDDhg3IyMhAUVER1q9fj4cPH8LDw6PU9hkZGQgLC0OHDh2qTRgCdByIYmNjERAQgMOHD0vTdz169EBOTo7UZvLkydi2bRs2btyI2NhY3LhxQ2s6sbCwED4+PsjLy8OhQ4ewdu1ahIaGYs6cOVKbpKQk+Pj4oEuXLtBoNJg0aRJGjx6NyMjIKj1eIiKi8nJzc4OjoyMCAwNx584d5OXl4fPPP8e1a9eQkpJS5n5SU1NRq1YtrTIDAwNYW1sjNTX1idtFREQgPz8fNjY2UCqVePfdd/H777+jQYMGWu1mzJgBMzMz2NjYIDk5GVu2bCnfgeqYTgPRrl27MGLECDRu3BjNmzdHaGgokpOTER8fDwDIyspCcHAwli1bhq5du6J169YICQnBoUOHcPjwYQDA7t278ddff+Hnn39GixYt0LNnTyxcuBCrVq1CXl4eACAoKAguLi748ssv0ahRI0yYMAEDBgzA8uXLdXbsREREZWFoaIhNmzbh/PnzsLa2hqmpKfbu3YuePXtCT6/yf41//PHHyMzMxJ9//onjx49jypQpGDRoEE6fPq3Vbtq0aUhISMDu3buhr68PPz8/CCEqfXwVRaeB6N+ysrIAANbW1gCA+Ph45Ofnw9PTU2pTnJTj4uIAPJoCbNq0Kezs7KQ2Xl5eyM7OxtmzZ6U2j/dR3Ka4j3/Lzc1Fdna21ouIiEhXWrduDY1Gg8zMTKSkpGDXrl24ffs26tWrV+Y+7O3tkZ6erlVWUFCAjIwM2Nvbl7rNpUuXsHLlSvz444/o1q0bmjdvjrlz56JNmzZYtWqVVtuaNWuiYcOG6N69O9avX4+dO3dKkxfVwQsTiIqKijBp0iR07NgRTZo0AfBoes/IyAhWVlZabe3s7KTpvdTUVK0wVFxfXPe0NtnZ2Xjw4EGJsSxatEjrUQIODg4VcoxERET/haWlJWxtbXHhwgUcP34cffr0KfO2arUamZmZ0lUYAIiOjkZRURHatWtX6jb3798HgBIzUfr6+k/96K7iuuL1T9XBCxOIAgICcObMGaxfv17XQ0FgYCCysrKk19WrV3U9JCIieondu3cPGo0GGo0GwKO1rxqNBsnJyQCAjRs3IiYmBpcvX8aWLVvQvXt39O3bFz169JD6SE1NhUajwcWLFwEAp0+fhkajQUZGBgCgUaNG8Pb2xpgxY3D06FEcPHgQEyZMwJAhQ6Q7zK5fvw43NzccPXoUwKOrMg0aNMC7776Lo0eP4tKlS/jyyy8RFRWFvn37AgCOHDmClStXQqPR4J9//kF0dDSGDh2K+vXrQ61WV8XpqxAvRCCaMGECtm/fjr1796Ju3bpSub29PfLy8ko8XCotLU2a3rO3ty9x11nx+2e1UalUMDExKTEepVIJlUql9SIiIqosxc/0Kb6NfcqUKWjZsqV0g1BKSgp8fX3h5uaG999/H76+vvjll1+0+ggKCkLLli0xZswYAECnTp3QsmVLbN26VWoTFhYGNzc3dOvWDb169cLrr7+O7777TqrPz89HYmKiNDNkaGiInTt3wtbWFm+++SaaNWuGdevWYe3atdLnm5qammLTpk3o1q0bXF1d4e/vj2bNmiE2Nlbr0y1edDp9DpEQAhMnTsTvv/+OmJgYuLi4aNW3bt0ahoaG2LNnD/r37w8ASExMRHJyspQ61Wo1Pv30U6Snp0ur56OioqBSqeDu7i612blzp1bfUVFR1Sq5EhHRy8vDw+OpC5Dff/99vP/++0/tY968eZg3b95T21hbWz/1IYzOzs4lxvHqq6+WeDL145o2bYro6Oin7rc60GkgCggIQHh4OLZs2QILCwtpzY+lpSVMTExgaWkJf39/TJkyBdbW1lCpVJg4cSLUajXat28PAOjRowfc3d3h6+uLJUuWIDU1FbNnz0ZAQICUTMeNG4eVK1di+vTpGDVqFKKjoxEREYEdO3bo7NiJiIjoxaHTS2Zr1qxBVlYWPDw8ULt2bem1YcMGqc3y5cvxv//9D/3790enTp1gb2+v9QGy+vr62L59O/T19aFWq/HOO+/Az88PCxYskNq4uLhgx44diIqKQvPmzfHll1/ihx9+gJeXV5UeLxEREb2YdH7J7FmMjY2xatWqErf3Pc7JyanEJbF/8/DwQEJCQrnHSERE9LycZ/JKRFldWeyj0/2/EIuqiYiIiHSJgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkT6eBaN++fXjzzTdRp04dKBQKbN68WateCIE5c+agdu3aMDExgaenJy5cuKDVJiMjA8OGDYNKpYKVlRX8/f1x7949rTanTp3CG2+8AWNjYzg4OGDJkiWVfWhERERUjeg0EOXk5KB58+ZYtWpVqfVLlizB119/jaCgIBw5cgRmZmbw8vLCw4cPpTbDhg3D2bNnERUVhe3bt2Pfvn0YO3asVJ+dnY0ePXrAyckJ8fHx+OKLLzBv3jx89913lX58REREVD0Y6HLnPXv2RM+ePUutE0Lgq6++wuzZs9GnTx8AwLp162BnZ4fNmzdjyJAhOHfuHHbt2oVjx46hTZs2AIBvvvkGvXr1wtKlS1GnTh2EhYUhLy8PP/74I4yMjNC4cWNoNBosW7ZMKzgRERGRfL2wa4iSkpKQmpoKT09PqczS0hLt2rVDXFwcACAuLg5WVlZSGAIAT09P6Onp4ciRI1KbTp06wcjISGrj5eWFxMRE3Llzp9R95+bmIjs7W+tFREREL68XNhClpqYCAOzs7LTK7ezspLrU1FTUqlVLq97AwADW1tZabUrr4/F9/NuiRYtgaWkpvRwcHP77AREREdEL64UNRLoUGBiIrKws6XX16lVdD4mIiIgq0QsbiOzt7QEAaWlpWuVpaWlSnb29PdLT07XqCwoKkJGRodWmtD4e38e/KZVKqFQqrRcRERG9vF7YQOTi4gJ7e3vs2bNHKsvOzsaRI0egVqsBAGq1GpmZmYiPj5faREdHo6ioCO3atZPa7Nu3D/n5+VKbqKgouLq6okaNGlV0NERERPQi02kgunfvHjQaDTQaDYBHC6k1Gg2Sk5OhUCgwadIkfPLJJ9i6dStOnz4NPz8/1KlTB3379gUANGrUCN7e3hgzZgyOHj2KgwcPYsKECRgyZAjq1KkDAHj77bdhZGQEf39/nD17Fhs2bMCKFSswZcoUHR01ERERvWh0etv98ePH0aVLF+l9cUgZPnw4QkNDMX36dOTk5GDs2LHIzMzE66+/jl27dsHY2FjaJiwsDBMmTEC3bt2gp6eH/v374+uvv5bqLS0tsXv3bgQEBKB169aoWbMm5syZw1vuiYiISKLTQOTh4QEhxBPrFQoFFixYgAULFjyxjbW1NcLDw5+6n2bNmmH//v3PPU4iIiJ6ub2wa4iIiIiIqgoDEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREcmerALRqlWr4OzsDGNjY7Rr1w5Hjx7V9ZCIiIjoBSCbQLRhwwZMmTIFc+fOxYkTJ9C8eXN4eXkhPT1d10MjIiIiHZNNIFq2bBnGjBmDkSNHwt3dHUFBQTA1NcWPP/6o66ERERGRjhnoegBVIS8vD/Hx8QgMDJTK9PT04Onpibi4uBLtc3NzkZubK73PysoCAGRnZ1fK+Ipy71dKvy+jivwa8LyXHc+7bvC86wbPu25Uxu/Y4j6FEM9sK4tAdOvWLRQWFsLOzk6r3M7ODn///XeJ9osWLcL8+fNLlDs4OFTaGKlsLL/S9QjkieddN3jedYPnXTcq87zfvXsXlpaWT20ji0BUXoGBgZgyZYr0vqioCBkZGbCxsYFCodDhyKpOdnY2HBwccPXqVahUKl0PRxZ4znWD5103eN51Q27nXQiBu3fvok6dOs9sK4tAVLNmTejr6yMtLU2rPC0tDfb29iXaK5VKKJVKrTIrK6vKHOILS6VSyeI/zYuE51w3eN51g+ddN+R03p81M1RMFouqjYyM0Lp1a+zZs0cqKyoqwp49e6BWq3U4MiIiInoRyGKGCACmTJmC4cOHo02bNnjttdfw1VdfIScnByNHjtT10IiIiEjHZBOIBg8ejJs3b2LOnDlITU1FixYtsGvXrhILrekRpVKJuXPnlrh0SJWH51w3eN51g+ddN3jen0whynIvGhEREdFLTBZriIiIiIiehoGIiIiIZI+BiIiIiGSPgYiIiIhkj4FIRm7evInx48fD0dERSqUS9vb28PLywsGDBwEAzs7OUCgUUCgU0NfXR506deDv7487d+5Ifdy/fx+BgYGoX78+jI2NYWtri86dO2PLli26OqwX2ogRI6BQKDBu3LgSdQEBAVAoFBgxYoTUtm/fvk/s6/Gvj5mZGVq1aoWNGzdW0sirryd9n8fGxqJmzZpYvHhxqdstXLgQdnZ2yM/PR2hoKBQKBRo1alSi3caNG6FQKODs7FzJR1K1CgsL0aFDB/Tr10+rPCsrCw4ODvjoo4+kst9++w1du3ZFjRo1YGJiAldXV4waNQoJCQlSm+JzWPwyNzdH69atsWnTpio7JgDw8PDApEmTqnSfVenNN9+Et7d3qXX79++HQqHAqVOntL4WRkZGaNCgAT755BOtz/hKSkrC22+/jTp16sDY2Bh169ZFnz59Sv2Iq5cRA5GM9O/fHwkJCVi7di3Onz+PrVu3wsPDA7dv35baLFiwACkpKUhOTkZYWBj27duH999/X6ofN24cNm3ahG+++QZ///03du3ahQEDBmj1QdocHBywfv16PHjwQCp7+PAhwsPD4ejoWK6+ir8+CQkJaNu2LQYPHoxDhw5V9JCrtSd9n2dlZeGdd95BSEhIiW2EEAgNDYWfnx8MDQ0BAGZmZkhPTy/xAdDBwcHl/rpVB/r6+ggNDcWuXbsQFhYmlU+cOBHW1taYO3cuAGDGjBkYPHgwWrRoga1btyIxMRHh4eGoV6+e1gdoA4+ehpySkiJ9z3p5eWHQoEFITEys0mN7mfn7+yMqKgrXrl0rURcSEoI2bdpIT6T+888/kZKSggsXLmD+/Pn49NNP8eOPPwIA8vPz0b17d2RlZWHTpk1ITEzEhg0b0LRpU2RmZlblIemOIFm4c+eOACBiYmKe2MbJyUksX75cq2zhwoXC3d1dem9paSlCQ0Mra5gvneHDh4s+ffqIJk2aiJ9//lkqDwsLE82aNRN9+vQRw4cP12r7JP/++uTn5wtTU1Mxc+bMShp99fOs7/NTp04JAGL//v1a5Xv37hUAxLlz54QQQoSEhAhLS0sxYcIEMXr0aKnd1atXhVKpFDNnzhROTk6Vdhy6tGLFClGjRg1x48YNsXnzZmFoaCg0Go0QQoi4uDgBQKxYsaLUbYuKiqR/F5/DxxUWFgpDQ0MREREhlWVkZAhfX19hZWUlTExMhLe3tzh//rzWdr/++qtwd3cXRkZGwsnJSSxdulSrftWqVaJBgwZCqVSKWrVqif79+wshHv2fAqD1SkpKet5T80LKz88XdnZ2YuHChVrld+/eFebm5mLNmjUiKSlJABAJCQlabbp16ybee+89IYQQCQkJAoC4cuVKVQ39hcMZIpkwNzeHubk5Nm/ejNzc3DJtc/36dWzbtg3t2rWTyuzt7bFz507cvXu3sob6Uho1apTWzMSPP/74n5+SbmBgAENDQ+Tl5f3X4b00nvV93rRpU7Rt21b6q7hYSEgIOnToADc3N63yUaNGISIiAvfv3wfw6DKQt7f3S/1A14kTJ6J58+bw9fXF2LFjMWfOHDRv3hwA8Msvv8Dc3Bzvvfdeqds+7cOvCwsLsXbtWgBAq1atpPIRI0bg+PHj2Lp1K+Li4iCEQK9evZCfnw8AiI+Px6BBgzBkyBCcPn0a8+bNw8cff4zQ0FAAwPHjx/H+++9jwYIFSExMxK5du9CpUycAwIoVK6BWqzFmzBhppsrBweE/n6MXiYGBAfz8/BAaGqp1+Wvjxo0oLCzE0KFDS93u+PHjiI+Pl36+29raQk9PD7/++isKCwurZOwvHF0nMqo6v/76q6hRo4YwNjYWHTp0EIGBgeLkyZNSvZOTkzAyMhJmZmbC2NhYABDt2rUTd+7ckdrExsaKunXrCkNDQ9GmTRsxadIkceDAAR0cTfVQPOuTnp4ulEqluHLlirhy5YowNjYWN2/efO4ZotzcXPHZZ58JAGL79u2VfyDVyLO+z4OCgoS5ubm4e/euEEKI7OxsYWpqKn744QepzeOzGy1atBBr164VRUVFon79+mLLli1i+fLlL+0MkRBCnDt3TgAQTZs2Ffn5+VK5t7e3aNasmVbbL7/8UpiZmUmvzMxMIcSjcwhAKtfT0xNKpVKEhIRI254/f14AEAcPHpTKbt26JUxMTKRZpLffflt0795da5/Tpk2TZq5/++03oVKpRHZ2dqnH0rlzZ/HBBx8897moDoq/Xnv37pXK3njjDfHOO+8IIYQ0Q2RiYiLMzMyEoaGhACDGjh2r1c/KlSuFqampsLCwEF26dBELFiwQly5dqspD0SnOEMlI//79cePGDWzduhXe3t6IiYlBq1atpL+0AGDatGnQaDQ4deqU9GG4Pj4+0l8MnTp1wuXLl7Fnzx4MGDAAZ8+exRtvvIGFCxfq4pCqDVtbW/j4+CA0NBQhISHw8fFBzZo1y93PjBkzYG5uDlNTU3z++edYvHgxfHx8KmHE1dezvs+HDh2KwsJCREREAAA2bNgAPT09DB48uNT+imf3YmNjkZOTg169elXVoejMjz/+CFNTUyQlJZW6NuVxo0aNgkajwbfffoucnBytWQoLCwtoNBpoNBokJCTgs88+w7hx47Bt2zYAwLlz52BgYKA1C21jYwNXV1ecO3dOatOxY0etfXbs2BEXLlxAYWEhunfvDicnJ9SrVw++vr4ICwuTZvTkws3NDR06dJBmPi9evIj9+/fD399fq92GDRug0Whw8uRJREREYMuWLZg5c6ZUHxAQgNTUVISFhUGtVmPjxo1o3LgxoqKiqvR4dEbXiYx0y9/fXzg6OgohSl9DVLxmICoq6ol9LFy4UBgaGorc3NzKHGq19Pisz/bt24Wzs7NwdnYWO3bsEEKIcs8QffTRR+LChQsiJSVFa70GPd3j3+dCCOHr6ytef/11IYQQHTp0EKNGjdJq//gM0e3bt4WxsbHo3LmzmDZtmhBCvNQzRAcPHhQGBgYiOjpadO3aVXTt2lX6Xps4caIwNzcXeXl5JbYrXodVPKNc2hoiIYTw8vISHTt2FEIIsWXLFmFgYCAKCgq02rRo0ULMnz9fCCFEy5Ytxbx587Tqi9c2FW+Xn58voqKixLRp00S9evVEgwYNpHHIYYZICCGCg4OFqampyM7OFrNmzRL169eXvm5PWkO0aNEiYWBgIB48eFBqn0VFRaJ79+6iU6dOlT38FwJniGTO3d0dOTk5T6zX19cHAK07pErro6CgAA8fPqzw8b1MvL29kZeXh/z8fHh5eT1XHzVr1kSDBg1gb2//1PUapO3f3+f+/v44cOAAtm/fjkOHDpX4S/px1tbW6N27N2JjYzFq1KiqGK7O3L9/HyNGjMD48ePRpUsXBAcH4+jRowgKCgLwaHbt3r17WL169XPvQ19fX/p50qhRIxQUFODIkSNS/e3bt5GYmAh3d3epTfGjQYodPHgQDRs2lH4+GRgYwNPTE0uWLMGpU6dw5coVREdHAwCMjIxksSZm0KBB0NPTQ3h4ONatW4dRo0Y982eEvr4+CgoKnrgOUaFQwM3N7am/I14msvm0e7m7ffs2Bg4ciFGjRqFZs2awsLDA8ePHsWTJEvTp00dqd/fuXaSmpkIIgatXr2L69OmwtbVFhw4dADx6psfQoUPRpk0b2NjY4K+//sKsWbPQpUsX6dZOKp2+vr50GaD4B/m/ZWVlQaPRaJXZ2Ni8dAtBK0tZv887deqEBg0awM/PT7rc8DShoaFYvXo1bGxsKvsQdCowMBBCCOlZTc7Ozli6dCmmTp2Knj17Qq1W48MPP8SHH36If/75B/369YODgwNSUlIQHBwMhUIBPb3//ztbCIHU1FQAj/6oioqKQmRkJObMmQMAePXVV9GnTx+MGTMG3377LSwsLDBz5ky88sor0tfrww8/RNu2bbFw4UIMHjwYcXFxWLlypRTKtm/fjsuXL6NTp06oUaMGdu7ciaKiIri6ukrHcOTIEVy5cgXm5uawtrbWGuPLwtzcHIMHD0ZgYCCys7Ol55s97vbt20hNTUVBQQFOnz6NFStWSD+7NRoN5s6dC19fX7i7u8PIyAixsbH48ccfMWPGjKo/IF3Q8QwVVZGHDx+KmTNnilatWglLS0thamoqXF1dxezZs8X9+/eFEI8uyeCx21NtbW1Fr169tKZZP/vsM6FWq4W1tbUwNjYW9erVE++//764deuWjo7sxfasy2D/vmSGf90iDED4+/sLIUq/pEnayvJ9Xqx4UfqSJUtK9POkyz3FXsZLZjExMUJfX7/EIwmEEKJHjx5al842bNggPDw8hKWlpTA0NBR169YVb7/9tjh8+LC0TfGi6uKXUqkUDRs2FJ9++qnWJbLi2+4tLS2FiYmJ8PLyeuJt94aGhsLR0VF88cUXUt3+/ftF586dRY0aNYSJiYlo1qyZ2LBhg1SfmJgo2rdvL0xMTF7K2+4fd+jQIQFA9OrVS6u8+JJZ8UtfX1/UrVtXjBkzRqSnpwshhLh586Z4//33RZMmTYS5ubmwsLAQTZs2FUuXLhWFhYW6OJwqpxDisRVwRERERDL08s0bEhEREZUTAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyd7/AWpb374rd7rAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Barplot to visualize the best results on the test set\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#Values from the previous tables, the values are biased towards the test set\n",
    "scores = {'SBS': [5106.399455], 'MLP': [5077.828221], 'SVM': [4976.304403], 'XGBoost': [2928.327679], 'VBS': [1910.833772]}\n",
    "scores = pd.DataFrame(scores)\n",
    "\n",
    "#Extract the values and labels from the DataFrame\n",
    "values = scores.iloc[0].values\n",
    "labels = scores.columns.values\n",
    "\n",
    "#Bar plot\n",
    "plt.bar(labels, values)\n",
    "\n",
    "#Increase y-axis limits\n",
    "plt.ylim(0, 5500)\n",
    "\n",
    "#Add value annotations with two decimal places to each bar\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v+50, '{:.2f}'.format(v), ha='center')\n",
    "\n",
    "#Set title and labels\n",
    "#plt.title(\"Results of the best models on the test set\")\n",
    "plt.ylabel(\"PAR10 Score\")\n",
    "\n",
    "#Save the figure in the directory \"AAS\"\n",
    "plt.savefig(\"models_results.pdf\")\n",
    "\n",
    "#Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135b5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
